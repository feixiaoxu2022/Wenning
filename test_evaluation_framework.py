#!/usr/bin/env python3
"""
è¯„æµ‹æ¡†æ¶å•å…ƒæµ‹è¯•

éªŒè¯RuleCheckerã€LLMJudgeã€Orchestratorçš„åŸºæœ¬åŠŸèƒ½
"""

import json
from evaluator import RuleChecker, EvaluationOrchestrator


def test_rule_checker():
    """æµ‹è¯•RuleCheckerçš„åŸºæœ¬åŠŸèƒ½"""
    print("=" * 60)
    print("æµ‹è¯• RuleChecker")
    print("=" * 60)

    checker = RuleChecker()

    # æ„é€ æµ‹è¯•æ•°æ®
    execution_result = {
        "sample_id": "TEST_001",
        "model_name": "test-model",
        "status": "success",
        "final_state": {
            "files": [
                {"path": "icon1.png", "size": 2048, "type": "png"},
                {"path": "icon2.svg", "size": 1024, "type": "svg"},
                {"path": "icon3.jpg", "size": 3072, "type": "jpg"},
            ]
        }
    }

    # æµ‹è¯•1: file_count_range (é€šè¿‡)
    check_item = {
        "check_type": "file_count_range",
        "description": "æ–‡ä»¶æ•°é‡èŒƒå›´æ£€æŸ¥",
        "params": {"min": 2, "max": 10},
        "weight": 1.0
    }
    result = checker.check(check_item, execution_result)
    print(f"\nâœ“ file_count_range æµ‹è¯•é€šè¿‡")
    print(f"  å¾—åˆ†: {result['score']}, é€šè¿‡: {result['passed']}")
    print(f"  è¯¦æƒ…: {result['details']}")
    assert result["passed"] is True
    assert result["score"] == 1.0

    # æµ‹è¯•2: file_count_equals (ä¸é€šè¿‡)
    check_item = {
        "check_type": "file_count_equals",
        "description": "æ–‡ä»¶æ•°é‡ç²¾ç¡®æ£€æŸ¥",
        "params": {"expected": 5},
        "weight": 1.0
    }
    result = checker.check(check_item, execution_result)
    print(f"\nâœ“ file_count_equals æµ‹è¯•é€šè¿‡")
    print(f"  å¾—åˆ†: {result['score']}, é€šè¿‡: {result['passed']}")
    print(f"  è¯¦æƒ…: {result['details']}")
    assert result["passed"] is False
    assert result["score"] == 0.6  # 3/5

    # æµ‹è¯•3: file_format_check
    check_item = {
        "check_type": "file_format_check",
        "description": "æ–‡ä»¶æ ¼å¼æ£€æŸ¥",
        "params": {"expected_formats": ["png", "svg"]},
        "weight": 2.0
    }
    result = checker.check(check_item, execution_result)
    print(f"\nâœ“ file_format_check æµ‹è¯•é€šè¿‡")
    print(f"  å¾—åˆ†: {result['score']:.2f}, é€šè¿‡: {result['passed']}")
    print(f"  è¯¦æƒ…: {result['details']}")
    assert result["passed"] is False
    assert abs(result["score"] - 0.667) < 0.01  # 2/3

    # æµ‹è¯•4: image_size_check
    execution_with_metadata = {
        "sample_id": "TEST_002",
        "model_name": "test-model",
        "final_state": {
            "files": [
                {
                    "path": "icon1.png",
                    "size": 2048,
                    "type": "png",
                    "metadata": {"dimensions": {"width": 512, "height": 512}}
                },
                {
                    "path": "icon2.png",
                    "size": 3072,
                    "type": "png",
                    "metadata": {"dimensions": {"width": 600, "height": 512}}
                }
            ]
        }
    }
    check_item = {
        "check_type": "image_size_check",
        "description": "å›¾ç‰‡å°ºå¯¸æ£€æŸ¥",
        "params": {"width": 512, "height": 512, "tolerance": 0.1},
        "weight": 1.5
    }
    result = checker.check(check_item, execution_with_metadata)
    print(f"\nâœ“ image_size_check æµ‹è¯•é€šè¿‡")
    print(f"  å¾—åˆ†: {result['score']}, é€šè¿‡: {result['passed']}")
    print(f"  è¯¦æƒ…: {result['details']}")
    assert result["score"] == 0.5  # 1/2

    print("\n" + "=" * 60)
    print("âœ“ RuleChecker æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
    print("=" * 60)


def test_orchestrator_without_llm():
    """æµ‹è¯•Orchestratorï¼ˆä¸åŒ…å«LLM Judgeï¼‰"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• EvaluationOrchestrator (ä»…Rule-based)")
    print("=" * 60)

    orchestrator = EvaluationOrchestrator()

    # æ„é€ æµ‹è¯•æ ·æœ¬
    sample = {
        "data_id": "TEST_SAMPLE_001",
        "query": "æµ‹è¯•ä»»åŠ¡",
        "models": {
            "model_a": "test-model-a",
            "model_b": "test-model-b"
        },
        "check_list": [
            {
                "check_type": "file_count_range",
                "description": "æ–‡ä»¶æ•°é‡èŒƒå›´æ£€æŸ¥",
                "params": {"min": 2, "max": 10},
                "weight": 0,
                "is_required": True
            },
            {
                "check_type": "file_format_check",
                "description": "æ–‡ä»¶æ ¼å¼æ£€æŸ¥",
                "params": {"expected_formats": ["png", "svg"]},
                "weight": 2.0
            }
        ]
    }

    # æ„é€ æ‰§è¡Œç»“æœ
    execution_result_a = {
        "sample_id": "TEST_SAMPLE_001",
        "model_name": "test-model-a",
        "status": "success",
        "final_state": {
            "files": [
                {"path": "icon1.png", "size": 2048, "type": "png"},
                {"path": "icon2.svg", "size": 1024, "type": "svg"},
                {"path": "icon3.jpg", "size": 3072, "type": "jpg"},
            ]
        },
        "conversation_history": [
            {"role": "user", "content": "ç”Ÿæˆå›¾æ ‡"},
            {"role": "assistant", "content": "å¥½çš„ï¼Œå¼€å§‹ç”Ÿæˆ"}
        ]
    }

    execution_result_b = {
        "sample_id": "TEST_SAMPLE_001",
        "model_name": "test-model-b",
        "status": "success",
        "final_state": {
            "files": [
                {"path": "icon1.png", "size": 2048, "type": "png"},
                {"path": "icon2.png", "size": 2048, "type": "png"},
                {"path": "icon3.svg", "size": 1024, "type": "svg"},
                {"path": "icon4.svg", "size": 1024, "type": "svg"},
            ]
        },
        "conversation_history": [
            {"role": "user", "content": "ç”Ÿæˆå›¾æ ‡"},
            {"role": "assistant", "content": "å¥½çš„ï¼Œå¼€å§‹ç”Ÿæˆ"}
        ]
    }

    # æ‰§è¡Œè¯„æµ‹
    result = orchestrator.evaluate(
        sample=sample,
        execution_result_a=execution_result_a,
        execution_result_b=execution_result_b,
        output_file=None  # ä¸ä¿å­˜æ–‡ä»¶
    )

    # éªŒè¯ç»“æœç»“æ„
    assert "sample_id" in result
    assert "evaluated_at" in result
    assert "executions" in result
    assert "check_results" in result

    # éªŒè¯Model Aç»“æœ
    assert result["executions"]["model_a"]["model_name"] == "test-model-a"
    assert result["executions"]["model_a"]["file_count"] == 3
    assert len(result["check_results"]["model_a"]) == 2

    # éªŒè¯Model Bç»“æœ
    assert result["executions"]["model_b"]["model_name"] == "test-model-b"
    assert result["executions"]["model_b"]["file_count"] == 4
    assert len(result["check_results"]["model_b"]) == 2

    # éªŒè¯æ£€æŸ¥ç»“æœ
    model_a_file_format = result["check_results"]["model_a"][1]
    assert model_a_file_format["check_type"] == "file_format_check"
    assert abs(model_a_file_format["score"] - 0.667) < 0.01

    model_b_file_format = result["check_results"]["model_b"][1]
    assert model_b_file_format["check_type"] == "file_format_check"
    assert model_b_file_format["score"] == 1.0
    assert model_b_file_format["passed"] is True

    print("\nâœ“ Model B (4/4æ ¼å¼æ­£ç¡®) ä¼˜äº Model A (2/3æ ¼å¼æ­£ç¡®)")
    print("\n" + "=" * 60)
    print("âœ“ EvaluationOrchestrator æµ‹è¯•é€šè¿‡!")
    print("=" * 60)


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "ğŸ§ª" * 30)
    print("CreativeFlow è¯„æµ‹æ¡†æ¶å•å…ƒæµ‹è¯•")
    print("ğŸ§ª" * 30 + "\n")

    try:
        test_rule_checker()
        test_orchestrator_without_llm()

        print("\n" + "=" * 60)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼è¯„æµ‹æ¡†æ¶å·¥ä½œæ­£å¸¸")
        print("=" * 60)
        print("\nè¯´æ˜:")
        print("1. RuleChecker çš„4ç§æ£€æŸ¥ç±»å‹å‡å¯æ­£å¸¸å·¥ä½œ")
        print("2. EvaluationOrchestrator èƒ½å¤Ÿæ­£ç¡®åè°ƒè¯„æµ‹æµç¨‹")
        print("3. è¯„æµ‹ç»“æœæ ¼å¼ç¬¦åˆschemaè§„èŒƒ")
        print("\nä¸‹ä¸€æ­¥:")
        print("- å‡†å¤‡å®é™…çš„Agentæ‰§è¡Œç»“æœæ–‡ä»¶")
        print("- é…ç½®LLM Judgeå‚æ•°ï¼ˆå¦‚éœ€è¦ï¼‰")
        print("- è¿è¡Œ run_evaluation.py è¿›è¡Œå®Œæ•´è¯„æµ‹")

    except AssertionError as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        raise
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        raise


if __name__ == "__main__":
    main()
