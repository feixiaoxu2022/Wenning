#!/usr/bin/env python3
"""
æ‰§è¡Œä¸¤ä¸ªæ¨¡å‹å¹¶è¿è¡Œè¯„æµ‹

å®Œæ•´æµç¨‹:
1. åŠ è½½æ ·æœ¬
2. æ‰§è¡ŒModel A
3. æ‰§è¡ŒModel B
4. è¿è¡Œè¯„æµ‹æ¡†æ¶(Rule + LLM Judge)
5. ç”Ÿæˆè¯„æµ‹ç»“æœ(å¾…Human Annotation)
"""

import json
import sys
import os
from pathlib import Path
from datetime import datetime

sys.path.insert(0, str(Path(__file__).parent))

from src.utils.config import get_config
from src.agent.master_agent import MasterAgent
from src.tools.registry import ToolRegistry
from src.tools.atomic.web_search import WebSearchTool
from src.tools.atomic.url_fetch import URLFetchTool
from src.tools.atomic.code_executor import CodeExecutor
from evaluator import EvaluationOrchestrator


def execute_agent(sample_id: str, query: str, model_name: str):
    """æ‰§è¡Œå•ä¸ªAgent"""
    print("\n" + "=" * 80)
    print(f"æ‰§è¡Œæ¨¡å‹: {model_name}")
    print("=" * 80)

    # åˆå§‹åŒ–Agent
    config = get_config()
    registry = ToolRegistry()
    registry.register_atomic_tool(WebSearchTool(config))
    registry.register_atomic_tool(URLFetchTool(config))
    registry.register_atomic_tool(CodeExecutor(config))

    agent = MasterAgent(config, registry, model_name)

    # è®¾ç½®conversation_id
    conversation_id = f"{sample_id}_{model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    agent.current_conversation_id = conversation_id
    print(f"âœ“ Conversation ID: {conversation_id}")

    # æ‰§è¡Œä»»åŠ¡
    print(f"\nå¼€å§‹æ‰§è¡Œ...")
    try:
        result = agent.process(query)

        # æ¶ˆè´¹ç”Ÿæˆå™¨
        if hasattr(result['result'], '__iter__') and hasattr(result['result'], '__next__'):
            for _ in result['result']:
                pass

        status = "success"
        print(f"âœ“ æ‰§è¡Œå®Œæˆ")

    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        status = "failed"

    # æ‰«æç”Ÿæˆçš„æ–‡ä»¶
    output_dir = Path("outputs") / conversation_id
    final_files = []

    if output_dir.exists():
        for file_path in output_dir.rglob("*"):
            if file_path.is_file():
                rel_path = file_path.relative_to(output_dir)
                file_info = {
                    "path": str(rel_path),
                    "size": file_path.stat().st_size,
                    "type": file_path.suffix[1:] if file_path.suffix else "unknown"
                }
                final_files.append(file_info)

    print(f"âœ“ ç”Ÿæˆæ–‡ä»¶æ•°: {len(final_files)}")
    if final_files:
        for f in final_files:
            print(f"  - {f['path']} ({f['size']} bytes)")

    # æ„å»ºexecution_result
    execution_result = {
        "sample_id": sample_id,
        "model_name": model_name,
        "status": status,
        "evaluated_at": datetime.now().isoformat(),
        "initial_state": {"files": []},
        "final_state": {"files": final_files},
        "conversation_history": agent.conversation_history
    }

    # ä¿å­˜
    executions_dir = Path("executions")
    executions_dir.mkdir(exist_ok=True)
    execution_path = executions_dir / f"{model_name}_{sample_id}.json"

    with open(execution_path, 'w', encoding='utf-8') as f:
        json.dump(execution_result, f, indent=2, ensure_ascii=False)

    print(f"âœ“ æ‰§è¡Œç»“æœå·²ä¿å­˜: {execution_path}")

    return execution_result, str(output_dir)


def main():
    """ä¸»æµç¨‹"""
    print("=" * 80)
    print("CreativeFlow å®Œæ•´è¯„æµ‹æµç¨‹")
    print("=" * 80)

    # 1. åŠ è½½æ ·æœ¬
    sample_path = Path("samples/EVAL_ICON_COLLECTION_TECH.json")
    with open(sample_path, 'r', encoding='utf-8') as f:
        sample = json.load(f)

    sample_id = sample["data_id"]
    query = sample["query"]
    model_a = sample["models"]["model_a"]
    model_b = sample["models"]["model_b"]

    print(f"\næ ·æœ¬ID: {sample_id}")
    print(f"Model A: {model_a}")
    print(f"Model B: {model_b}")
    print(f"\nQuery: {query}")

    # 2. æ‰§è¡Œä¸¤ä¸ªæ¨¡å‹
    execution_a, output_dir_a = execute_agent(sample_id, query, model_a)
    execution_b, output_dir_b = execute_agent(sample_id, query, model_b)

    # 3. è¿è¡Œè¯„æµ‹
    print("\n" + "=" * 80)
    print("è¿è¡Œè¯„æµ‹æ¡†æ¶")
    print("=" * 80)

    # é…ç½®LLM Judge
    judge_model = "gemini-3-pro-preview"
    judge_base_url = "http://yy.dbh.baidu-int.com/v1"
    judge_api_key = os.getenv("AGENT_MODEL_API_KEY")

    if not judge_api_key:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ°AGENT_MODEL_API_KEY")
        sys.exit(1)

    print(f"âœ“ Judgeæ¨¡å‹: {judge_model}")

    orchestrator = EvaluationOrchestrator(
        judge_model=judge_model,
        judge_base_url=judge_base_url,
        judge_api_key=judge_api_key
    )

    result_path = Path(f"results/{sample_id}_result.json")
    result_path.parent.mkdir(exist_ok=True)

    eval_result = orchestrator.evaluate(
        sample=sample,
        execution_result_a=execution_a,
        execution_result_b=execution_b,
        output_file=result_path
    )

    # 4. è¾“å‡ºç»“æœæ‘˜è¦
    print("\n" + "=" * 80)
    print("è¯„æµ‹å®Œæˆ!")
    print("=" * 80)

    print(f"\nè¯„æµ‹ç»“æœ: {result_path}")

    print(f"\nğŸ“Š Model A ({model_a}):")
    print(f"  æ–‡ä»¶æ•°: {len(execution_a['final_state']['files'])}")
    print(f"  è¾“å‡ºç›®å½•: {output_dir_a}")

    print(f"\nğŸ“Š Model B ({model_b}):")
    print(f"  æ–‡ä»¶æ•°: {len(execution_b['final_state']['files'])}")
    print(f"  è¾“å‡ºç›®å½•: {output_dir_b}")

    print("\n" + "=" * 80)
    print("ä¸‹ä¸€æ­¥: Human Annotation")
    print("=" * 80)
    print("\nè¯·æ‰‹åŠ¨:")
    print(f"1. æŸ¥çœ‹Model Aè¾“å‡º: {output_dir_a}")
    print(f"2. æŸ¥çœ‹Model Bè¾“å‡º: {output_dir_b}")
    print(f"3. ç¼–è¾‘è¯„æµ‹ç»“æœ: {result_path}")
    print("4. åœ¨human_annotationå­—æ®µå¡«å†™:")
    print('   - winner: "model_a" | "model_b" | "tie"')
    print('   - reason: "..."')
    print('   - annotator: "ä½ çš„åå­—"')
    print('   - annotated_at: "å½“å‰æ—¶é—´"')


if __name__ == "__main__":
    main()
