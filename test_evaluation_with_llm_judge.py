#!/usr/bin/env python3
"""
å®Œæ•´è¯„æµ‹æµ‹è¯•ï¼šåŒ…å«Rule-based + LLM Judge

ä½¿ç”¨mockæ•°æ®æµ‹è¯•å®Œæ•´è¯„æµ‹æµç¨‹
"""

import json
import sys
import os
from pathlib import Path
from datetime import datetime

# æ·»åŠ srcåˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from evaluator import EvaluationOrchestrator


def create_mock_execution_result(sample_id: str, model_name: str, file_count: int) -> dict:
    """åˆ›å»ºmockæ‰§è¡Œç»“æœ"""
    files = []

    # æ¨¡æ‹Ÿç”Ÿæˆçš„å›¾æ ‡æ–‡ä»¶
    categories = ["ai", "cloud", "code", "data", "security"]
    formats = ["png", "svg"]

    for i in range(file_count):
        category = categories[i % len(categories)]
        format_type = formats[i % len(formats)]

        file_info = {
            "path": f"{category}/icon_{i+1}.{format_type}",
            "size": 2048 + i * 100,
            "type": format_type
        }

        # æ·»åŠ å›¾ç‰‡å…ƒæ•°æ®
        if format_type in ["png", "jpg"]:
            file_info["metadata"] = {
                "dimensions": {
                    "width": 512,
                    "height": 512
                }
            }

        files.append(file_info)

    return {
        "sample_id": sample_id,
        "model_name": model_name,
        "status": "success",
        "evaluated_at": datetime.now().isoformat(),
        "initial_state": {"files": []},
        "final_state": {"files": files},
        "conversation_history": [
            {
                "role": "user",
                "content": "æœé›†ç§‘æŠ€ç±»å›¾æ ‡",
                "timestamp": datetime.now().isoformat()
            },
            {
                "role": "assistant",
                "content": f"æˆ‘å·²ç»æœé›†äº†{file_count}ä¸ªç§‘æŠ€ç±»å›¾æ ‡ï¼Œè¦†ç›–äº†AIã€äº‘è®¡ç®—ã€ä»£ç ã€æ•°æ®ã€ç½‘ç»œå®‰å…¨ç­‰åœºæ™¯ï¼Œå¹¶æŒ‰ç±»åˆ«ç»„ç»‡åœ¨ä¸åŒæ–‡ä»¶å¤¹ä¸­ã€‚",
                "timestamp": datetime.now().isoformat()
            }
        ]
    }


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("CreativeFlow å®Œæ•´è¯„æµ‹æµ‹è¯• (Rule-based + LLM Judge)")
    print("=" * 80)

    # 1. åŠ è½½æ ·æœ¬
    sample_path = Path("samples/EVAL_ICON_COLLECTION_TECH.json")
    print(f"\nğŸ“‚ åŠ è½½æ ·æœ¬: {sample_path}")

    with open(sample_path, 'r', encoding='utf-8') as f:
        sample = json.load(f)

    sample_id = sample["data_id"]
    model_a = sample["models"]["model_a"]
    model_b = sample["models"]["model_b"]

    print(f"âœ“ æ ·æœ¬ID: {sample_id}")
    print(f"âœ“ Model A: {model_a}")
    print(f"âœ“ Model B: {model_b}")

    # 2. åˆ›å»ºmockæ‰§è¡Œç»“æœ
    print("\n" + "=" * 80)
    print("åˆ›å»ºMockæ‰§è¡Œç»“æœ")
    print("=" * 80)

    # Model A: 15ä¸ªæ–‡ä»¶,è´¨é‡è¾ƒé«˜
    execution_result_a = create_mock_execution_result(sample_id, model_a, 15)
    print(f"âœ“ Model A mock: {len(execution_result_a['final_state']['files'])} ä¸ªæ–‡ä»¶")

    # Model B: 25ä¸ªæ–‡ä»¶,æ•°é‡æ›´å¤š
    execution_result_b = create_mock_execution_result(sample_id, model_b, 25)
    print(f"âœ“ Model B mock: {len(execution_result_b['final_state']['files'])} ä¸ªæ–‡ä»¶")

    # 3. é…ç½®LLM Judge
    print("\n" + "=" * 80)
    print("é…ç½®LLM Judge")
    print("=" * 80)

    judge_model = os.getenv("JUDGE_MODEL", "claude-sonnet-4-5-20250929")
    judge_base_url = os.getenv("JUDGE_BASE_URL", "https://api.anthropic.com/v1")
    judge_api_key = os.getenv("ANTHROPIC_API_KEY")

    if not judge_api_key:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ°ANTHROPIC_API_KEYç¯å¢ƒå˜é‡")
        print("è¯·åœ¨.envæ–‡ä»¶ä¸­æ·»åŠ : ANTHROPIC_API_KEY=your_key")
        sys.exit(1)

    print(f"âœ“ Judgeæ¨¡å‹: {judge_model}")
    print(f"âœ“ Base URL: {judge_base_url}")

    # 4. è¿è¡Œè¯„æµ‹(åŒ…å«LLM Judge)
    print("\n" + "=" * 80)
    print("è¿è¡Œå®Œæ•´è¯„æµ‹ (Rule-based + LLM Judge)")
    print("=" * 80)

    # è¿‡æ»¤æ‰human_annotation
    sample_copy = sample.copy()
    sample_copy["check_list"] = [
        check for check in sample["check_list"]
        if check["check_type"] != "human_annotation"
    ]

    print(f"âœ“ åŒ…å« {len(sample_copy['check_list'])} ä¸ªæ£€æŸ¥é¡¹:")
    for check in sample_copy["check_list"]:
        print(f"  - [{check['check_type']}] {check['description']}")

    # åˆ›å»ºè¯„æµ‹å™¨
    orchestrator = EvaluationOrchestrator(
        judge_model=judge_model,
        judge_base_url=judge_base_url,
        judge_api_key=judge_api_key
    )

    # æ‰§è¡Œè¯„æµ‹
    result_path = Path(f"results/{sample_id}_full_result.json")
    result_path.parent.mkdir(exist_ok=True)

    try:
        eval_result = orchestrator.evaluate(
            sample=sample_copy,
            execution_result_a=execution_result_a,
            execution_result_b=execution_result_b,
            output_file=result_path
        )

        print(f"\nâœ“ è¯„æµ‹ç»“æœå·²ä¿å­˜: {result_path}")

        # 5. è¾“å‡ºç»“æœæ‘˜è¦
        print("\n" + "=" * 80)
        print("è¯„æµ‹ç»“æœæ‘˜è¦")
        print("=" * 80)

        print(f"\nğŸ“Š Model A ({model_a}):")
        print(f"  - æ–‡ä»¶æ•°: {len(execution_result_a['final_state']['files'])}")
        for check_result in eval_result['check_results']['model_a']:
            print(f"  - [{check_result['check_type']}] {check_result['description']}")
            print(f"    å¾—åˆ†: {check_result['score']:.2f}, é€šè¿‡: {check_result['passed']}")

        print(f"\nğŸ“Š Model B ({model_b}):")
        print(f"  - æ–‡ä»¶æ•°: {len(execution_result_b['final_state']['files'])}")
        for check_result in eval_result['check_results']['model_b']:
            print(f"  - [{check_result['check_type']}] {check_result['description']}")
            print(f"    å¾—åˆ†: {check_result['score']:.2f}, é€šè¿‡: {check_result['passed']}")

        print("\nâœ… å®Œæ•´è¯„æµ‹æµ‹è¯•æˆåŠŸ!")

    except Exception as e:
        print(f"\nâŒ è¯„æµ‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

    print("\nè¯´æ˜:")
    print("1. âœ“ ä½¿ç”¨äº†mockæ•°æ®")
    print("2. âœ“ æµ‹è¯•äº†Rule-basedæ£€æŸ¥")
    print("3. âœ“ æµ‹è¯•äº†LLM Judgeæ£€æŸ¥")
    print("4. â­  è·³è¿‡äº†Human Annotation")


if __name__ == "__main__":
    main()
