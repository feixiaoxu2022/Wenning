#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆæµ‹è¯•ï¼šä½¿ç”¨mockæ•°æ®æµ‹è¯•è¯„æµ‹æ¡†æ¶

è·³è¿‡Agentæ‰§è¡Œ,ç›´æ¥ä½¿ç”¨æ„é€ çš„æ‰§è¡Œç»“æœæµ‹è¯•è¯„æµ‹æµç¨‹
"""

import json
import sys
from pathlib import Path
from datetime import datetime

# æ·»åŠ srcåˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from evaluator import EvaluationOrchestrator


def create_mock_execution_result(sample_id: str, model_name: str, file_count: int) -> dict:
    """åˆ›å»ºmockæ‰§è¡Œç»“æœ"""
    files = []

    # æ¨¡æ‹Ÿç”Ÿæˆçš„å›¾æ ‡æ–‡ä»¶
    categories = ["ai", "cloud", "code", "data", "security"]
    formats = ["png", "svg"]

    for i in range(file_count):
        category = categories[i % len(categories)]
        format_type = formats[i % len(formats)]

        file_info = {
            "path": f"{category}/icon_{i+1}.{format_type}",
            "size": 2048 + i * 100,
            "type": format_type
        }

        # æ·»åŠ å›¾ç‰‡å…ƒæ•°æ®
        if format_type in ["png", "jpg"]:
            file_info["metadata"] = {
                "dimensions": {
                    "width": 512,
                    "height": 512
                }
            }

        files.append(file_info)

    return {
        "sample_id": sample_id,
        "model_name": model_name,
        "status": "success",
        "evaluated_at": datetime.now().isoformat(),
        "initial_state": {"files": []},
        "final_state": {"files": files},
        "conversation_history": [
            {
                "role": "user",
                "content": "æœé›†ç§‘æŠ€ç±»å›¾æ ‡",
                "timestamp": datetime.now().isoformat()
            },
            {
                "role": "assistant",
                "content": f"æˆ‘å·²ç»æœé›†äº†{file_count}ä¸ªç§‘æŠ€ç±»å›¾æ ‡ï¼Œè¦†ç›–äº†AIã€äº‘è®¡ç®—ã€ä»£ç ã€æ•°æ®ã€ç½‘ç»œå®‰å…¨ç­‰åœºæ™¯ï¼Œå¹¶æŒ‰ç±»åˆ«ç»„ç»‡åœ¨ä¸åŒæ–‡ä»¶å¤¹ä¸­ã€‚",
                "timestamp": datetime.now().isoformat()
            }
        ]
    }


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("CreativeFlow è¯„æµ‹æ¡†æ¶æµ‹è¯• (ä½¿ç”¨Mockæ•°æ®)")
    print("=" * 80)

    # 1. åŠ è½½æ ·æœ¬
    sample_path = Path("samples/EVAL_ICON_COLLECTION_TECH.json")
    print(f"\nğŸ“‚ åŠ è½½æ ·æœ¬: {sample_path}")

    with open(sample_path, 'r', encoding='utf-8') as f:
        sample = json.load(f)

    sample_id = sample["data_id"]
    model_a = sample["models"]["model_a"]
    model_b = sample["models"]["model_b"]

    print(f"âœ“ æ ·æœ¬ID: {sample_id}")
    print(f"âœ“ Model A: {model_a}")
    print(f"âœ“ Model B: {model_b}")

    # 2. åˆ›å»ºmockæ‰§è¡Œç»“æœ
    print("\n" + "=" * 80)
    print("åˆ›å»ºMockæ‰§è¡Œç»“æœ")
    print("=" * 80)

    # Model A: 15ä¸ªæ–‡ä»¶,è´¨é‡è¾ƒé«˜
    execution_result_a = create_mock_execution_result(sample_id, model_a, 15)
    print(f"âœ“ Model A mock: {len(execution_result_a['final_state']['files'])} ä¸ªæ–‡ä»¶")

    # Model B: 25ä¸ªæ–‡ä»¶,æ•°é‡æ›´å¤š
    execution_result_b = create_mock_execution_result(sample_id, model_b, 25)
    print(f"âœ“ Model B mock: {len(execution_result_b['final_state']['files'])} ä¸ªæ–‡ä»¶")

    # ä¿å­˜mockç»“æœ
    executions_dir = Path("executions")
    executions_dir.mkdir(exist_ok=True)

    execution_a_path = executions_dir / f"{model_a}_{sample_id}_mock.json"
    with open(execution_a_path, 'w', encoding='utf-8') as f:
        json.dump(execution_result_a, f, indent=2, ensure_ascii=False)
    print(f"âœ“ ä¿å­˜è‡³: {execution_a_path}")

    execution_b_path = executions_dir / f"{model_b}_{sample_id}_mock.json"
    with open(execution_b_path, 'w', encoding='utf-8') as f:
        json.dump(execution_result_b, f, indent=2, ensure_ascii=False)
    print(f"âœ“ ä¿å­˜è‡³: {execution_b_path}")

    # 3. è¿è¡Œè¯„æµ‹(ä»…Rule-based,è·³è¿‡LLM Judge)
    print("\n" + "=" * 80)
    print("è¿è¡Œè¯„æµ‹æ¡†æ¶ (ä»…Rule-basedæ£€æŸ¥)")
    print("=" * 80)

    # è¿‡æ»¤æ‰LLM Judgeæ£€æŸ¥é¡¹
    sample_copy = sample.copy()
    sample_copy["check_list"] = [
        check for check in sample["check_list"]
        if check["check_type"] != "llm_judge" and check["check_type"] != "human_annotation"
    ]

    print(f"âœ“ ä¿ç•™ {len(sample_copy['check_list'])} ä¸ªRuleæ£€æŸ¥é¡¹:")
    for check in sample_copy["check_list"]:
        print(f"  - {check['description']}")

    # åˆ›å»ºè¯„æµ‹å™¨(ä¸é…ç½®LLM Judge)
    orchestrator = EvaluationOrchestrator()

    # æ‰§è¡Œè¯„æµ‹
    result_path = Path(f"results/{sample_id}_mock_result.json")
    result_path.parent.mkdir(exist_ok=True)

    eval_result = orchestrator.evaluate(
        sample=sample_copy,
        execution_result_a=execution_result_a,
        execution_result_b=execution_result_b,
        output_file=result_path
    )

    print(f"\nâœ“ è¯„æµ‹ç»“æœå·²ä¿å­˜: {result_path}")

    # 4. è¾“å‡ºç»“æœæ‘˜è¦
    print("\n" + "=" * 80)
    print("è¯„æµ‹ç»“æœæ‘˜è¦")
    print("=" * 80)

    print(f"\nğŸ“Š Model A ({model_a}):")
    print(f"  - æ–‡ä»¶æ•°: {len(execution_result_a['final_state']['files'])}")
    for check_result in eval_result['check_results']['model_a']:
        print(f"  - {check_result['description']}: {check_result['score']:.2f} (é€šè¿‡: {check_result['passed']})")

    print(f"\nğŸ“Š Model B ({model_b}):")
    print(f"  - æ–‡ä»¶æ•°: {len(execution_result_b['final_state']['files'])}")
    for check_result in eval_result['check_results']['model_b']:
        print(f"  - {check_result['description']}: {check_result['score']:.2f} (é€šè¿‡: {check_result['passed']})")

    print("\nâœ… è¯„æµ‹æ¡†æ¶æµ‹è¯•å®Œæˆ!")
    print("\nè¯´æ˜:")
    print("1. ä½¿ç”¨äº†mockæ•°æ®,æ²¡æœ‰å®é™…æ‰§è¡ŒAgent")
    print("2. ä»…æµ‹è¯•äº†Rule-basedæ£€æŸ¥(file_count_range, file_format_check)")
    print("3. è·³è¿‡äº†LLM Judgeå’ŒHuman Annotation")
    print("\nä¸‹ä¸€æ­¥:")
    print("1. ä¿®å¤Agentæ‰§è¡Œé—®é¢˜,ç”ŸæˆçœŸå®æ–‡ä»¶")
    print("2. é…ç½®LLM Judge APIå¯†é’¥(å¦‚éœ€è¦)")
    print("3. è¿è¡Œå®Œæ•´è¯„æµ‹æµç¨‹")


if __name__ == "__main__":
    main()
