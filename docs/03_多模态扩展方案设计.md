# 多模态扩展方案设计

## 文档概述

本文档定义创作Agent的多模态能力扩展方案，使Agent能够"看到"和"理解"用户上传的图片/视频，从而提供更智能的创作服务。

---

## 1. 需求背景

### 1.1 核心问题

当前icon_collection_agent是**纯文本模式**，但创作场景**强依赖视觉理解**：

| 场景 | 需要的视觉能力 | 当前限制 |
|------|---------------|---------|
| **封面生成** | 分析参考图风格、识别背景主体位置 | ❌ 无法理解用户上传的参考图 |
| **批量处理** | 判断图片内容决定裁剪方式 | ❌ 只能按固定规则处理 |
| **视频剪辑** | 识别关键帧、字幕位置、场景切分 | ❌ 需要手动指定时间点 |
| **PPT生成** | 理解内容匹配合适配图 | ❌ 只能关键词搜索，无法验证相关性 |

### 1.2 用户交互场景

**期望体验**:
```
用户: "帮我生成封面，要类似这种风格的" + [上传 reference.jpg]
Agent: "我看到这是科技风格，蓝色调，标题居中。我将使用tech_minimal模板..."

用户: "这张图帮我智能裁剪，保留主体"  + [上传 photo.jpg]
Agent: "识别到主体是人物在左侧，我将以人脸为中心裁剪..."
```

**当前实现**:
```
用户: "帮我生成封面，要类似这种风格的" + [上传 reference.jpg]
Agent: "请描述一下图片的风格特点"  ← 用户需要人工描述
```

---

## 2. 方案对比分析

### 2.1 三种实现方案

#### 方案A: 多模态LLM (End-to-End)

**技术架构**:
```python
LLM: GPT-4V / Claude 3 Opus / Gemini Pro Vision / Qwen-VL

消息格式:
messages = [{
    "role": "user",
    "content": [
        {"type": "text", "text": "分析这张图的风格"},
        {"type": "image_url", "image_url": {"url": "file:///path/to/image.jpg"}}
    ]
}]

Agent: 直接理解图像内容并推理
```

**优势**:
- ✅ 最自然的用户体验
- ✅ 可以做复杂的视觉推理
- ✅ 理解图像与任务上下文的关联

**劣势**:
- ❌ 成本高 (GPT-4V $0.01/图)
- ❌ 延迟高 (单次请求3-10秒)
- ❌ 依赖外部API (需要网络/可能被限流)
- ❌ 当前使用的EBSPV4可能不支持

---

#### 方案B: Vision工具封装 (Tool-Based)

**技术架构**:
```python
Vision服务: 独立部署的视觉分析服务
  - 开源模型: CLIP / BLIP-2 / LLaVA (本地部署)
  - 云API: Azure Computer Vision / Google Vision AI
  - 传统CV: OpenCV + 目标检测模型

工具封装:
def analyze_image(path: str, type: str) -> Dict:
    """
    type:
    - "style": 风格分类 (用CLIP)
    - "layout": 布局分析 (用目标检测)
    - "text": 文字识别 (用OCR)
    - "quality": 质量评估 (用IQA模型)
    """
    pass

Agent: 通过工具获取结构化的分析结果
```

**优势**:
- ✅ 成本可控 (本地模型免费)
- ✅ 延迟低 (1-2秒)
- ✅ 结果结构化 (JSON格式，易于处理)
- ✅ 可定制 (针对特定需求优化模型)

**劣势**:
- ⚠️ 需要额外部署vision服务
- ⚠️ 理解精度可能不如多模态LLM
- ⚠️ 需要设计合理的工具接口

---

#### 方案C: 混合方案 (Hybrid)

**技术架构**:
```python
分阶段调用:
1. 上传阶段: Vision工具快速预分析 (免费/快速)
2. 需求理解: 纯文本交互 (基于预分析结果)
3. 精细设计: 按需调用多模态LLM (付费/慢速，仅复杂场景)

决策逻辑:
if task_complexity == "simple":  # 如"提取主色调"
    use_vision_tool()
elif task_complexity == "complex":  # 如"理解图片情感并推荐音乐"
    use_multimodal_llm()
```

**优势**:
- ✅ 平衡成本与性能
- ✅ 大部分场景用免费工具
- ✅ 关键场景用高精度模型

**劣势**:
- ⚠️ 需要实现智能调度逻辑
- ⚠️ 增加系统复杂度

---

### 2.2 方案选择矩阵

| 维度 | 方案A: 多模态LLM | 方案B: Vision工具 | 方案C: 混合 | 推荐 |
|------|-----------------|------------------|------------|------|
| **成本** | ❌ 高($0.01/图) | ✅ 低(本地免费) | ⚠️ 中 | B/C |
| **延迟** | ❌ 3-10秒 | ✅ 1-2秒 | ⚠️ 1-10秒 | B/C |
| **精度** | ✅ 最高 | ⚠️ 中等 | ✅ 高 | A/C |
| **开发成本** | ✅ 低(API即用) | ❌ 高(需部署) | ❌ 最高 | A |
| **可控性** | ⚠️ 依赖API | ✅ 完全可控 | ✅ 灵活 | B/C |
| **扩展性** | ⚠️ 受API限制 | ✅ 易扩展 | ✅ 易扩展 | B/C |

**推荐决策**:
- **MVP阶段**: 方案B (Vision工具) - 快速验证需求，成本可控
- **V1.0阶段**: 方案C (混合) - 80%用工具，20%用多模态LLM
- **V2.0阶段**: 优化方案C - 根据数据智能调度，持续降低成本

---

## 3. 推荐方案详细设计 (方案B + C)

### 3.1 Vision工具体系设计

#### 工具清单

```python
# ===== 风格分析工具 =====
def analyze_image_style(image_path: str) -> Dict:
    """
    分析图片风格，用于模板匹配

    返回:
    {
        "dominant_colors": ["#2E5090", "#F4F4F4"],  # 主色调(最多5个)
        "color_temperature": "cool",  # 色温: cool/warm/neutral
        "style_tags": ["科技", "简约", "商务"],  # 风格标签
        "mood": "专业、冷静",  # 情绪/氛围
        "composition": "对称",  # 构图: 对称/三分法/中心/自由
        "complexity": "简洁",  # 复杂度: 简洁/中等/复杂
        "recommended_template": "tech_minimal"  # 推荐模板
    }

    实现: CLIP (ViT-B/32) 做zero-shot分类
    """
    pass

# ===== 布局分析工具 =====
def analyze_image_layout(image_path: str) -> Dict:
    """
    分析图片布局，用于文字位置决策

    返回:
    {
        "main_subject_bbox": [200, 100, 800, 600],  # 主体边界框(x1,y1,x2,y2)
        "main_subject_type": "person",  # 主体类型: person/object/text/scene
        "safe_text_regions": [  # 可以放文字的安全区域
            {"area": "top", "coords": [0, 0, 1920, 300], "confidence": 0.9},
            {"area": "bottom", "coords": [0, 800, 1920, 1080], "confidence": 0.95}
        ],
        "background_complexity": "中等",  # 背景复杂度(影响文字对比度要求)
        "has_faces": True,
        "face_locations": [[100, 200, 300, 400]],  # 人脸位置(避免遮挡)
        "text_regions": [[50, 50, 200, 100]]  # 已有文字区域
    }

    实现: YOLOv8 目标检测 + Mediapipe 人脸检测
    """
    pass

# ===== 内容识别工具 =====
def analyze_image_content(image_path: str) -> Dict:
    """
    识别图片内容，用于配图匹配验证

    返回:
    {
        "objects": [  # 识别的物体
            {"name": "laptop", "confidence": 0.92, "bbox": [...]},
            {"name": "coffee", "confidence": 0.85, "bbox": [...]}
        ],
        "scene": "office",  # 场景类型
        "tags": ["工作", "科技", "专业"],  # 语义标签
        "text_detected": "Hello World",  # OCR提取的文字
        "is_screenshot": False,  # 是否截图
        "has_watermark": False  # 是否有水印
    }

    实现: BLIP-2 图像描述 + EasyOCR
    """
    pass

# ===== 质量评估工具 =====
def analyze_image_quality(image_path: str) -> Dict:
    """
    评估图片质量，用于素材筛选

    返回:
    {
        "resolution": {"width": 1920, "height": 1080},
        "file_size": 1024000,  # bytes
        "format": "JPEG",
        "dpi": 72,
        "is_blurry": False,
        "is_overexposed": False,
        "is_underexposed": False,
        "aesthetic_score": 7.5,  # 美学评分(0-10)
        "suitable_for_print": False,
        "suitable_for_web": True
    }

    实现: 传统CV指标 + NIMA美学评估模型
    """
    pass

# ===== 相似度对比工具 =====
def compare_images(image1: str, image2: str) -> Dict:
    """
    对比两张图片的相似度

    返回:
    {
        "overall_similarity": 0.85,  # 整体相似度(0-1)
        "style_similarity": 0.90,  # 风格相似度
        "color_similarity": 0.88,  # 色彩相似度
        "layout_similarity": 0.75,  # 布局相似度
        "is_duplicate": False  # 是否重复图片
    }

    实现: CLIP特征余弦相似度
    """
    pass
```

#### 视频分析工具

```python
# ===== 关键帧提取工具 =====
def extract_video_keyframes(video_path: str, num_frames: int = 5) -> Dict:
    """
    提取视频关键帧，用于缩略图/封面生成

    返回:
    {
        "keyframes": [  # 关键帧列表
            {
                "path": "keyframe_001.jpg",
                "timestamp": 2.5,  # 时间戳(秒)
                "score": 0.92,  # 重要性评分
                "is_good_thumbnail": True  # 是否适合做缩略图
            }
        ],
        "best_thumbnail_index": 0,  # 最佳缩略图索引
        "video_style": {...}  # 整体视频风格(聚合所有帧)
    }

    实现: PySceneDetect + 美学评分模型
    """
    pass

# ===== 场景切分工具 =====
def detect_video_scenes(video_path: str) -> Dict:
    """
    检测视频场景切换点，用于自动剪辑

    返回:
    {
        "scenes": [
            {"start": 0.0, "end": 5.2, "type": "intro"},
            {"start": 5.2, "end": 15.8, "type": "content"},
            {"start": 15.8, "end": 18.0, "type": "outro"}
        ],
        "scene_count": 3,
        "avg_scene_duration": 6.0
    }

    实现: PySceneDetect (基于内容变化)
    """
    pass

# ===== 语音转文字工具 =====
def transcribe_video_audio(video_path: str, language: str = "zh") -> Dict:
    """
    视频语音识别，用于字幕生成

    返回:
    {
        "full_text": "大家好，今天我们来讲...",
        "segments": [
            {"text": "大家好", "start": 0.5, "end": 1.2},
            {"text": "今天我们来讲", "start": 1.5, "end": 3.0}
        ],
        "language": "zh",
        "confidence": 0.95
    }

    实现: Whisper (OpenAI)
    """
    pass

# ===== 静音检测工具 =====
def detect_video_silence(video_path: str, threshold_db: float = -40) -> Dict:
    """
    检测视频静音片段，用于自动剪辑

    返回:
    {
        "silence_segments": [
            {"start": 10.5, "end": 12.3, "duration": 1.8},
            {"start": 25.0, "end": 27.5, "duration": 2.5}
        ],
        "total_silence_duration": 4.3,
        "silence_ratio": 0.15  # 静音占比
    }

    实现: ffmpeg silencedetect
    """
    pass
```

---

### 3.2 工具集成到Workflow

#### 封面生成Workflow集成示例

```python
# === 阶段2: 素材准备 ===

## 策略A: 用户提供参考图 (新增vision分析)

if user_uploaded_reference_image:
    # 1. 快速风格分析
    print("📊 正在分析参考图风格...")
    style_info = analyze_image_style(ref_image)

    # 2. 展示理解结果
    print(f"✅ 已理解参考图:")
    print(f"   风格标签: {', '.join(style_info['style_tags'])}")
    print(f"   主色调: {style_info['dominant_colors']}")
    print(f"   情绪: {style_info['mood']}")
    print(f"   推荐模板: {style_info['recommended_template']}")

    # 3. 询问用户意图
    ask_user("您想要完全相同的风格,还是只是参考?")

    # 4. 基于风格信息搜索相似素材
    if user_says("参考"):
        search_images_with_style_filter(
            keywords=keywords,
            style_filter=style_info['style_tags'],
            color_filter=style_info['dominant_colors']
        )

## 策略B: 智能背景选择 (新增布局分析)

for bg_candidate in downloaded_backgrounds:
    # 分析布局，选择文字安全区域最大的
    layout = analyze_image_layout(bg_candidate)

    # 评分: 安全区域面积 × 置信度
    score = sum(
        region['coords'][2] * region['coords'][3] * region['confidence']
        for region in layout['safe_text_regions']
    )

    if score > best_score:
        best_background = bg_candidate
        best_layout = layout

# 保存布局信息供后续使用
save_metadata({"background_layout": best_layout})
```

#### 批量图片处理Workflow集成示例

```python
# === 阶段2: 智能处理策略 ===

for image in batch_images:
    # 1. 分析图片内容
    content = analyze_image_content(image)
    layout = analyze_image_layout(image)

    # 2. 根据内容智能决策裁剪方式
    if layout['main_subject_type'] == "person":
        # 人物图: 以人脸为中心裁剪
        crop_strategy = "face_centered"
        crop_params = {
            "center": layout['face_locations'][0],
            "aspect_ratio": target_ratio,
            "padding": 0.2  # 留20%边距
        }

    elif layout['main_subject_type'] == "text":
        # 文字图: 保留所有文字区域
        crop_strategy = "text_aware"
        crop_params = {
            "text_regions": layout['text_regions'],
            "min_margin": 50
        }

    elif content['is_screenshot']:
        # 截图: 去除边框/工具栏
        crop_strategy = "smart_screenshot"
        crop_params = detect_screenshot_borders(image)

    else:
        # 通用: 主体居中
        crop_strategy = "subject_centered"
        crop_params = {
            "subject_bbox": layout['main_subject_bbox']
        }

    # 3. 执行裁剪
    cropped = smart_crop(image, crop_strategy, crop_params)
    save_result(cropped)
```

---

### 3.3 多模态LLM按需调用 (混合方案)

#### 复杂度评估与决策

```python
def should_use_multimodal_llm(task: Dict, image: str) -> bool:
    """
    决策是否需要调用多模态LLM

    简单任务用Vision工具 (快速/免费)
    复杂任务用多模态LLM (慢速/付费)
    """

    # 简单任务: Vision工具足够
    simple_tasks = [
        "提取主色调",
        "识别主体位置",
        "OCR文字识别",
        "判断图片模糊度"
    ]
    if task['type'] in simple_tasks:
        return False

    # 复杂任务: 需要多模态LLM
    complex_tasks = [
        "理解图片情感",
        "提供设计建议",
        "创意brainstorming",
        "多图对比推理"
    ]
    if task['type'] in complex_tasks:
        return True

    # 中等任务: 根据上下文决定
    # 如果用户明确要求"详细分析"，使用多模态LLM
    if "详细" in task['user_query'] or "深入" in task['user_query']:
        return True

    # 如果是付费用户，优先使用高精度模型
    if user.is_premium:
        return True

    # 默认使用Vision工具
    return False
```

#### 调用示例

```python
# === 场景1: 简单任务 - 使用Vision工具 ===

user: "这张图主要是什么颜色?"
agent:
    colors = analyze_image_style(image)['dominant_colors']
    reply(f"主要颜色是: {colors}")

# === 场景2: 复杂任务 - 使用多模态LLM ===

user: "这张图给人什么感觉?适合配什么音乐?"
agent:
    if should_use_multimodal_llm(task, image):
        # 调用GPT-4V
        response = multimodal_llm.chat([
            {"role": "user", "content": [
                {"type": "text", "text": "这张图给人什么感觉?适合配什么音乐?"},
                {"type": "image_url", "image_url": {"url": image}}
            ]}
        ])
        reply(response)
    else:
        # 降级到Vision工具
        style = analyze_image_style(image)
        reply(f"根据图片风格({style['mood']}),建议配{recommend_music(style)}")
```

---

### 3.4 Vision服务部署架构

#### 本地部署方案 (推荐MVP)

```
┌────────────────────────────────────────┐
│         CreativeFlow Agent             │
│  (LLM + Function Calling)              │
└───────────┬────────────────────────────┘
            │ HTTP API调用
┌───────────▼────────────────────────────┐
│      Vision Service (FastAPI)          │
│  ┌──────────────────────────────────┐  │
│  │ /analyze_style                   │  │
│  │ /analyze_layout                  │  │
│  │ /analyze_content                 │  │
│  │ /analyze_quality                 │  │
│  └──────────────────────────────────┘  │
│             ↓ 调用                      │
│  ┌──────────────────────────────────┐  │
│  │ 模型层                            │  │
│  │ - CLIP (风格分类)                 │  │
│  │ - YOLOv8 (目标检测)               │  │
│  │ - BLIP-2 (图像描述)               │  │
│  │ - EasyOCR (文字识别)              │  │
│  │ - Mediapipe (人脸检测)            │  │
│  └──────────────────────────────────┘  │
└────────────────────────────────────────┘
```

**代码示例**:
```python
# vision_service.py (FastAPI服务)

from fastapi import FastAPI, UploadFile
from PIL import Image
import torch
from transformers import CLIPProcessor, CLIPModel

app = FastAPI()

# 加载模型(启动时加载一次)
clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

@app.post("/analyze_style")
async def analyze_style(file: UploadFile):
    """分析图片风格"""
    image = Image.open(file.file)

    # CLIP zero-shot分类
    style_labels = ["科技", "文艺", "商务", "可爱", "复古", "现代"]
    inputs = clip_processor(
        text=style_labels,
        images=image,
        return_tensors="pt",
        padding=True
    )

    outputs = clip_model(**inputs)
    probs = outputs.logits_per_image.softmax(dim=1)

    # 提取主色调
    dominant_colors = extract_dominant_colors(image, k=5)

    return {
        "style_tags": [
            style_labels[i] for i, prob in enumerate(probs[0])
            if prob > 0.3
        ],
        "dominant_colors": dominant_colors,
        "mood": infer_mood(style_tags),  # 基于风格标签推断情绪
        ...
    }

@app.post("/analyze_layout")
async def analyze_layout(file: UploadFile):
    """分析图片布局"""
    # YOLOv8目标检测
    # Mediapipe人脸检测
    # 计算安全文字区域
    ...
```

**优势**:
- ✅ 完全免费
- ✅ 数据隐私(本地处理)
- ✅ 延迟低(<1秒)
- ✅ 可离线运行

**成本**:
- GPU推荐: NVIDIA RTX 3060 (12GB VRAM) 或以上
- CPU也可运行，但速度较慢(3-5秒/图)

---

#### 云API方案 (备选)

如果不想自己部署模型，可以使用云服务：

| 服务 | 功能 | 价格 | 推荐度 |
|------|------|------|--------|
| **Azure Computer Vision** | 目标检测、OCR、描述生成 | $1/1000次 | ⭐️⭐️⭐️⭐️ |
| **Google Cloud Vision** | 标签检测、人脸检测、文字识别 | $1.5/1000次 | ⭐️⭐️⭐️ |
| **AWS Rekognition** | 物体检测、人脸分析 | $1/1000次 | ⭐️⭐️⭐️ |
| **Replicate (托管模型)** | CLIP/BLIP-2等开源模型 | $0.0005/次 | ⭐️⭐️⭐️⭐️⭐️ |

**推荐**: Replicate - 便宜且支持多种开源模型

---

### 3.5 用户交互体验设计

#### 上传时自动预处理

```python
def on_user_upload_image(file_path: str):
    """用户上传图片时的处理流程"""

    # 1. 立即反馈
    print(f"📤 正在分析上传的图片...")

    # 2. 快速分析(1-2秒)
    quick_result = analyze_image_style(file_path)

    # 3. 展示理解结果
    print(f"✅ 已理解图片风格:")
    print(f"   风格: {', '.join(quick_result['style_tags'])}")
    print(f"   主色调: {', '.join(quick_result['dominant_colors'])}")
    print(f"   推荐模板: {quick_result['recommended_template']}")

    # 4. 保存元数据(后续直接使用)
    save_image_metadata(file_path, quick_result)

    # 5. 引导下一步
    print(f"\n💡 继续告诉我:")
    print(f"   - '按这个风格生成封面'")
    print(f"   - '找类似风格的背景图'")
    print(f"   - 或者描述你的具体需求")

    return quick_result
```

#### 视觉分析结果的可视化

```python
def visualize_image_analysis(image_path: str, analysis_result: Dict):
    """
    生成分析结果的可视化图片
    在原图上标注检测到的元素
    """
    from PIL import Image, ImageDraw, ImageFont

    img = Image.open(image_path).copy()
    draw = ImageDraw.Draw(img)

    # 绘制主体边界框
    if 'main_subject_bbox' in analysis_result:
        bbox = analysis_result['main_subject_bbox']
        draw.rectangle(bbox, outline="red", width=3)
        draw.text((bbox[0], bbox[1]-20), "主体", fill="red")

    # 绘制人脸位置
    if 'face_locations' in analysis_result:
        for face in analysis_result['face_locations']:
            draw.rectangle(face, outline="green", width=2)

    # 绘制安全文字区域
    if 'safe_text_regions' in analysis_result:
        for region in analysis_result['safe_text_regions']:
            coords = region['coords']
            draw.rectangle(coords, outline="blue", width=2, fill=(0,0,255,50))
            draw.text((coords[0], coords[1]), f"文字安全区(置信度:{region['confidence']})", fill="blue")

    # 保存可视化结果
    output_path = image_path.replace(".jpg", "_analysis.jpg")
    img.save(output_path)

    return output_path
```

**用户体验示例**:
```
用户: [上传 photo.jpg]

Agent:
📤 正在分析上传的图片...

✅ 已理解图片:
   风格: 科技、简约、商务
   主色调: #2E5090, #FFFFFF, #333333
   检测到主体: 笔记本电脑(置信度0.95)
   人脸检测: 无
   文字安全区域: 顶部(300px)、底部(280px)

📊 分析可视化: [photo_analysis.jpg]
   (图中用彩色框标注了主体、安全区域等)

💡 我可以帮你:
   1. 按这个风格生成封面
   2. 智能裁剪(保留主体)
   3. 查找类似风格的素材

请告诉我你的需求~
```

---

## 4. 技术实现细节

### 4.1 CLIP风格分析实现

```python
import torch
from transformers import CLIPProcessor, CLIPModel
from PIL import Image
import numpy as np

class StyleAnalyzer:
    def __init__(self):
        self.model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
        self.processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

        # 预定义风格标签
        self.style_labels = {
            "类型": ["科技", "文艺", "商务", "娱乐", "教育", "生活"],
            "情绪": ["专业", "活泼", "温馨", "严肃", "轻松", "激情"],
            "视觉": ["简约", "复杂", "现代", "复古", "扁平", "立体"],
            "色调": ["冷色调", "暖色调", "黑白", "彩色", "高对比", "柔和"]
        }

    def analyze(self, image_path: str) -> Dict:
        image = Image.open(image_path)

        results = {}
        for category, labels in self.style_labels.items():
            # CLIP zero-shot分类
            inputs = self.processor(
                text=labels,
                images=image,
                return_tensors="pt",
                padding=True
            )

            with torch.no_grad():
                outputs = self.model(**inputs)
                probs = outputs.logits_per_image.softmax(dim=1)[0]

            # 选择概率>阈值的标签
            results[category] = [
                labels[i] for i, prob in enumerate(probs)
                if prob > 0.25
            ]

        # 提取主色调
        dominant_colors = self._extract_colors(image)

        # 推荐模板
        template = self._match_template(results, dominant_colors)

        return {
            "style_tags": results["类型"] + results["视觉"],
            "mood": ", ".join(results["情绪"]),
            "color_temperature": self._infer_temperature(results["色调"]),
            "dominant_colors": dominant_colors,
            "recommended_template": template
        }

    def _extract_colors(self, image: Image, k: int = 5) -> List[str]:
        """提取主色调 (K-means聚类)"""
        from sklearn.cluster import KMeans

        # 缩小图片加速处理
        img = image.resize((150, 150))
        pixels = np.array(img).reshape(-1, 3)

        # K-means聚类
        kmeans = KMeans(n_clusters=k, random_state=42)
        kmeans.fit(pixels)

        # 转换为hex颜色
        colors = [
            f"#{int(c[0]):02x}{int(c[1]):02x}{int(c[2]):02x}"
            for c in kmeans.cluster_centers_
        ]

        return colors

    def _match_template(self, style_results: Dict, colors: List[str]) -> str:
        """根据风格匹配模板"""
        # 简单规则匹配
        if "科技" in style_results["类型"]:
            return "tech_minimal"
        elif "文艺" in style_results["类型"]:
            return "artistic_clean"
        elif "商务" in style_results["类型"]:
            return "professional_dark"
        else:
            return "generic_default"
```

### 4.2 YOLOv8布局分析实现

```python
from ultralytics import YOLO
import cv2

class LayoutAnalyzer:
    def __init__(self):
        self.model = YOLO('yolov8n.pt')  # 使用nano模型(速度快)

    def analyze(self, image_path: str) -> Dict:
        # YOLOv8目标检测
        results = self.model(image_path)

        # 提取主体(面积最大的对象)
        boxes = results[0].boxes
        if len(boxes) > 0:
            areas = [(box.xyxy[0], (box.xyxy[0][2]-box.xyxy[0][0]) * (box.xyxy[0][3]-box.xyxy[0][1]))
                     for box in boxes]
            main_subject_bbox = max(areas, key=lambda x: x[1])[0].tolist()
            main_subject_type = results[0].names[int(boxes[0].cls[0])]
        else:
            # 没检测到对象,默认中心区域
            img = cv2.imread(image_path)
            h, w = img.shape[:2]
            main_subject_bbox = [w*0.25, h*0.25, w*0.75, h*0.75]
            main_subject_type = "unknown"

        # 计算安全文字区域
        safe_regions = self._calculate_safe_text_regions(
            image_path, main_subject_bbox
        )

        # 人脸检测
        face_locations = self._detect_faces(image_path)

        return {
            "main_subject_bbox": main_subject_bbox,
            "main_subject_type": main_subject_type,
            "safe_text_regions": safe_regions,
            "face_locations": face_locations,
            "has_faces": len(face_locations) > 0
        }

    def _calculate_safe_text_regions(self, image_path: str, subject_bbox: List) -> List:
        """计算可以放置文字的安全区域"""
        img = cv2.imread(image_path)
        h, w = img.shape[:2]

        sx1, sy1, sx2, sy2 = subject_bbox

        regions = []

        # 顶部区域
        if sy1 > h * 0.15:  # 主体不在最上方
            regions.append({
                "area": "top",
                "coords": [0, 0, w, int(sy1)],
                "confidence": 0.9
            })

        # 底部区域
        if sy2 < h * 0.85:  # 主体不在最下方
            regions.append({
                "area": "bottom",
                "coords": [0, int(sy2), w, h],
                "confidence": 0.95  # 底部通常更安全
            })

        # 左侧区域
        if sx1 > w * 0.2:
            regions.append({
                "area": "left",
                "coords": [0, 0, int(sx1), h],
                "confidence": 0.7
            })

        # 右侧区域
        if sx2 < w * 0.8:
            regions.append({
                "area": "right",
                "coords": [int(sx2), 0, w, h],
                "confidence": 0.7
            })

        return regions

    def _detect_faces(self, image_path: str) -> List:
        """人脸检测 (使用Mediapipe)"""
        import mediapipe as mp

        mp_face_detection = mp.solutions.face_detection

        img = cv2.imread(image_path)
        with mp_face_detection.FaceDetection(min_detection_confidence=0.5) as face_detection:
            results = face_detection.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))

            if results.detections:
                faces = []
                for detection in results.detections:
                    bbox = detection.location_data.relative_bounding_box
                    h, w, _ = img.shape
                    faces.append([
                        int(bbox.xmin * w),
                        int(bbox.ymin * h),
                        int((bbox.xmin + bbox.width) * w),
                        int((bbox.ymin + bbox.height) * h)
                    ])
                return faces

        return []
```

---

## 5. 成本与性能分析

### 5.1 方案成本对比

#### 方案A: 纯多模态LLM

```
假设: 每天处理1000张图片

GPT-4V:
  - 单价: $0.01/图
  - 日成本: $10
  - 月成本: $300
  - 年成本: $3,650

Gemini Pro Vision:
  - 单价: $0.0025/图
  - 月成本: $75

结论: 成本随用户量线性增长，规模化后很贵
```

#### 方案B: Vision工具 (本地部署)

```
硬件成本:
  - GPU服务器 (RTX 3060): ¥15,000 (一次性)
  - 或云GPU (T4): ¥2/小时 × 720小时/月 = ¥1,440/月

运营成本:
  - 电费: 可忽略
  - 维护: 0

结论: 固定成本，用户增长不增加成本
```

#### 方案C: 混合 (80% Vision工具 + 20% GPT-4V)

```
假设: 每天1000张图，20%用GPT-4V

成本:
  - Vision工具: 800张 × $0 = $0
  - GPT-4V: 200张 × $0.01 = $2/天 = $60/月

结论: 大幅降低成本，保留高精度能力
```

**推荐**: 方案C - 成本与性能最优平衡

---

### 5.2 性能对比

| 指标 | 方案A (GPT-4V) | 方案B (本地) | 方案C (混合) |
|------|---------------|-------------|-------------|
| **延迟** | 3-10秒 | 1-2秒 | 1-10秒 |
| **吞吐量** | 10 QPS | 50 QPS | 30 QPS |
| **精度** | 95% | 85% | 90% |
| **可用性** | 依赖网络 | 99.9% | 95% |

**结论**: 方案B延迟最低，方案C综合最优

---

## 6. 实施路线图

### MVP阶段 (Week 1-2)

**目标**: 验证Vision工具可行性

**交付物**:
1. 部署Vision Service (3个核心API)
   - analyze_style
   - analyze_layout
   - analyze_content

2. 集成到封面生成Workflow
   - 用户上传参考图 → 自动风格分析
   - 背景图选择 → 自动布局分析

3. 测试指标
   - 风格分类准确率 > 80%
   - 布局分析延迟 < 2秒
   - 用户满意度 > 3.5/5

---

### V1.0阶段 (Week 3-4)

**目标**: 完善工具体系，引入混合方案

**新增功能**:
1. 5个完整工具
   - analyze_quality (质量评估)
   - compare_images (相似度对比)
   - extract_keyframes (视频关键帧)

2. 多模态LLM按需调用
   - 实现复杂度评估逻辑
   - 集成GPT-4V (仅复杂场景)

3. 可视化分析结果
   - 生成标注图片
   - HTML预览页面

---

### V2.0阶段 (Week 5+)

**目标**: 优化性能，降低成本

**优化方向**:
1. 模型优化
   - 模型量化(减少显存)
   - 批处理(提升吞吐)
   - 缓存机制(重复图片直接返回)

2. 智能调度
   - 根据用户历史偏好选择模型
   - 根据时间段调整策略(高峰期用本地)

3. 数据驱动改进
   - 收集用户反馈
   - A/B测试不同方案
   - 持续迭代模型

---

## 7. 总结

### 核心结论

1. **推荐方案**: 方案B (Vision工具) + 方案C (混合调用)
   - MVP用纯Vision工具
   - V1.0引入多模态LLM补充

2. **关键优势**:
   - ✅ 成本可控 (本地免费 + 按需付费)
   - ✅ 延迟低 (1-2秒)
   - ✅ 可扩展 (模型可定制优化)

3. **实施路径**:
   ```
   Week 1-2: 部署Vision Service (3个API)
   Week 3-4: 集成混合方案 (+ GPT-4V)
   Week 5+: 优化性能与成本
   ```

### 风险与应对

| 风险 | 影响 | 应对措施 |
|------|------|---------|
| Vision工具精度不足 | 用户体验差 | 引入多模态LLM兜底 |
| 部署成本高 | 增加初期投入 | 先用云API (Replicate) |
| 模型推理慢 | 延迟高 | 模型量化 + GPU加速 |
| 用户数据隐私 | 合规风险 | 本地部署 + 数据加密 |

---

**文档版本**: v1.0
**最后更新**: 2025-11-10
**负责人**: 多模态组
**审核状态**: ✅ 已审核
