# CreativeFlow è¯„æµ‹æ¨¡å¼è®¾è®¡æ–¹æ¡ˆ

## ğŸ“‹ ç›®å½•

- [1. æ¦‚è¿°](#1-æ¦‚è¿°)
- [2. æ ¸å¿ƒç†å¿µ](#2-æ ¸å¿ƒç†å¿µ)
- [3. ç³»ç»Ÿæ¶æ„](#3-ç³»ç»Ÿæ¶æ„)
- [4. ä¸‰å±‚è¯„æµ‹ä½“ç³»](#4-ä¸‰å±‚è¯„æµ‹ä½“ç³»)
- [5. è¯„æµ‹é¢æ¿UIè®¾è®¡](#5-è¯„æµ‹é¢æ¿uiè®¾è®¡)
- [6. æ•°æ®æ¨¡å‹è®¾è®¡](#6-æ•°æ®æ¨¡å‹è®¾è®¡)
- [7. æŠ€æœ¯å®ç°æ–¹æ¡ˆ](#7-æŠ€æœ¯å®ç°æ–¹æ¡ˆ)
- [8. è¯„æµ‹ä»»åŠ¡é…ç½®](#8-è¯„æµ‹ä»»åŠ¡é…ç½®)
- [9. å®æ–½è·¯å¾„](#9-å®æ–½è·¯å¾„)
- [10. å¾…è®¨è®ºé—®é¢˜](#10-å¾…è®¨è®ºé—®é¢˜)

---

## 1. æ¦‚è¿°

### 1.1 ç›®æ ‡

ä¸ºCreativeFlowæ„å»ºä¸€ä¸ªç³»ç»ŸåŒ–çš„è¯„æµ‹æ¨¡å¼ï¼Œç”¨äºï¼š
- **å¯¹æ¯”ä¸åŒæ¨¡å‹**åœ¨åŒä¸€ä»»åŠ¡ä¸Šçš„è¡¨ç°
- **å‘ç°å·¥å…·è®¾è®¡ç¼ºé™·**å’Œä¼˜åŒ–ç‚¹
- **ç§¯ç´¯è¯„æµ‹æ•°æ®**å½¢æˆbenchmark
- **æ¢ç´¢ç³»ç»Ÿè¯„æµ‹æ–¹æ³•è®º**

### 1.2 æ ¸å¿ƒç‰¹æ€§

- âœ… **åŒæ¨¡å‹å¯¹æ¯”** - åŒæ—¶è¿è¡Œä¸¤ä¸ªæ¨¡å‹ï¼Œå¹¶æ’å±•ç¤ºç»“æœ
- âœ… **ä¸‰å±‚è¯„æµ‹ä½“ç³»** - Rule-based + LLM Judge + Human Annotation
- âœ… **äººå·¥å¯å¹²é¢„** - æ‰€æœ‰è‡ªåŠ¨è¯„æµ‹ç»“æœéƒ½å…è®¸äººå·¥ä¿®æ­£
- âœ… **Checklisté©±åŠ¨** - åŸºäºå¯éªŒè¯çš„æ£€æŸ¥ç‚¹è€Œéä¸»è§‚è¯„åˆ†
- âœ… **æ•°æ®å¯æ²‰æ·€** - è¯„æµ‹ç»“æœæŒä¹…åŒ–ï¼Œæ”¯æŒå¯¼å‡ºæŠ¥å‘Š

### 1.3 å…³é”®æŒ‡æ ‡

**æ ¸å¿ƒæŒ‡æ ‡ï¼šç”¨æˆ·æ„å›¾è¾¾æˆåº¦**
- å®šä¹‰ï¼šAgentæ˜¯å¦ç†è§£äº†ç”¨æˆ·æƒ³è¦ä»€ä¹ˆï¼Œå¹¶äº¤ä»˜äº†èƒ½ç”¨çš„ä¸œè¥¿
- è®¡ç®—ï¼šåŸºäºChecklistçš„åŠ æƒå¾—åˆ†

---

## 2. æ ¸å¿ƒç†å¿µ

### 2.1 ä¸ºä»€ä¹ˆä¸ç”¨"æˆåŠŸ/å¤±è´¥"äºŒåˆ†æ³•ï¼Ÿ

åˆ›æ„ç±»ä»»åŠ¡**æ²¡æœ‰æ ‡å‡†ç­”æ¡ˆ**ï¼ŒåŒä¸€ä¸ªä»»åŠ¡å¯èƒ½æœ‰å¤šç§åˆç†çš„å®ç°æ–¹å¼ã€‚

### 2.2 ä¸ºä»€ä¹ˆç”¨Checklistï¼Ÿ

- æŠŠæŠ½è±¡çš„"è´¨é‡"æ‹†è§£æˆ**å¯éªŒè¯çš„æ£€æŸ¥ç‚¹**
- æ¯ä¸ªæ£€æŸ¥ç‚¹éƒ½æœ‰**æ˜ç¡®çš„éªŒè¯æ ‡å‡†**
- æ”¯æŒ**è‡ªåŠ¨åŒ–**ï¼ˆè§„åˆ™æ£€æŸ¥ï¼‰å’Œ**åŠè‡ªåŠ¨åŒ–**ï¼ˆLLMè¾…åŠ©ï¼‰
- å…è®¸**äººå·¥ä¿®æ­£**ï¼Œä¿è¯æœ€ç»ˆå‡†ç¡®æ€§

### 2.3 è¯„æµ‹ä¸‰åŸåˆ™

1. **å®¢è§‚å¯éªŒè¯** - ä¼˜å…ˆä½¿ç”¨è§„åˆ™æ£€æŸ¥ï¼Œå‡å°‘ä¸»è§‚åˆ¤æ–­
2. **åˆ†å±‚è¯„ä¼°** - è‡ªåŠ¨åŒ–å¤„ç†åŸºç¡€é¡¹ï¼Œäººå·¥èšç„¦å…³é”®é¡¹
3. **æŒç»­è¿­ä»£** - è¯„æµ‹ç»“æœåå“ºç³»ç»Ÿä¼˜åŒ–

---

## 3. ç³»ç»Ÿæ¶æ„

### 3.1 æ•´ä½“æµç¨‹

```
ç”¨æˆ·è¿›å…¥è¯„æµ‹æ¨¡å¼
    â†“
é€‰æ‹©è¯„æµ‹ä»»åŠ¡ (ä»ä»»åŠ¡åº“ä¸­é€‰æ‹©)
    â†“
é€‰æ‹©2ä¸ªæ¨¡å‹ (Model A vs Model B)
    â†“
å¹¶è¡Œæ‰§è¡Œä¸¤ä¸ªæ¨¡å‹
    â†“
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ä¸‰å±‚è¯„æµ‹è‡ªåŠ¨è¿è¡Œï¼š
  1. Rule-based è‡ªåŠ¨æ£€æŸ¥
  2. LLM Judge è¯„ä¼°
  3. åˆå§‹åŒ–äººå·¥æ ‡æ³¨è¡¨å•
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    â†“
è¯„æµ‹é¢æ¿å±•ç¤ºå¯¹æ¯”ç»“æœ
    â†“
äººå·¥æ ‡æ³¨ + ä¿®æ­£
    â†“
ç”Ÿæˆæœ€ç»ˆå¾—åˆ†å’ŒæŠ¥å‘Š
    â†“
ä¿å­˜è¯„æµ‹æ•°æ®
```

### 3.2 ç³»ç»Ÿç»„ä»¶

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             Evaluation Frontend                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  ä»»åŠ¡é€‰æ‹©å™¨   â”‚  â”‚    è¯„æµ‹é¢æ¿             â”‚ â”‚
â”‚  â”‚  æ¨¡å‹é€‰æ‹©å™¨   â”‚  â”‚  - æ‰§è¡ŒçŠ¶æ€å±•ç¤º          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  - Checklistå±•ç¤º         â”‚ â”‚
â”‚                    â”‚  - äººå·¥æ ‡æ³¨ç•Œé¢          â”‚ â”‚
â”‚                    â”‚  - ç»“æœå¯¹æ¯”å¯è§†åŒ–        â”‚ â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†• REST API
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Evaluation Backend                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Evaluation Orchestrator                 â”‚  â”‚
â”‚  â”‚  - å¹¶è¡Œæ‰§è¡Œæ¨¡å‹                           â”‚  â”‚
â”‚  â”‚  - è¿è¡Œä¸‰å±‚è¯„æµ‹                           â”‚  â”‚
â”‚  â”‚  - è®¡ç®—æœ€ç»ˆå¾—åˆ†                           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Rule-based  â”‚  â”‚  LLM Judge   â”‚  â”‚ Human  â”‚â”‚
â”‚  â”‚   Checker   â”‚  â”‚   Evaluator  â”‚  â”‚Storage â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚       Task Definition Loader             â”‚  â”‚
â”‚  â”‚  - åŠ è½½è¯„æµ‹ä»»åŠ¡é…ç½®                        â”‚  â”‚
â”‚  â”‚  - è§£æChecklistå®šä¹‰                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†•
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Master Agent                       â”‚
â”‚  æ‰§è¡Œç”¨æˆ·ä»»åŠ¡ï¼Œç”Ÿæˆæ–‡ä»¶                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4. ä¸‰å±‚è¯„æµ‹ä½“ç³»

### 4.1 Layer 1: Rule-based è‡ªåŠ¨è¯„æµ‹

**ç›®æ ‡**ï¼šç”¨ç¡®å®šæ€§è§„åˆ™æ£€æŸ¥å®¢è§‚æŒ‡æ ‡

**é€‚ç”¨åœºæ™¯**ï¼š
- æ–‡ä»¶å­˜åœ¨æ€§æ£€æŸ¥
- æ–‡ä»¶æ ¼å¼éªŒè¯
- æ•°å€¼å‹æŒ‡æ ‡ï¼ˆå°ºå¯¸ã€å¤§å°ã€æ•°é‡ï¼‰
- ç»“æ„åŒ–æ•°æ®å®Œæ•´æ€§æ£€æŸ¥

**å®ç°ç¤ºä¾‹**ï¼š

```python
# è§„åˆ™æ£€æŸ¥å™¨å®šä¹‰
class RuleChecker:
    """åŸºäºè§„åˆ™çš„è‡ªåŠ¨æ£€æŸ¥å™¨"""

    @staticmethod
    def check_file_count(generated_files: List[str], expected_count: int) -> CheckResult:
        """æ£€æŸ¥æ–‡ä»¶æ•°é‡"""
        actual_count = len(generated_files)
        passed = actual_count == expected_count
        score = 1.0 if passed else actual_count / expected_count

        return CheckResult(
            passed=passed,
            score=score,
            details=f"ç”Ÿæˆ {actual_count}/{expected_count} ä¸ªæ–‡ä»¶"
        )

    @staticmethod
    def check_image_size(image_path: str, expected_size: tuple, tolerance: float = 0.1) -> CheckResult:
        """æ£€æŸ¥å›¾ç‰‡å°ºå¯¸ï¼ˆå…è®¸è¯¯å·®ï¼‰"""
        from PIL import Image
        img = Image.open(image_path)
        actual_size = img.size
        expected_w, expected_h = expected_size

        w_diff = abs(actual_size[0] - expected_w) / expected_w
        h_diff = abs(actual_size[1] - expected_h) / expected_h

        passed = w_diff <= tolerance and h_diff <= tolerance
        score = 1.0 if passed else max(0, 1 - max(w_diff, h_diff))

        return CheckResult(
            passed=passed,
            score=score,
            details=f"å®é™…å°ºå¯¸ {actual_size}, é¢„æœŸ {expected_size}"
        )

    @staticmethod
    def check_excel_sheets(excel_path: str, expected_sheets: List[str]) -> CheckResult:
        """æ£€æŸ¥Excelæ˜¯å¦åŒ…å«é¢„æœŸçš„sheet"""
        import pandas as pd
        xl = pd.ExcelFile(excel_path)
        actual_sheets = set(xl.sheet_names)
        expected_set = set(expected_sheets)

        matched = actual_sheets & expected_set
        score = len(matched) / len(expected_set) if expected_set else 1.0

        return CheckResult(
            passed=score == 1.0,
            score=score,
            details=f"åŒ…å« {len(matched)}/{len(expected_set)} ä¸ªé¢„æœŸsheet"
        )

    @staticmethod
    def check_dominant_color(image_path: str, expected_colors: List[str]) -> CheckResult:
        """æ£€æŸ¥å›¾ç‰‡ä¸»è‰²è°ƒ"""
        from PIL import Image
        import numpy as np

        img = Image.open(image_path).convert('RGB')
        pixels = np.array(img)

        # ç®€åŒ–ï¼šå–ä¸­ä½æ•°é¢œè‰²
        median_color = np.median(pixels.reshape(-1, 3), axis=0)

        # é¢œè‰²åˆ¤æ–­é€»è¾‘ï¼ˆç®€åŒ–ï¼‰
        color_name = rgb_to_color_name(median_color)
        passed = color_name in expected_colors

        return CheckResult(
            passed=passed,
            score=1.0 if passed else 0.5,
            details=f"ä¸»è‰²è°ƒ: {color_name}"
        )
```

**ä¼˜ç‚¹**ï¼š
- âœ… å¿«é€Ÿã€ç¨³å®šã€å¯é‡å¤
- âœ… ä¸ä¾èµ–LLMï¼Œæˆæœ¬ä½
- âœ… ç»“æœæ˜ç¡®ï¼Œæ— æ­§ä¹‰

**å±€é™**ï¼š
- âŒ æ— æ³•è¯„ä¼°è¯­ä¹‰å±‚é¢çš„è´¨é‡
- âŒ è§„åˆ™å®šä¹‰éœ€è¦äººå·¥è®¾è®¡
- âŒ éš¾ä»¥å¤„ç†å¼€æ”¾æ€§è¦æ±‚

---

### 4.2 Layer 2: LLM Judge è¯„æµ‹

**ç›®æ ‡**ï¼šç”¨LLMè¯„ä¼°è¯­ä¹‰ã€è´¨é‡ã€ä¸“ä¸šæ€§ç­‰ä¸»è§‚æŒ‡æ ‡

**é€‚ç”¨åœºæ™¯**ï¼š
- å†…å®¹å‡†ç¡®æ€§ï¼ˆå¦‚"æ˜¯å¦å‡†ç¡®å±•ç¤ºäº†åŒæ­¥vså¼‚æ­¥æ¦‚å¿µ"ï¼‰
- è§†è§‰è´¨é‡ï¼ˆå¦‚"å›¾è¡¨æ˜¯å¦ä¸“ä¸š"ï¼‰
- æ–‡æ¡ˆè´¨é‡ï¼ˆå¦‚"æ ‡é¢˜æ˜¯å¦å¸å¼•äºº"ï¼‰
- å®Œæ•´æ€§è¯„ä¼°ï¼ˆå¦‚"æ˜¯å¦åŒ…å«æ‰€æœ‰å…³é”®ä¿¡æ¯ç‚¹"ï¼‰

**å®ç°ç¤ºä¾‹**ï¼š

```python
class LLMJudge:
    """åŸºäºLLMçš„è¯„æµ‹"""

    def __init__(self, llm_client):
        self.llm = llm_client

    async def evaluate_content_accuracy(
        self,
        image_path: str,
        requirement: str
    ) -> CheckResult:
        """è¯„ä¼°å†…å®¹å‡†ç¡®æ€§"""

        # æ„é€ è¯„æµ‹prompt
        prompt = f"""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å†…å®¹è¯„å®¡ä¸“å®¶ã€‚è¯·è¯„ä¼°è¿™å¼ å›¾ç‰‡æ˜¯å¦æ»¡è¶³ä»¥ä¸‹è¦æ±‚ï¼š

ã€è¦æ±‚ã€‘
{requirement}

ã€è¯„åˆ†æ ‡å‡†ã€‘
5åˆ†ï¼šå®Œå…¨ç¬¦åˆè¦æ±‚ï¼Œå†…å®¹å‡†ç¡®æ¸…æ™°
4åˆ†ï¼šåŸºæœ¬ç¬¦åˆè¦æ±‚ï¼Œæœ‰å°ç‘•ç–µ
3åˆ†ï¼šéƒ¨åˆ†ç¬¦åˆè¦æ±‚ï¼Œæœ‰æ˜æ˜¾ä¸è¶³
2åˆ†ï¼šåŸºæœ¬ä¸ç¬¦åˆè¦æ±‚
1åˆ†ï¼šå®Œå…¨ä¸ç¬¦åˆè¦æ±‚

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¿”å›ï¼š
åˆ†æ•°ï¼š<1-5>
ç†ç”±ï¼š<ä¸€å¥è¯è¯´æ˜>
"""

        # è°ƒç”¨LLMï¼ˆéœ€æ”¯æŒå›¾ç‰‡è¾“å…¥ï¼‰
        response = await self.llm.chat_with_image(
            prompt=prompt,
            image_path=image_path
        )

        # è§£æLLMè¿”å›
        score_match = re.search(r'åˆ†æ•°[ï¼š:]\s*(\d)', response)
        reason_match = re.search(r'ç†ç”±[ï¼š:]\s*(.+)', response)

        score = int(score_match.group(1)) if score_match else 3
        reason = reason_match.group(1).strip() if reason_match else response[:100]

        # å½’ä¸€åŒ–åˆ°0-1
        normalized_score = score / 5.0

        return CheckResult(
            passed=score >= 4,
            score=normalized_score,
            details=reason,
            raw_response=response
        )

    async def evaluate_professionalism(
        self,
        file_paths: List[str],
        file_type: str = "image"
    ) -> CheckResult:
        """è¯„ä¼°æ•´ä½“ä¸“ä¸šæ€§"""

        prompt = f"""
è¯·ä»ä¸“ä¸šè§’åº¦è¯„ä¼°è¿™äº›{file_type}çš„è´¨é‡ï¼š

è¯„ä¼°ç»´åº¦ï¼š
1. å¸ƒå±€è®¾è®¡æ˜¯å¦åˆç†
2. é…è‰²æ˜¯å¦ä¸“ä¸šåè°ƒ
3. ä¿¡æ¯å±‚æ¬¡æ˜¯å¦æ¸…æ™°
4. æ•´ä½“è§†è§‰æ•ˆæœ

ç»¼åˆæ‰“åˆ†(1-5åˆ†)ï¼Œå¹¶è¯´æ˜ç†ç”±ã€‚
"""

        # è°ƒç”¨LLMï¼ˆå¤šå›¾ç‰‡è¾“å…¥ï¼‰
        response = await self.llm.chat_with_images(
            prompt=prompt,
            image_paths=file_paths
        )

        # è§£æè¿”å›...
        return CheckResult(...)

    async def evaluate_completeness(
        self,
        generated_content: str,
        requirements: List[str]
    ) -> CheckResult:
        """è¯„ä¼°å†…å®¹å®Œæ•´æ€§"""

        prompt = f"""
è¯·æ£€æŸ¥ä»¥ä¸‹å†…å®¹æ˜¯å¦è¦†ç›–äº†æ‰€æœ‰è¦æ±‚çš„è¦ç‚¹ï¼š

ã€è¦æ±‚çš„è¦ç‚¹ã€‘
{chr(10).join(f"{i+1}. {req}" for i, req in enumerate(requirements))}

ã€å®é™…ç”Ÿæˆçš„å†…å®¹ã€‘
{generated_content}

è¯·é€é¡¹æ£€æŸ¥ï¼Œå¹¶ç»™å‡ºï¼š
- è¦†ç›–ç‡ï¼šX/Y
- ç¼ºå¤±é¡¹ï¼š...
- ç»¼åˆè¯„åˆ†ï¼š1-5åˆ†
"""

        response = await self.llm.chat(prompt)

        # è§£æè¿”å›...
        return CheckResult(...)
```

**LLM Judge Promptè®¾è®¡åŸåˆ™**ï¼š

1. **æ˜ç¡®è¯„åˆ†æ ‡å‡†** - ç»™å‡º1-5åˆ†çš„å…·ä½“å«ä¹‰
2. **æä¾›è¯„ä¼°ç»´åº¦** - åˆ—å‡ºè¦å…³æ³¨çš„æ–¹é¢
3. **è¦æ±‚ç»“æ„åŒ–è¾“å‡º** - åˆ†æ•° + ç†ç”±ï¼Œä¾¿äºè§£æ
4. **ä¿æŒè¯„æµ‹ä¸€è‡´æ€§** - ç›¸åŒçš„promptå¯¹æ‰€æœ‰æ ·æœ¬ä½¿ç”¨

**ä¼˜ç‚¹**ï¼š
- âœ… èƒ½è¯„ä¼°è¯­ä¹‰å’Œè´¨é‡
- âœ… çµæ´»ï¼Œé€‚åº”å¼€æ”¾æ€§è¦æ±‚
- âœ… å¯ä»¥æä¾›è¯„ä¼°ç†ç”±

**å±€é™**ï¼š
- âŒ æœ‰æˆæœ¬ï¼ˆLLMè°ƒç”¨ï¼‰
- âŒ ç»“æœå¯èƒ½ä¸ç¨³å®šï¼ˆéœ€è¦å¤šæ¬¡è¯„æµ‹å–å¹³å‡ï¼‰
- âŒ ä»ç„¶æœ‰ä¸»è§‚æ€§ï¼Œå¯èƒ½ä¸äººç±»åˆ¤æ–­ä¸ä¸€è‡´

---

### 4.3 Layer 3: Human Annotation

**ç›®æ ‡**ï¼šäººå·¥æ ‡æ³¨å…³é”®ç»´åº¦ï¼Œä½œä¸ºæœ€ç»ˆåˆ¤æ–­

**é€‚ç”¨åœºæ™¯**ï¼š
- æ•´ä½“è´¨é‡å¯¹æ¯”ï¼ˆModel A vs Model B å“ªä¸ªæ›´å¥½ï¼‰
- ç”¨æˆ·åå¥½ï¼ˆå¦‚æœä½ æ˜¯ç”¨æˆ·ï¼Œä½ ä¼šé€‰å“ªä¸ªï¼‰
- åˆ›æ„æ€§ã€æƒŠè‰³åº¦ç­‰éš¾ä»¥é‡åŒ–çš„ç»´åº¦
- ä¿®æ­£è‡ªåŠ¨è¯„æµ‹çš„é”™è¯¯

**æ ‡æ³¨ç•Œé¢è®¾è®¡**ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  [äººå·¥æ ‡æ³¨] æ•´ä½“è´¨é‡å¯¹æ¯”                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚  ç»´åº¦1: ä¸“ä¸šæ€§                                  â”‚
â”‚    â—‹ Model A æ›´å¥½                              â”‚
â”‚    â—‹ Model B æ›´å¥½                              â”‚
â”‚    â— ä¸¤è€…ç›¸å½“                                  â”‚
â”‚                                                â”‚
â”‚  ç»´åº¦2: åˆ›æ„æ€§                                  â”‚
â”‚    â— Model A æ›´å¥½                              â”‚
â”‚    â—‹ Model B æ›´å¥½                              â”‚
â”‚    â—‹ ä¸¤è€…ç›¸å½“                                  â”‚
â”‚                                                â”‚
â”‚  ç»´åº¦3: å¯ç”¨æ€§                                  â”‚
â”‚    â—‹ Model A æ›´å¥½                              â”‚
â”‚    â— Model B æ›´å¥½                              â”‚
â”‚    â—‹ ä¸¤è€…ç›¸å½“                                  â”‚
â”‚                                                â”‚
â”‚  ç»´åº¦4: æƒŠè‰³åº¦                                  â”‚
â”‚    â—‹ Model A æ›´å¥½                              â”‚
â”‚    â—‹ Model B æ›´å¥½                              â”‚
â”‚    â— ä¸¤è€…ç›¸å½“                                  â”‚
â”‚                                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ç»¼åˆåˆ¤æ–­ï¼šå¦‚æœä½ æ˜¯ç”¨æˆ·ï¼Œä½ æ›´å€¾å‘äºä½¿ç”¨å“ªä¸ªï¼Ÿ      â”‚
â”‚    â—‹ Model A                                   â”‚
â”‚    â— Model B                                   â”‚
â”‚    â—‹ æ— æ˜æ˜¾åå¥½                                â”‚
â”‚                                                â”‚
â”‚  å¤‡æ³¨ï¼š                                         â”‚
â”‚  [Model Bçš„é…è‰²æ›´ä¸“ä¸šï¼Œå›¾è¡¨å¸ƒå±€æ›´æ¸…æ™°ï¼Œè™½ç„¶      â”‚
â”‚   Model Aåœ¨åˆ›æ„æ€§ä¸Šæœ‰äº®ç‚¹ï¼Œä½†æ•´ä½“å¯ç”¨æ€§Bæ›´å¼º]   â”‚
â”‚                                                â”‚
â”‚  [ä¿å­˜æ ‡æ³¨]                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ ‡æ³¨æ•°æ®æ¨¡å‹**ï¼š

```python
@dataclass
class HumanAnnotation:
    """äººå·¥æ ‡æ³¨ç»“æœ"""

    # å¤šç»´åº¦å¯¹æ¯”
    professionalism: Literal["model_a", "model_b", "tie"]
    creativity: Literal["model_a", "model_b", "tie"]
    usability: Literal["model_a", "model_b", "tie"]
    wow_factor: Literal["model_a", "model_b", "tie"]

    # ç»¼åˆåå¥½
    overall_preference: Literal["model_a", "model_b", "no_preference"]

    # å¤‡æ³¨
    notes: Optional[str] = None

    # å…ƒä¿¡æ¯
    annotated_by: str
    annotated_at: datetime
    time_spent_seconds: Optional[int] = None
```

**è®¡ç®—äººå·¥æ ‡æ³¨å¾—åˆ†**ï¼š

```python
def calculate_human_score(annotation: HumanAnnotation) -> tuple[float, float]:
    """
    è®¡ç®—äººå·¥æ ‡æ³¨çš„å¾—åˆ†

    Returns:
        (model_a_score, model_b_score)
    """
    dimensions = [
        annotation.professionalism,
        annotation.creativity,
        annotation.usability,
        annotation.wow_factor
    ]

    score_a = 0.0
    score_b = 0.0

    # æ¯ä¸ªç»´åº¦ï¼šèƒœå‡ºå¾—1åˆ†ï¼Œå¹³å±€å¾—0.5åˆ†
    for dim in dimensions:
        if dim == "model_a":
            score_a += 1.0
        elif dim == "model_b":
            score_b += 1.0
        else:  # tie
            score_a += 0.5
            score_b += 0.5

    # ç»¼åˆåå¥½åŠ æƒï¼ˆæƒé‡æ›´é«˜ï¼‰
    if annotation.overall_preference == "model_a":
        score_a += 2.0
    elif annotation.overall_preference == "model_b":
        score_b += 2.0
    else:
        score_a += 1.0
        score_b += 1.0

    # å½’ä¸€åŒ–åˆ°0-1
    max_score = 6.0  # 4ä¸ªç»´åº¦ + 2å€æƒé‡çš„ç»¼åˆåå¥½
    score_a /= max_score
    score_b /= max_score

    return score_a, score_b
```

---

### 4.4 ä¸‰å±‚è¯„æµ‹çš„æƒé‡é…ç½®

ä¸åŒå±‚æ¬¡çš„è¯„æµ‹é¡¹æœ‰ä¸åŒçš„æƒé‡ï¼š

```yaml
# æƒé‡é…ç½®ç¤ºä¾‹
weight_config:
  rule_based:
    file_existence: 1.0      # æ–‡ä»¶å¿…é¡»å­˜åœ¨
    file_format: 1.0         # æ ¼å¼å¿…é¡»æ­£ç¡®
    file_count: 1.0          # æ•°é‡å¿…é¡»ç¬¦åˆ
    image_size: 0.5          # å°ºå¯¸å…è®¸ä¸€å®šè¯¯å·®
    color_scheme: 0.5        # é…è‰²ä¸æ˜¯ç¡¬æ€§è¦æ±‚

  llm_judge:
    content_accuracy: 2.0    # å†…å®¹å‡†ç¡®æ€§å¾ˆé‡è¦
    professionalism: 1.5     # ä¸“ä¸šæ€§é‡è¦
    completeness: 1.0        # å®Œæ•´æ€§

  human_annotation:
    overall_quality: 3.0     # äººå·¥åˆ¤æ–­æƒé‡æœ€é«˜
    user_preference: 2.0
```

**æœ€ç»ˆå¾—åˆ†è®¡ç®—**ï¼š

```python
def calculate_final_score(check_results: List[CheckResult], weights: dict) -> float:
    """
    è®¡ç®—åŠ æƒæ€»åˆ†

    score = Î£(check_score * weight) / Î£(weight)
    """
    weighted_sum = 0.0
    total_weight = 0.0

    for result in check_results:
        if result.human_override:
            # å¦‚æœè¢«äººå·¥ä¿®æ­£ï¼Œä½¿ç”¨ä¿®æ­£åçš„åˆ†æ•°
            score = result.human_corrected_score
        else:
            score = result.score

        weight = weights.get(result.check_id, 1.0)
        weighted_sum += score * weight
        total_weight += weight

    return weighted_sum / total_weight if total_weight > 0 else 0.0
```

---

## 5. è¯„æµ‹é¢æ¿UIè®¾è®¡

### 5.1 æ•´ä½“å¸ƒå±€

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ¯ è¯„æµ‹æ¨¡å¼                                     [é€€å‡ºè¯„æµ‹]   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  ğŸ“‹ å½“å‰ä»»åŠ¡ï¼šæŠ€æœ¯æ–‡ç« é…å›¾æ‰¹é‡ç”Ÿæˆå™¨                          â”‚
â”‚  ğŸ¯ éš¾åº¦ï¼šâ­â­â­â­                                            â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚   Model A: GPT-4     â”‚   Model B: Claude-3.5â”‚           â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤           â”‚
â”‚  â”‚  âœ… å®Œæˆ (è€—æ—¶: 45s)  â”‚  âœ… å®Œæˆ (è€—æ—¶: 38s)  â”‚           â”‚
â”‚  â”‚  ğŸ’° Token: 15,234    â”‚  ğŸ’° Token: 12,890    â”‚           â”‚
â”‚  â”‚  ğŸ”„ é‡è¯•æ¬¡æ•°: 0      â”‚  ğŸ”„ é‡è¯•æ¬¡æ•°: 1      â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“Š è¯„æµ‹é¡¹åˆ—è¡¨                                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â–º [è‡ªåŠ¨] æ–‡ä»¶ç”Ÿæˆæ£€æŸ¥                                       â”‚
â”‚    â”œâ”€ æ–‡ä»¶æ•°é‡      A: 5/5 âœ…   B: 5/5 âœ…                    â”‚
â”‚    â”œâ”€ æ–‡ä»¶æ ¼å¼      A: âœ…       B: âœ…                        â”‚
â”‚    â”œâ”€ æ–‡ä»¶å¯è¯»æ€§    A: âœ…       B: âœ…                        â”‚
â”‚    â””â”€ æƒé‡: 1.0                                             â”‚
â”‚        [ä¿®æ­£ A: ___] [ä¿®æ­£ B: ___]                          â”‚
â”‚                                                              â”‚
â”‚  â–º [è‡ªåŠ¨] å›¾ç‰‡è§„æ ¼æ£€æŸ¥                                       â”‚
â”‚    â”œâ”€ å°ºå¯¸ç¬¦åˆ      A: 5/5 âœ…   B: 4/5 âš ï¸                    â”‚
â”‚    â”‚   è¯¦æƒ…: Bçš„async_flow.pngå°ºå¯¸ä¸º1100x800                â”‚
â”‚    â”œâ”€ ä¸»è‰²è°ƒæ£€æµ‹    A: è“è‰² âœ…   B: è“è‰² âœ…                   â”‚
â”‚    â””â”€ æƒé‡: 0.5                                             â”‚
â”‚        [ä¿®æ­£ A: ___] [ä¿®æ­£ B: ___]                          â”‚
â”‚                                                              â”‚
â”‚  â–º [LLM] async_concept.png å†…å®¹å‡†ç¡®æ€§                        â”‚
â”‚    â”œâ”€ A: 4/5 ğŸ“Š                                             â”‚
â”‚    â”‚   ç†ç”±: æ¦‚å¿µå¯¹æ¯”æ¸…æ™°ï¼Œä½†æ–‡å­—ç•¥å°                         â”‚
â”‚    â”œâ”€ B: 5/5 ğŸ“Š                                             â”‚
â”‚    â”‚   ç†ç”±: å®Œç¾å±•ç¤ºåŒæ­¥å¼‚æ­¥å·®å¼‚ï¼Œå›¾æ–‡å¹¶èŒ‚                   â”‚
â”‚    â””â”€ æƒé‡: 2.0                                             â”‚
â”‚        [ä¿®æ­£ A: ___] [ä¿®æ­£ B: ___] [æŸ¥çœ‹åŸå§‹è¯„æµ‹]            â”‚
â”‚                                                              â”‚
â”‚  â–º [LLM] æ•´ä½“ä¸“ä¸šæ€§è¯„ä¼°                                      â”‚
â”‚    â”œâ”€ A: 3/5 ğŸ“Š                                             â”‚
â”‚    â”‚   ç†ç”±: é…è‰²è¾ƒå•è°ƒï¼Œå¸ƒå±€æœ‰æ”¹è¿›ç©ºé—´                       â”‚
â”‚    â”œâ”€ B: 5/5 ğŸ“Š                                             â”‚
â”‚    â”‚   ç†ç”±: é…è‰²ä¸“ä¸šåè°ƒï¼Œå¸ƒå±€æ¸…æ™°ç¾è§‚                       â”‚
â”‚    â””â”€ æƒé‡: 1.5                                             â”‚
â”‚        [ä¿®æ­£ A: ___] [ä¿®æ­£ B: ___]                          â”‚
â”‚                                                              â”‚
â”‚  â–º [äººå·¥] æ•´ä½“è´¨é‡å¯¹æ¯”                                       â”‚
â”‚    â”œâ”€ ä¸“ä¸šæ€§        â—‹ A  â— B  â—‹ å¹³å±€                        â”‚
â”‚    â”œâ”€ åˆ›æ„æ€§        â— A  â—‹ B  â—‹ å¹³å±€                        â”‚
â”‚    â”œâ”€ å¯ç”¨æ€§        â—‹ A  â— B  â—‹ å¹³å±€                        â”‚
â”‚    â”œâ”€ æƒŠè‰³åº¦        â—‹ A  â—‹ B  â— å¹³å±€                        â”‚
â”‚    â””â”€ æƒé‡: 3.0                                             â”‚
â”‚                                                              â”‚
â”‚  â–º [äººå·¥] ç»¼åˆåå¥½                                           â”‚
â”‚    å¦‚æœä½ æ˜¯ç”¨æˆ·ï¼Œä½ æ›´å€¾å‘äºä½¿ç”¨å“ªä¸ªï¼Ÿ                         â”‚
â”‚    â—‹ Model A  â— Model B  â—‹ æ— æ˜æ˜¾åå¥½                       â”‚
â”‚    æƒé‡: 2.0                                                â”‚
â”‚                                                              â”‚
â”‚  ğŸ’¬ è¯„æµ‹å¤‡æ³¨ï¼š                                               â”‚
â”‚    [Model Bæ•´ä½“æ›´ä¸“ä¸šå¯ç”¨ï¼Œè™½ç„¶Aåœ¨åˆ›æ„ä¸Šæœ‰äº®ç‚¹               â”‚
â”‚     ä½†Bçš„è§†è§‰å‘ˆç°å’Œä¿¡æ¯ä¼ è¾¾æ›´æ¸…æ™°æœ‰æ•ˆ]                        â”‚
â”‚                                                              â”‚
â”‚    [ä¿å­˜æ ‡æ³¨]                                               â”‚
â”‚                                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“Š ç»¼åˆå¾—åˆ†                                                 â”‚
â”‚                                                              â”‚
â”‚  Model A (GPT-4)           Model B (Claude-3.5)             â”‚
â”‚  â”â”â”â”â”â”â”â”â”â”â”â”â” 85%         â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 92% ğŸ†          â”‚
â”‚  17.0 / 20.0               18.4 / 20.0                      â”‚
â”‚                                                              â”‚
â”‚  å¾—åˆ†æ˜ç»†ï¼š                                                  â”‚
â”‚  - Rule-based:  A: 2.5/3.0    B: 2.4/3.0                   â”‚
â”‚  - LLM Judge:   A: 7.0/10.5   B: 10.0/10.5                 â”‚
â”‚  - Human:       A: 7.5/13.5   B: 6.0/13.5                  â”‚
â”‚                                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ¬ ç”Ÿæˆæ–‡ä»¶å¯¹æ¯”                                             â”‚
â”‚                                                              â”‚
â”‚  Model A ç”Ÿæˆçš„æ–‡ä»¶ï¼š                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ ğŸ“„ async_concept.png      [é¢„è§ˆ] [ä¸‹è½½]    â”‚            â”‚
â”‚  â”‚ ğŸ“„ async_flow.png         [é¢„è§ˆ] [ä¸‹è½½]    â”‚            â”‚
â”‚  â”‚ ğŸ“„ async_performance.png  [é¢„è§ˆ] [ä¸‹è½½]    â”‚            â”‚
â”‚  â”‚ ğŸ“„ async_architecture.png [é¢„è§ˆ] [ä¸‹è½½]    â”‚            â”‚
â”‚  â”‚ ğŸ“„ async_summary.png      [é¢„è§ˆ] [ä¸‹è½½]    â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                              â”‚
â”‚  Model B ç”Ÿæˆçš„æ–‡ä»¶ï¼š                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ ğŸ“„ async_concept.png      [é¢„è§ˆ] [ä¸‹è½½]    â”‚            â”‚
â”‚  â”‚ ğŸ“„ async_flow.png         [é¢„è§ˆ] [ä¸‹è½½]    â”‚            â”‚
â”‚  â”‚ ğŸ“„ async_performance.png  [é¢„è§ˆ] [ä¸‹è½½]    â”‚            â”‚
â”‚  â”‚ ğŸ“„ async_architecture.png [é¢„è§ˆ] [ä¸‹è½½]    â”‚            â”‚
â”‚  â”‚ ğŸ“„ async_summary.png      [é¢„è§ˆ] [ä¸‹è½½]    â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                              â”‚
â”‚  [å¹¶æ’é¢„è§ˆå¯¹æ¯”]                                              â”‚
â”‚                                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [ğŸ“¥ å¯¼å‡ºè¯„æµ‹æŠ¥å‘Š (Markdown)]  [ğŸ’¾ ä¿å­˜è¯„æµ‹æ•°æ® (JSON)]      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.2 äº¤äº’æµç¨‹

#### æµç¨‹1ï¼šå¯åŠ¨è¯„æµ‹

```
1. ç”¨æˆ·ç‚¹å‡»"è¿›å…¥è¯„æµ‹æ¨¡å¼"
2. å±•ç¤ºä»»åŠ¡åº“ï¼Œç”¨æˆ·é€‰æ‹©è¯„æµ‹ä»»åŠ¡
3. ç”¨æˆ·é€‰æ‹©Model Aå’ŒModel B
4. ç‚¹å‡»"å¼€å§‹è¯„æµ‹"
5. åç«¯å¹¶è¡Œæ‰§è¡Œä¸¤ä¸ªæ¨¡å‹
6. å‰ç«¯æ˜¾ç¤ºæ‰§è¡Œè¿›åº¦ï¼ˆå®æ—¶æµå¼æ›´æ–°ï¼‰
7. æ‰§è¡Œå®Œæˆåï¼Œè‡ªåŠ¨è¿è¡ŒRule-basedå’ŒLLM Judge
8. å±•ç¤ºè¯„æµ‹é¢æ¿ï¼Œç­‰å¾…äººå·¥æ ‡æ³¨
```

#### æµç¨‹2ï¼šäººå·¥æ ‡æ³¨

```
1. ç”¨æˆ·æŸ¥çœ‹ç”Ÿæˆçš„æ–‡ä»¶ï¼ˆç‚¹å‡»é¢„è§ˆï¼‰
2. ç”¨æˆ·æŸ¥çœ‹è‡ªåŠ¨è¯„æµ‹ç»“æœ
3. ç”¨æˆ·å¡«å†™äººå·¥æ ‡æ³¨è¡¨å•ï¼š
   - é€‰æ‹©å„ç»´åº¦çš„èƒœå‡ºæ–¹
   - å¡«å†™ç»¼åˆåå¥½
   - æ·»åŠ å¤‡æ³¨è¯´æ˜
4. ç‚¹å‡»"ä¿å­˜æ ‡æ³¨"
5. ç³»ç»Ÿé‡æ–°è®¡ç®—æœ€ç»ˆå¾—åˆ†
6. æ›´æ–°å¾—åˆ†å±•ç¤º
```

#### æµç¨‹3ï¼šä¿®æ­£è‡ªåŠ¨è¯„æµ‹

```
1. ç”¨æˆ·å‘ç°æŸä¸ªè‡ªåŠ¨è¯„æµ‹ç»“æœä¸å‡†ç¡®
2. ç‚¹å‡»è¯¥æ£€æŸ¥é¡¹çš„"ä¿®æ­£"æŒ‰é’®
3. å¼¹å‡ºä¿®æ­£å¯¹è¯æ¡†ï¼š
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  ä¿®æ­£è¯„æµ‹ç»“æœ                   â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚  æ£€æŸ¥é¡¹ï¼šå›¾ç‰‡å°ºå¯¸æ£€æŸ¥            â”‚
   â”‚  æ¨¡å‹ï¼šModel A                  â”‚
   â”‚                                â”‚
   â”‚  è‡ªåŠ¨è¯„æµ‹ç»“æœï¼š5/5 âœ…           â”‚
   â”‚  ä¿®æ­£ä¸ºï¼š[4] / 5               â”‚
   â”‚                                â”‚
   â”‚  ä¿®æ­£ç†ç”±ï¼š                     â”‚
   â”‚  [æœ‰ä¸€å¼ å›¾ç‰‡å°ºå¯¸å®é™…ä¸ç¬¦åˆ]     â”‚
   â”‚                                â”‚
   â”‚  [ç¡®è®¤ä¿®æ­£]  [å–æ¶ˆ]            â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
4. ç¡®è®¤åï¼Œè¯¥æ£€æŸ¥é¡¹æ ‡è®°ä¸º"äººå·¥ä¿®æ­£"
5. ä½¿ç”¨ä¿®æ­£åçš„åˆ†æ•°é‡æ–°è®¡ç®—æ€»åˆ†
```

#### æµç¨‹4ï¼šå¯¼å‡ºæŠ¥å‘Š

```
1. ç”¨æˆ·ç‚¹å‡»"å¯¼å‡ºè¯„æµ‹æŠ¥å‘Š"
2. ç³»ç»Ÿç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š
3. ä¸‹è½½report.mdæ–‡ä»¶
```

---

## 6. æ•°æ®æ¨¡å‹è®¾è®¡

### 6.1 æ ¸å¿ƒæ•°æ®ç»“æ„

```python
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Literal, Callable
from datetime import datetime
from enum import Enum

# ========== æšä¸¾å®šä¹‰ ==========

class CheckType(Enum):
    """è¯„æµ‹ç±»å‹"""
    RULE = "rule"           # è§„åˆ™æ£€æŸ¥
    LLM_JUDGE = "llm_judge" # LLMè¯„æµ‹
    HUMAN = "human"         # äººå·¥æ ‡æ³¨

class ExecutionStatus(Enum):
    """æ‰§è¡ŒçŠ¶æ€"""
    PENDING = "pending"
    RUNNING = "running"
    SUCCESS = "success"
    FAILED = "failed"
    PARTIAL = "partial"

# ========== è¯„æµ‹ä»»åŠ¡å®šä¹‰ ==========

@dataclass
class CheckItemDefinition:
    """å•ä¸ªè¯„æµ‹é¡¹å®šä¹‰"""
    check_id: str                    # å”¯ä¸€ID
    check_name: str                  # æ˜¾ç¤ºåç§°
    check_type: CheckType            # è¯„æµ‹ç±»å‹
    weight: float                    # æƒé‡
    description: Optional[str] = None # è¯´æ˜

    # Rule-based é…ç½®
    rule_name: Optional[str] = None
    rule_params: Optional[Dict] = None

    # LLM Judge é…ç½®
    llm_prompt: Optional[str] = None
    llm_score_range: Optional[tuple] = None  # (min, max)

    # Human é…ç½®
    human_dimensions: Optional[List[str]] = None
    human_options: Optional[List[str]] = None

@dataclass
class EvaluationTaskDefinition:
    """è¯„æµ‹ä»»åŠ¡å®Œæ•´å®šä¹‰"""
    task_id: str
    task_name: str
    description: str
    difficulty: int                  # 1-5
    prompt: str                      # ç»™Agentçš„ä»»åŠ¡æè¿°
    expected_outputs: List[str]      # é¢„æœŸç”Ÿæˆçš„æ–‡ä»¶åˆ—è¡¨
    checklist: List[CheckItemDefinition]  # è¯„æµ‹é¡¹åˆ—è¡¨
    timeout: int = 180               # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    tags: List[str] = field(default_factory=list)  # æ ‡ç­¾

# ========== æ‰§è¡Œç»“æœ ==========

@dataclass
class ModelExecution:
    """å•ä¸ªæ¨¡å‹çš„æ‰§è¡Œç»“æœ"""
    model_name: str
    conversation_id: str
    status: ExecutionStatus

    start_time: datetime
    end_time: Optional[datetime] = None
    duration_seconds: Optional[float] = None

    generated_files: List[str] = field(default_factory=list)
    output_directory: Optional[str] = None

    token_usage: Optional[int] = None
    retry_count: int = 0

    error_message: Optional[str] = None
    execution_log: Optional[str] = None

# ========== è¯„æµ‹ç»“æœ ==========

@dataclass
class CheckResult:
    """å•é¡¹è¯„æµ‹ç»“æœï¼ˆé’ˆå¯¹å•ä¸ªæ¨¡å‹ï¼‰"""
    check_id: str
    model_name: str

    # è¯„æµ‹ç»“æœ
    score: float                     # 0.0 - 1.0
    passed: bool
    details: Optional[str] = None

    # åŸå§‹æ•°æ®ï¼ˆä¾›è°ƒè¯•ï¼‰
    raw_data: Optional[Dict] = None

    # äººå·¥ä¿®æ­£
    human_override: bool = False
    human_corrected_score: Optional[float] = None
    human_correction_reason: Optional[str] = None
    corrected_by: Optional[str] = None
    corrected_at: Optional[datetime] = None

@dataclass
class HumanAnnotation:
    """äººå·¥æ ‡æ³¨ç»“æœ"""

    # å¤šç»´åº¦å¯¹æ¯”ï¼ˆæ¯ä¸ªç»´åº¦åˆ¤æ–­å“ªä¸ªæ¨¡å‹æ›´å¥½ï¼‰
    professionalism: Literal["model_a", "model_b", "tie"]
    creativity: Literal["model_a", "model_b", "tie"]
    usability: Literal["model_a", "model_b", "tie"]
    wow_factor: Literal["model_a", "model_b", "tie"]

    # ç»¼åˆåå¥½
    overall_preference: Literal["model_a", "model_b", "no_preference"]

    # æ–‡å­—è¯´æ˜
    notes: Optional[str] = None

    # å…ƒä¿¡æ¯
    annotated_by: str
    annotated_at: datetime
    time_spent_seconds: Optional[int] = None

# ========== è¯„æµ‹ä¼šè¯ ==========

@dataclass
class EvaluationSession:
    """å®Œæ•´çš„è¯„æµ‹ä¼šè¯"""
    session_id: str
    task_definition: EvaluationTaskDefinition

    # å¯¹æ¯”çš„ä¸¤ä¸ªæ¨¡å‹
    model_a_name: str
    model_b_name: str

    # æ‰§è¡Œç»“æœ
    execution_a: ModelExecution
    execution_b: ModelExecution

    # å„å±‚è¯„æµ‹ç»“æœ
    check_results_a: List[CheckResult] = field(default_factory=list)
    check_results_b: List[CheckResult] = field(default_factory=list)

    # äººå·¥æ ‡æ³¨
    human_annotation: Optional[HumanAnnotation] = None

    # æœ€ç»ˆå¾—åˆ†
    final_score_a: Optional[float] = None
    final_score_b: Optional[float] = None
    winner: Optional[Literal["model_a", "model_b", "tie"]] = None

    # å…ƒä¿¡æ¯
    created_at: datetime
    created_by: str
    status: Literal["pending", "in_progress", "completed"] = "pending"

    # å¾—åˆ†æ˜ç»†
    score_breakdown_a: Optional[Dict[str, float]] = None  # {check_id: weighted_score}
    score_breakdown_b: Optional[Dict[str, float]] = None
```

### 6.2 æ•°æ®å­˜å‚¨ç»“æ„

```
data/
â”œâ”€â”€ evaluation_tasks/           # è¯„æµ‹ä»»åŠ¡å®šä¹‰
â”‚   â”œâ”€â”€ async_images.yaml
â”‚   â”œâ”€â”€ competitor_analysis.yaml
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ evaluation_sessions/        # è¯„æµ‹ä¼šè¯æ•°æ®
â”‚   â”œâ”€â”€ session_20250115_001.json
â”‚   â”œâ”€â”€ session_20250115_002.json
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ evaluation_reports/         # å¯¼å‡ºçš„æŠ¥å‘Š
â”‚   â”œâ”€â”€ report_20250115_001.md
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ evaluation_cache/           # LLM Judgeç¼“å­˜
    â”œâ”€â”€ llm_judge_cache.json    # é¿å…é‡å¤è°ƒç”¨
    â””â”€â”€ ...
```

---

## 7. æŠ€æœ¯å®ç°æ–¹æ¡ˆ

### 7.1 åç«¯APIè®¾è®¡

#### API 1: è·å–è¯„æµ‹ä»»åŠ¡åˆ—è¡¨

```http
GET /api/evaluation/tasks
```

**Response:**
```json
{
  "tasks": [
    {
      "task_id": "async_images_v1",
      "task_name": "æŠ€æœ¯æ–‡ç« é…å›¾æ‰¹é‡ç”Ÿæˆå™¨",
      "difficulty": 4,
      "tags": ["image_generation", "technical"],
      "check_count": 8
    },
    ...
  ]
}
```

#### API 2: å¯åŠ¨è¯„æµ‹

```http
POST /api/evaluation/sessions
Content-Type: application/json

{
  "task_id": "async_images_v1",
  "model_a": "gpt-4",
  "model_b": "claude-3.5"
}
```

**Response:**
```json
{
  "session_id": "session_20250115_001",
  "status": "in_progress",
  "websocket_url": "ws://localhost:8000/ws/evaluation/session_20250115_001"
}
```

**WebSocketæ¨é€ï¼ˆå®æ—¶è¿›åº¦ï¼‰:**
```json
// æ¨¡å‹æ‰§è¡ŒçŠ¶æ€æ›´æ–°
{
  "type": "execution_status",
  "model": "model_a",
  "status": "running",
  "progress": "æ­£åœ¨ç”Ÿæˆç¬¬3å¼ å›¾ç‰‡..."
}

// æ¨¡å‹æ‰§è¡Œå®Œæˆ
{
  "type": "execution_completed",
  "model": "model_a",
  "duration": 45.2,
  "files": ["async_concept.png", ...]
}

// è‡ªåŠ¨è¯„æµ‹è¿›åº¦
{
  "type": "auto_check_progress",
  "check_id": "file_count",
  "status": "completed"
}

// å…¨éƒ¨å®Œæˆ
{
  "type": "evaluation_ready",
  "session_id": "session_20250115_001"
}
```

#### API 3: è·å–è¯„æµ‹ç»“æœ

```http
GET /api/evaluation/sessions/{session_id}
```

**Response:**
```json
{
  "session_id": "session_20250115_001",
  "task": {...},
  "model_a": "gpt-4",
  "model_b": "claude-3.5",
  "execution_a": {
    "status": "success",
    "duration": 45.2,
    "files": [...],
    "token_usage": 15234
  },
  "execution_b": {...},
  "check_results": [
    {
      "check_id": "file_count",
      "check_name": "æ–‡ä»¶æ•°é‡",
      "check_type": "rule",
      "weight": 1.0,
      "result_a": {
        "score": 1.0,
        "passed": true,
        "details": "ç”Ÿæˆ 5/5 ä¸ªæ–‡ä»¶"
      },
      "result_b": {
        "score": 1.0,
        "passed": true,
        "details": "ç”Ÿæˆ 5/5 ä¸ªæ–‡ä»¶"
      }
    },
    ...
  ],
  "human_annotation": null,  // å¾…æ ‡æ³¨
  "final_score_a": null,
  "final_score_b": null
}
```

#### API 4: æäº¤äººå·¥æ ‡æ³¨

```http
POST /api/evaluation/sessions/{session_id}/annotate
Content-Type: application/json

{
  "professionalism": "model_b",
  "creativity": "model_a",
  "usability": "model_b",
  "wow_factor": "tie",
  "overall_preference": "model_b",
  "notes": "Model Bæ•´ä½“æ›´ä¸“ä¸š..."
}
```

**Response:**
```json
{
  "success": true,
  "final_score_a": 0.85,
  "final_score_b": 0.92,
  "winner": "model_b"
}
```

#### API 5: ä¿®æ­£è¯„æµ‹ç»“æœ

```http
POST /api/evaluation/sessions/{session_id}/corrections
Content-Type: application/json

{
  "check_id": "image_size",
  "model": "model_a",
  "corrected_score": 0.8,
  "reason": "æœ‰ä¸€å¼ å›¾ç‰‡å°ºå¯¸ä¸ç¬¦"
}
```

#### API 6: å¯¼å‡ºæŠ¥å‘Š

```http
GET /api/evaluation/sessions/{session_id}/report?format=markdown
```

**Response:**
```
Content-Type: text/markdown
Content-Disposition: attachment; filename="evaluation_report_20250115_001.md"

# è¯„æµ‹æŠ¥å‘Š
...
```

### 7.2 æ ¸å¿ƒæ¨¡å—å®ç°

#### æ¨¡å—1: Evaluation Orchestrator

```python
# src/evaluation/orchestrator.py

class EvaluationOrchestrator:
    """è¯„æµ‹æµç¨‹ç¼–æ’å™¨"""

    def __init__(self, config, llm_client):
        self.config = config
        self.llm = llm_client
        self.rule_checker = RuleBasedChecker()
        self.llm_judge = LLMJudge(llm_client)

    async def run_evaluation(
        self,
        task_def: EvaluationTaskDefinition,
        model_a: str,
        model_b: str,
        user: str
    ) -> EvaluationSession:
        """è¿è¡Œå®Œæ•´è¯„æµ‹æµç¨‹"""

        session_id = generate_session_id()
        session = EvaluationSession(
            session_id=session_id,
            task_definition=task_def,
            model_a_name=model_a,
            model_b_name=model_b,
            created_by=user,
            created_at=datetime.now()
        )

        # 1. å¹¶è¡Œæ‰§è¡Œä¸¤ä¸ªæ¨¡å‹
        logger.info(f"å¹¶è¡Œæ‰§è¡Œæ¨¡å‹ {model_a} å’Œ {model_b}")
        exec_a, exec_b = await asyncio.gather(
            self._execute_model(model_a, task_def, session_id, "model_a"),
            self._execute_model(model_b, task_def, session_id, "model_b")
        )

        session.execution_a = exec_a
        session.execution_b = exec_b

        # 2. è¿è¡ŒRule-basedæ£€æŸ¥
        logger.info("è¿è¡ŒRule-basedè‡ªåŠ¨æ£€æŸ¥")
        for check_def in task_def.checklist:
            if check_def.check_type == CheckType.RULE:
                result_a = await self.rule_checker.check(
                    check_def, exec_a
                )
                result_b = await self.rule_checker.check(
                    check_def, exec_b
                )
                session.check_results_a.append(result_a)
                session.check_results_b.append(result_b)

        # 3. è¿è¡ŒLLM Judge
        logger.info("è¿è¡ŒLLM Judgeè¯„æµ‹")
        for check_def in task_def.checklist:
            if check_def.check_type == CheckType.LLM_JUDGE:
                result_a = await self.llm_judge.evaluate(
                    check_def, exec_a
                )
                result_b = await self.llm_judge.evaluate(
                    check_def, exec_b
                )
                session.check_results_a.append(result_a)
                session.check_results_b.append(result_b)

        # 4. ä¿å­˜ä¼šè¯ï¼ˆç­‰å¾…äººå·¥æ ‡æ³¨ï¼‰
        session.status = "in_progress"
        await self._save_session(session)

        return session

    async def _execute_model(
        self,
        model_name: str,
        task_def: EvaluationTaskDefinition,
        session_id: str,
        model_label: str
    ) -> ModelExecution:
        """æ‰§è¡Œå•ä¸ªæ¨¡å‹"""

        start_time = datetime.now()

        try:
            # åˆ›å»ºæ–°å¯¹è¯
            conv_id = await self._create_conversation(model_name)

            # å‘é€ä»»åŠ¡
            result = await self._send_task(
                conv_id,
                model_name,
                task_def.prompt,
                timeout=task_def.timeout
            )

            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()

            # æ”¶é›†ç”Ÿæˆçš„æ–‡ä»¶
            generated_files = await self._collect_files(conv_id)

            return ModelExecution(
                model_name=model_name,
                conversation_id=conv_id,
                status=ExecutionStatus.SUCCESS,
                start_time=start_time,
                end_time=end_time,
                duration_seconds=duration,
                generated_files=generated_files,
                output_directory=f"outputs/{conv_id}"
            )

        except Exception as e:
            logger.error(f"æ¨¡å‹æ‰§è¡Œå¤±è´¥: {e}")
            return ModelExecution(
                model_name=model_name,
                conversation_id="",
                status=ExecutionStatus.FAILED,
                start_time=start_time,
                error_message=str(e)
            )

    def apply_human_annotation(
        self,
        session: EvaluationSession,
        annotation: HumanAnnotation
    ):
        """åº”ç”¨äººå·¥æ ‡æ³¨å¹¶è®¡ç®—æœ€ç»ˆå¾—åˆ†"""

        session.human_annotation = annotation

        # è®¡ç®—äººå·¥æ ‡æ³¨çš„å¾—åˆ†
        human_score_a, human_score_b = self._calculate_human_score(annotation)

        # æ·»åŠ äººå·¥æ ‡æ³¨ç»“æœåˆ°check_results
        human_check_result_a = CheckResult(
            check_id="human_annotation",
            model_name=session.model_a_name,
            score=human_score_a,
            passed=True,
            details="äººå·¥æ ‡æ³¨å¾—åˆ†"
        )
        human_check_result_b = CheckResult(
            check_id="human_annotation",
            model_name=session.model_b_name,
            score=human_score_b,
            passed=True,
            details="äººå·¥æ ‡æ³¨å¾—åˆ†"
        )

        # è®¡ç®—æœ€ç»ˆå¾—åˆ†
        session.final_score_a = self._calculate_final_score(
            session.check_results_a + [human_check_result_a],
            session.task_definition.checklist
        )
        session.final_score_b = self._calculate_final_score(
            session.check_results_b + [human_check_result_b],
            session.task_definition.checklist
        )

        # åˆ¤æ–­èƒœè´Ÿ
        if abs(session.final_score_a - session.final_score_b) < 0.05:
            session.winner = "tie"
        elif session.final_score_a > session.final_score_b:
            session.winner = "model_a"
        else:
            session.winner = "model_b"

        session.status = "completed"

    def _calculate_final_score(
        self,
        check_results: List[CheckResult],
        checklist: List[CheckItemDefinition]
    ) -> float:
        """è®¡ç®—åŠ æƒæ€»åˆ†"""

        # æ„å»ºæƒé‡æ˜ å°„
        weights = {c.check_id: c.weight for c in checklist}
        weights["human_annotation"] = 5.0  # äººå·¥æ ‡æ³¨æƒé‡

        weighted_sum = 0.0
        total_weight = 0.0

        for result in check_results:
            # ä½¿ç”¨ä¿®æ­£åçš„åˆ†æ•°ï¼ˆå¦‚æœæœ‰ï¼‰
            score = result.human_corrected_score if result.human_override else result.score
            weight = weights.get(result.check_id, 1.0)

            weighted_sum += score * weight
            total_weight += weight

        return weighted_sum / total_weight if total_weight > 0 else 0.0
```

#### æ¨¡å—2: Rule-based Checker

```python
# src/evaluation/rule_checker.py

class RuleBasedChecker:
    """è§„åˆ™æ£€æŸ¥å™¨"""

    async def check(
        self,
        check_def: CheckItemDefinition,
        execution: ModelExecution
    ) -> CheckResult:
        """æ‰§è¡Œè§„åˆ™æ£€æŸ¥"""

        rule_name = check_def.rule_name
        params = check_def.rule_params or {}

        # æ ¹æ®è§„åˆ™åç§°è°ƒç”¨å¯¹åº”çš„æ£€æŸ¥å‡½æ•°
        if rule_name == "file_count_equals":
            return await self._check_file_count(execution, params)
        elif rule_name == "image_size_check":
            return await self._check_image_size(execution, params)
        elif rule_name == "dominant_color_check":
            return await self._check_dominant_color(execution, params)
        elif rule_name == "excel_sheets_check":
            return await self._check_excel_sheets(execution, params)
        else:
            raise ValueError(f"Unknown rule: {rule_name}")

    async def _check_file_count(
        self,
        execution: ModelExecution,
        params: dict
    ) -> CheckResult:
        """æ£€æŸ¥æ–‡ä»¶æ•°é‡"""
        expected = params.get("expected", 0)
        actual = len(execution.generated_files)

        passed = actual == expected
        score = 1.0 if passed else actual / expected

        return CheckResult(
            check_id="file_count",
            model_name=execution.model_name,
            score=score,
            passed=passed,
            details=f"ç”Ÿæˆ {actual}/{expected} ä¸ªæ–‡ä»¶"
        )

    async def _check_image_size(
        self,
        execution: ModelExecution,
        params: dict
    ) -> CheckResult:
        """æ£€æŸ¥å›¾ç‰‡å°ºå¯¸"""
        from PIL import Image

        expected_width = params.get("width", 1200)
        expected_height = params.get("height", 800)
        tolerance = params.get("tolerance", 0.1)

        image_files = [f for f in execution.generated_files if f.endswith(('.png', '.jpg', '.jpeg'))]

        passed_count = 0
        details_list = []

        for img_file in image_files:
            img_path = Path(execution.output_directory) / img_file
            try:
                img = Image.open(img_path)
                w, h = img.size

                w_diff = abs(w - expected_width) / expected_width
                h_diff = abs(h - expected_height) / expected_height

                if w_diff <= tolerance and h_diff <= tolerance:
                    passed_count += 1
                else:
                    details_list.append(f"{img_file}: {w}x{h} (é¢„æœŸ {expected_width}x{expected_height})")
            except Exception as e:
                details_list.append(f"{img_file}: æ— æ³•è¯»å–")

        score = passed_count / len(image_files) if image_files else 0.0
        passed = score == 1.0

        details = f"{passed_count}/{len(image_files)} å¼ å›¾ç‰‡å°ºå¯¸ç¬¦åˆ"
        if details_list:
            details += f"\nä¸ç¬¦åˆé¡¹: {'; '.join(details_list)}"

        return CheckResult(
            check_id="image_size",
            model_name=execution.model_name,
            score=score,
            passed=passed,
            details=details
        )

    # å…¶ä»–æ£€æŸ¥æ–¹æ³•...
```

#### æ¨¡å—3: LLM Judge

```python
# src/evaluation/llm_judge.py

class LLMJudge:
    """LLMè¯„æµ‹å™¨"""

    def __init__(self, llm_client):
        self.llm = llm_client

    async def evaluate(
        self,
        check_def: CheckItemDefinition,
        execution: ModelExecution
    ) -> CheckResult:
        """æ‰§è¡ŒLLMè¯„æµ‹"""

        prompt = check_def.llm_prompt
        score_range = check_def.llm_score_range or (1, 5)

        # æ ¹æ®è¯„æµ‹å†…å®¹ç±»å‹é€‰æ‹©ä¸åŒçš„è¯„æµ‹æ–¹æ³•
        if "å›¾ç‰‡" in check_def.check_name or "image" in check_def.check_id.lower():
            return await self._evaluate_images(
                check_def, execution, prompt, score_range
            )
        elif "Excel" in check_def.check_name:
            return await self._evaluate_excel(
                check_def, execution, prompt, score_range
            )
        else:
            return await self._evaluate_text(
                check_def, execution, prompt, score_range
            )

    async def _evaluate_images(
        self,
        check_def: CheckItemDefinition,
        execution: ModelExecution,
        prompt: str,
        score_range: tuple
    ) -> CheckResult:
        """è¯„æµ‹å›¾ç‰‡ç±»è¾“å‡º"""

        image_files = [f for f in execution.generated_files if f.endswith(('.png', '.jpg', '.jpeg'))]

        if not image_files:
            return CheckResult(
                check_id=check_def.check_id,
                model_name=execution.model_name,
                score=0.0,
                passed=False,
                details="æœªç”Ÿæˆå›¾ç‰‡æ–‡ä»¶"
            )

        # è°ƒç”¨LLMï¼ˆæ”¯æŒå›¾ç‰‡è¾“å…¥ï¼‰
        image_paths = [Path(execution.output_directory) / f for f in image_files]

        response = await self.llm.chat_with_images(
            prompt=prompt,
            image_paths=image_paths
        )

        # è§£æLLMè¿”å›çš„åˆ†æ•°å’Œç†ç”±
        score, reason = self._parse_llm_response(response, score_range)

        # å½’ä¸€åŒ–åˆ°0-1
        normalized_score = (score - score_range[0]) / (score_range[1] - score_range[0])

        return CheckResult(
            check_id=check_def.check_id,
            model_name=execution.model_name,
            score=normalized_score,
            passed=score >= (score_range[0] + score_range[1]) / 2,
            details=reason,
            raw_data={"llm_response": response, "raw_score": score}
        )

    def _parse_llm_response(self, response: str, score_range: tuple) -> tuple[int, str]:
        """è§£æLLMè¿”å›çš„è¯„åˆ†"""

        # æå–åˆ†æ•°
        score_pattern = r'åˆ†æ•°[ï¼š:]\s*(\d+)'
        score_match = re.search(score_pattern, response)
        score = int(score_match.group(1)) if score_match else score_range[0]

        # æå–ç†ç”±
        reason_pattern = r'ç†ç”±[ï¼š:]\s*(.+?)(?:\n|$)'
        reason_match = re.search(reason_pattern, response, re.DOTALL)
        reason = reason_match.group(1).strip() if reason_match else response[:200]

        return score, reason
```

### 7.3 å‰ç«¯å®ç°è¦ç‚¹

#### Reactç»„ä»¶ç»“æ„

```
EvaluationMode/
â”œâ”€â”€ TaskSelector.tsx           # ä»»åŠ¡é€‰æ‹©å™¨
â”œâ”€â”€ ModelSelector.tsx          # æ¨¡å‹é€‰æ‹©å™¨
â”œâ”€â”€ EvaluationPanel.tsx        # ä¸»è¯„æµ‹é¢æ¿
â”‚   â”œâ”€â”€ ExecutionStatus.tsx    # æ‰§è¡ŒçŠ¶æ€å±•ç¤º
â”‚   â”œâ”€â”€ ChecklistTable.tsx     # è¯„æµ‹é¡¹åˆ—è¡¨
â”‚   â”‚   â”œâ”€â”€ RuleCheckRow.tsx   # è§„åˆ™æ£€æŸ¥è¡Œ
â”‚   â”‚   â”œâ”€â”€ LLMJudgeRow.tsx    # LLMè¯„æµ‹è¡Œ
â”‚   â”‚   â””â”€â”€ HumanAnnotationRow.tsx  # äººå·¥æ ‡æ³¨è¡Œ
â”‚   â”œâ”€â”€ FileComparison.tsx     # æ–‡ä»¶å¯¹æ¯”
â”‚   â”œâ”€â”€ ScoreSummary.tsx       # å¾—åˆ†æ±‡æ€»
â”‚   â””â”€â”€ AnnotationForm.tsx     # æ ‡æ³¨è¡¨å•
â””â”€â”€ ReportExporter.tsx         # æŠ¥å‘Šå¯¼å‡º
```

#### WebSocketå®æ—¶æ›´æ–°

```typescript
// useEvaluationWebSocket.ts

export function useEvaluationWebSocket(sessionId: string) {
  const [executionStatus, setExecutionStatus] = useState({});
  const [checkProgress, setCheckProgress] = useState([]);

  useEffect(() => {
    const ws = new WebSocket(`ws://localhost:8000/ws/evaluation/${sessionId}`);

    ws.onmessage = (event) => {
      const data = JSON.parse(event.data);

      switch (data.type) {
        case 'execution_status':
          setExecutionStatus(prev => ({
            ...prev,
            [data.model]: data
          }));
          break;

        case 'auto_check_progress':
          setCheckProgress(prev => [...prev, data]);
          break;

        case 'evaluation_ready':
          // è¯„æµ‹å®Œæˆï¼Œåˆ·æ–°æ•°æ®
          fetchEvaluationResults(sessionId);
          break;
      }
    };

    return () => ws.close();
  }, [sessionId]);

  return { executionStatus, checkProgress };
}
```

---

## 8. è¯„æµ‹ä»»åŠ¡é…ç½®

### 8.1 é…ç½®æ–‡ä»¶æ ¼å¼ï¼ˆYAMLï¼‰

```yaml
# tasks/eval_tasks/async_images.yaml

task_id: "async_images_v1"
task_name: "æŠ€æœ¯æ–‡ç« é…å›¾æ‰¹é‡ç”Ÿæˆå™¨"
description: "ä¸ºPythonå¼‚æ­¥ç¼–ç¨‹ä¸»é¢˜ç”Ÿæˆ5å¼ é…å›¾"
difficulty: 4
tags: ["image_generation", "technical", "visualization"]

prompt: |
  ä¸ºä¸€ç¯‡å…³äº"Pythonå¼‚æ­¥ç¼–ç¨‹"çš„æŠ€æœ¯åšå®¢ç”Ÿæˆ5å¼ é…å›¾:

  1. async_concept.png - åŒæ­¥vså¼‚æ­¥æ¦‚å¿µå¯¹æ¯”å›¾(ç”¨ç®€å•å›¾å½¢å¯¹æ¯”ä¸¤ç§æ¨¡å¼)
  2. async_flow.png - asyncioäº‹ä»¶å¾ªç¯æµç¨‹å›¾(å±•ç¤ºäº‹ä»¶å¾ªç¯çš„å·¥ä½œåŸç†)
  3. async_performance.png - æ€§èƒ½å¯¹æ¯”æŸ±çŠ¶å›¾(æ¨¡æ‹Ÿæ•°æ®:åŒæ­¥è€—æ—¶10s,å¼‚æ­¥è€—æ—¶2s)
  4. async_architecture.png - åç¨‹æ¶æ„ç¤ºæ„å›¾(å±•ç¤ºåç¨‹ã€ä»»åŠ¡ã€äº‹ä»¶å¾ªç¯çš„å…³ç³»)
  5. async_summary.png - çŸ¥è¯†ç‚¹æ€»ç»“å¡ç‰‡(åˆ—å‡º3-5ä¸ªæ ¸å¿ƒè¦ç‚¹)

  è¦æ±‚:
  - æ‰€æœ‰å›¾ç‰‡å°ºå¯¸1200x800
  - ä½¿ç”¨ä¸“ä¸šçš„é…è‰²æ–¹æ¡ˆ(è“è‰²ç³»ä¸ºä¸»)
  - æ–‡å­—æ¸…æ™°æ˜“è¯»
  - é£æ ¼ç»Ÿä¸€

expected_outputs:
  - async_concept.png
  - async_flow.png
  - async_performance.png
  - async_architecture.png
  - async_summary.png

timeout: 150

checklist:
  # ========== Rule-based æ£€æŸ¥ ==========

  - check_id: file_count
    check_name: "æ–‡ä»¶æ•°é‡"
    check_type: rule
    weight: 1.0
    description: "æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†é¢„æœŸæ•°é‡çš„æ–‡ä»¶"
    rule_name: file_count_equals
    rule_params:
      expected: 5

  - check_id: file_format
    check_name: "æ–‡ä»¶æ ¼å¼"
    check_type: rule
    weight: 1.0
    description: "æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºæœ‰æ•ˆçš„å›¾ç‰‡æ ¼å¼"
    rule_name: file_format_check
    rule_params:
      expected_formats: ["png"]

  - check_id: file_readable
    check_name: "æ–‡ä»¶å¯è¯»æ€§"
    check_type: rule
    weight: 0.5
    description: "æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶æ˜¯å¦å¯ä»¥æ­£å¸¸æ‰“å¼€"
    rule_name: image_readable_check

  - check_id: image_size
    check_name: "å›¾ç‰‡å°ºå¯¸"
    check_type: rule
    weight: 0.5
    description: "æ£€æŸ¥å›¾ç‰‡å°ºå¯¸æ˜¯å¦ç¬¦åˆè¦æ±‚"
    rule_name: image_size_check
    rule_params:
      width: 1200
      height: 800
      tolerance: 0.1  # å…è®¸10%è¯¯å·®

  - check_id: color_scheme
    check_name: "é…è‰²æ£€æŸ¥"
    check_type: rule
    weight: 0.5
    description: "æ£€æŸ¥å›¾ç‰‡ä¸»è‰²è°ƒæ˜¯å¦ä¸ºè“è‰²ç³»"
    rule_name: dominant_color_check
    rule_params:
      expected_colors: ["blue", "lightblue", "darkblue", "skyblue"]

  # ========== LLM Judge æ£€æŸ¥ ==========

  - check_id: concept_accuracy
    check_name: "æ¦‚å¿µå‡†ç¡®æ€§ (async_concept.png)"
    check_type: llm_judge
    weight: 2.0
    description: "è¯„ä¼°æ¦‚å¿µå¯¹æ¯”å›¾æ˜¯å¦å‡†ç¡®å±•ç¤ºåŒæ­¥vså¼‚æ­¥"
    llm_prompt: |
      è¯·è¯„ä¼°async_concept.pngè¿™å¼ å›¾ç‰‡æ˜¯å¦å‡†ç¡®å±•ç¤ºäº†"åŒæ­¥vså¼‚æ­¥"çš„æ¦‚å¿µå¯¹æ¯”ã€‚

      è¯„åˆ†æ ‡å‡†(1-5åˆ†):
      5åˆ†: æ¦‚å¿µæ¸…æ™°å‡†ç¡®ï¼Œå¯¹æ¯”æ˜æ˜¾ï¼Œæ˜“äºç†è§£
      4åˆ†: åŸºæœ¬å‡†ç¡®ï¼Œå¯¹æ¯”æ¸…æ™°ï¼Œæœ‰å°ç‘•ç–µ
      3åˆ†: æ¦‚å¿µåŸºæœ¬æ­£ç¡®ï¼Œä½†è¡¨è¾¾ä¸å¤Ÿæ¸…æ™°
      2åˆ†: æ¦‚å¿µæœ‰è¯¯æˆ–å¯¹æ¯”ä¸æ˜æ˜¾
      1åˆ†: å®Œå…¨é”™è¯¯æˆ–æ— å…³å†…å®¹

      è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¿”å›:
      åˆ†æ•°: <1-5>
      ç†ç”±: <ä¸€å¥è¯è¯´æ˜>
    llm_score_range: [1, 5]

  - check_id: flow_diagram_quality
    check_name: "æµç¨‹å›¾è´¨é‡ (async_flow.png)"
    check_type: llm_judge
    weight: 2.0
    description: "è¯„ä¼°äº‹ä»¶å¾ªç¯æµç¨‹å›¾çš„ä¸“ä¸šæ€§å’Œæ¸…æ™°åº¦"
    llm_prompt: |
      è¯·è¯„ä¼°async_flow.pngå±•ç¤ºasyncioäº‹ä»¶å¾ªç¯æµç¨‹çš„è´¨é‡ã€‚

      è¯„ä¼°ç»´åº¦:
      - æµç¨‹é€»è¾‘æ˜¯å¦æ­£ç¡®
      - å›¾å½¢è¡¨è¾¾æ˜¯å¦æ¸…æ™°
      - æ˜¯å¦åŒ…å«å…³é”®è¦ç´ (äº‹ä»¶å¾ªç¯ã€ä»»åŠ¡é˜Ÿåˆ—ç­‰)

      ç»¼åˆæ‰“åˆ†(1-5åˆ†)å¹¶è¯´æ˜ç†ç”±ã€‚
    llm_score_range: [1, 5]

  - check_id: chart_professionalism
    check_name: "å›¾è¡¨ä¸“ä¸šæ€§ (async_performance.png)"
    check_type: llm_judge
    weight: 1.5
    description: "è¯„ä¼°æ€§èƒ½å¯¹æ¯”å›¾è¡¨çš„ä¸“ä¸šç¨‹åº¦"
    llm_prompt: |
      è¯·è¯„ä¼°async_performance.pngè¿™ä¸ªæ€§èƒ½å¯¹æ¯”å›¾è¡¨çš„ä¸“ä¸šæ€§ã€‚

      å…³æ³¨ç‚¹:
      - æ˜¯å¦ä¸ºæŸ±çŠ¶å›¾ç±»å‹
      - åæ ‡è½´æ ‡ç­¾æ˜¯å¦æ¸…æ™°
      - æ•°å€¼æ ‡æ³¨æ˜¯å¦å®Œæ•´
      - æ•´ä½“è§†è§‰æ•ˆæœ

      æ‰“åˆ†(1-5åˆ†)ã€‚
    llm_score_range: [1, 5]

  - check_id: overall_professionalism
    check_name: "æ•´ä½“ä¸“ä¸šæ€§"
    check_type: llm_judge
    weight: 1.5
    description: "è¯„ä¼°æ‰€æœ‰å›¾ç‰‡çš„æ•´ä½“ä¸“ä¸šç¨‹åº¦"
    llm_prompt: |
      è¯·ä»ä¸“ä¸šè§’åº¦è¯„ä¼°è¿™5å¼ å›¾ç‰‡çš„æ•´ä½“è´¨é‡:

      è¯„ä¼°ç»´åº¦:
      1. å¸ƒå±€è®¾è®¡æ˜¯å¦åˆç†
      2. é…è‰²æ˜¯å¦ä¸“ä¸šåè°ƒ
      3. é£æ ¼æ˜¯å¦ç»Ÿä¸€
      4. ä¿¡æ¯å±‚æ¬¡æ˜¯å¦æ¸…æ™°
      5. æ•´ä½“è§†è§‰æ•ˆæœ

      ç»¼åˆæ‰“åˆ†(1-5åˆ†)ï¼Œå¹¶è¯´æ˜ç†ç”±ã€‚
    llm_score_range: [1, 5]

  - check_id: text_readability
    check_name: "æ–‡å­—æ¸…æ™°åº¦"
    check_type: llm_judge
    weight: 1.0
    description: "è¯„ä¼°å›¾ç‰‡ä¸­æ–‡å­—çš„æ¸…æ™°åº¦å’Œæ˜“è¯»æ€§"
    llm_prompt: |
      è¯·è¯„ä¼°æ‰€æœ‰å›¾ç‰‡ä¸­çš„æ–‡å­—æ˜¯å¦æ¸…æ™°æ˜“è¯»ã€‚

      æ£€æŸ¥:
      - å­—å·æ˜¯å¦åˆé€‚
      - å­—ä½“æ˜¯å¦æ¸…æ™°
      - é¢œè‰²å¯¹æ¯”åº¦æ˜¯å¦è¶³å¤Ÿ
      - æ’ç‰ˆæ˜¯å¦æ•´é½

      æ‰“åˆ†(1-5åˆ†)ã€‚
    llm_score_range: [1, 5]

  # ========== Human Annotation ==========

  - check_id: human_overall
    check_name: "æ•´ä½“è´¨é‡å¯¹æ¯”"
    check_type: human
    weight: 3.0
    description: "äººå·¥æ ‡æ³¨æ•´ä½“è´¨é‡"
    human_dimensions:
      - professionalism   # ä¸“ä¸šæ€§
      - creativity        # åˆ›æ„æ€§
      - usability         # å¯ç”¨æ€§
      - wow_factor        # æƒŠè‰³åº¦
    human_options:
      - model_a
      - model_b
      - tie

  - check_id: human_preference
    check_name: "ç»¼åˆåå¥½"
    check_type: human
    weight: 2.0
    description: "å¦‚æœä½ æ˜¯ç”¨æˆ·ï¼Œä½ æ›´å€¾å‘äºä½¿ç”¨å“ªä¸ªæ¨¡å‹çš„è¾“å‡º?"
    human_options:
      - model_a
      - model_b
      - no_preference
```

### 8.2 é…ç½®æ–‡ä»¶åŠ è½½å™¨

```python
# src/evaluation/task_loader.py

import yaml
from pathlib import Path
from typing import List

class EvaluationTaskLoader:
    """è¯„æµ‹ä»»åŠ¡åŠ è½½å™¨"""

    def __init__(self, tasks_dir: Path = Path("tasks/eval_tasks")):
        self.tasks_dir = tasks_dir

    def load_task(self, task_id: str) -> EvaluationTaskDefinition:
        """åŠ è½½å•ä¸ªä»»åŠ¡"""

        task_file = self.tasks_dir / f"{task_id}.yaml"
        if not task_file.exists():
            raise FileNotFoundError(f"Task file not found: {task_file}")

        with open(task_file, 'r', encoding='utf-8') as f:
            data = yaml.safe_load(f)

        return self._parse_task_definition(data)

    def load_all_tasks(self) -> List[EvaluationTaskDefinition]:
        """åŠ è½½æ‰€æœ‰ä»»åŠ¡"""

        tasks = []
        for task_file in self.tasks_dir.glob("*.yaml"):
            with open(task_file, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)
            tasks.append(self._parse_task_definition(data))

        return tasks

    def _parse_task_definition(self, data: dict) -> EvaluationTaskDefinition:
        """è§£æä»»åŠ¡å®šä¹‰"""

        checklist = []
        for check_data in data.get("checklist", []):
            check_type_str = check_data["check_type"]
            check_type = CheckType[check_type_str.upper().replace("-", "_")]

            check_item = CheckItemDefinition(
                check_id=check_data["check_id"],
                check_name=check_data["check_name"],
                check_type=check_type,
                weight=check_data["weight"],
                description=check_data.get("description"),
                rule_name=check_data.get("rule_name"),
                rule_params=check_data.get("rule_params"),
                llm_prompt=check_data.get("llm_prompt"),
                llm_score_range=tuple(check_data["llm_score_range"]) if check_data.get("llm_score_range") else None,
                human_dimensions=check_data.get("human_dimensions"),
                human_options=check_data.get("human_options")
            )
            checklist.append(check_item)

        return EvaluationTaskDefinition(
            task_id=data["task_id"],
            task_name=data["task_name"],
            description=data["description"],
            difficulty=data["difficulty"],
            prompt=data["prompt"],
            expected_outputs=data["expected_outputs"],
            checklist=checklist,
            timeout=data.get("timeout", 180),
            tags=data.get("tags", [])
        )
```

---

## 9. å®æ–½è·¯å¾„

### 9.1 MVPé˜¶æ®µï¼ˆ1-2å¤©ï¼‰

**ç›®æ ‡**: éªŒè¯å¯è¡Œæ€§ï¼Œè·‘é€šåŸºæœ¬æµç¨‹

**èŒƒå›´**:
- âœ… 1ä¸ªè¯„æµ‹ä»»åŠ¡ï¼ˆasync_imagesï¼‰
- âœ… åªå®ç°Rule-basedè‡ªåŠ¨æ£€æŸ¥ï¼ˆ3-5ä¸ªè§„åˆ™ï¼‰
- âœ… ç®€å•çš„å¯¹æ¯”å±•ç¤ºé¡µé¢ï¼ˆæ— WebSocketï¼‰
- âœ… æ‰‹åŠ¨è§¦å‘è¯„æµ‹ï¼ŒæŸ¥çœ‹ç»“æœ

**äº¤ä»˜ç‰©**:
- [ ] åç«¯API: `/evaluation/sessions` (POST/GET)
- [ ] Rule-based Checkerå®ç°
- [ ] ç®€å•çš„å‰ç«¯å¯¹æ¯”é¡µé¢
- [ ] 1ä¸ªYAMLé…ç½®æ–‡ä»¶ç¤ºä¾‹

**éªŒè¯ç‚¹**:
- èƒ½å¦å¹¶è¡Œæ‰§è¡Œä¸¤ä¸ªæ¨¡å‹
- Rule-basedæ£€æŸ¥æ˜¯å¦å‡†ç¡®
- å¯¹æ¯”å±•ç¤ºæ˜¯å¦æ¸…æ™°

### 9.2 åŠŸèƒ½å®Œå–„é˜¶æ®µï¼ˆ1å‘¨ï¼‰

**ç›®æ ‡**: è¡¥é½ä¸‰å±‚è¯„æµ‹ä½“ç³»

**èŒƒå›´**:
- âœ… åŠ å…¥LLM Judgeï¼ˆ2-3ä¸ªè¯„æµ‹é¡¹ï¼‰
- âœ… åŠ å…¥äººå·¥æ ‡æ³¨ç•Œé¢
- âœ… å®ç°å¾—åˆ†è®¡ç®—å’ŒæŠ¥å‘Šå¯¼å‡º
- âœ… æ‰©å±•åˆ°5ä¸ªè¯„æµ‹ä»»åŠ¡

**äº¤ä»˜ç‰©**:
- [ ] LLM Judgeæ¨¡å—
- [ ] äººå·¥æ ‡æ³¨è¡¨å•ç»„ä»¶
- [ ] æŠ¥å‘Šç”Ÿæˆå™¨ï¼ˆMarkdownï¼‰
- [ ] 5ä¸ªå®Œæ•´çš„è¯„æµ‹ä»»åŠ¡é…ç½®

**éªŒè¯ç‚¹**:
- LLM Judgeæ˜¯å¦ç¨³å®šå¯é 
- äººå·¥æ ‡æ³¨æ˜¯å¦æ–¹ä¾¿å¿«æ·
- å¾—åˆ†è®¡ç®—æ˜¯å¦åˆç†

### 9.3 ä½“éªŒä¼˜åŒ–é˜¶æ®µï¼ˆæŒç»­ï¼‰

**ç›®æ ‡**: æå‡æ˜“ç”¨æ€§å’Œå‡†ç¡®æ€§

**ä¼˜åŒ–æ–¹å‘**:
- ğŸ¨ UI/UXä¼˜åŒ–ï¼ˆWebSocketå®æ—¶æ›´æ–°ã€åŠ¨ç”»æ•ˆæœï¼‰
- ğŸ“Š æ•°æ®å¯è§†åŒ–å¢å¼ºï¼ˆå¾—åˆ†é›·è¾¾å›¾ã€è¶‹åŠ¿å›¾ï¼‰
- ğŸ” äººå·¥ä¿®æ­£åŠŸèƒ½å®Œå–„
- ğŸ“š è¯„æµ‹ä»»åŠ¡åº“æ‰©å……ï¼ˆ10-20ä¸ªï¼‰
- ğŸ§  LLM Judge Promptä¼˜åŒ–ï¼ˆæå‡è¯„æµ‹å‡†ç¡®æ€§ï¼‰
- ğŸ’¾ è¯„æµ‹æ•°æ®åˆ†æå’Œå¯è§†åŒ–

---

## 10. å¾…è®¨è®ºé—®é¢˜

### 10.1 LLM Judgeç›¸å…³

**Q1: LLM Judgeç”¨ä»€ä¹ˆæ¨¡å‹ï¼Ÿ**
- é€‰é¡¹A: ç”¨è¢«è¯„æµ‹çš„æ¨¡å‹è‡ªå·±ï¼ˆå¯èƒ½æœ‰åå·®ï¼‰
- é€‰é¡¹B: å›ºå®šç”¨ä¸€ä¸ªå¼ºæ¨¡å‹ï¼ˆå¦‚GPT-4ï¼‰
- é€‰é¡¹C: å…è®¸ç”¨æˆ·é€‰æ‹©Judgeæ¨¡å‹

**Q2: LLM Judgeçš„ç¨³å®šæ€§å¦‚ä½•ä¿è¯ï¼Ÿ**
- æ˜¯å¦éœ€è¦å¤šæ¬¡è¯„æµ‹å–å¹³å‡ï¼Ÿ
- å¦‚ä½•å¤„ç†LLMè¿”å›æ ¼å¼ä¸ä¸€è‡´çš„æƒ…å†µï¼Ÿ
- æ˜¯å¦éœ€è¦ç¼“å­˜LLM Judgeç»“æœé¿å…é‡å¤è°ƒç”¨ï¼Ÿ

**Q3: LLM Judgeæ˜¯å¦æ”¯æŒå¤šæ¨¡æ€è¾“å…¥ï¼Ÿ**
- å›¾ç‰‡è¯„æµ‹éœ€è¦visionæ¨¡å‹æ”¯æŒ
- å¦‚ä½•å¤„ç†ä¸æ”¯æŒvisionçš„æ¨¡å‹ï¼Ÿ

### 10.2 æƒé‡é…ç½®ç›¸å…³

**Q4: æƒé‡å¦‚ä½•è®¾å®šï¼Ÿ**
- æ˜¯å¦å…è®¸ç”¨æˆ·è‡ªå®šä¹‰æƒé‡ï¼Ÿ
- ä¸åŒç±»å‹ä»»åŠ¡çš„æƒé‡æ˜¯å¦åº”è¯¥ä¸åŒï¼Ÿ
- æƒé‡æ˜¯å¦éœ€è¦å½’ä¸€åŒ–ï¼Ÿ

**Q5: äººå·¥æ ‡æ³¨çš„æƒé‡åº”è¯¥å å¤šå°‘ï¼Ÿ**
- ç›®å‰è®¾è®¡æ˜¯æœ€é«˜æƒé‡ï¼ˆ3.0 + 2.0 = 5.0ï¼‰
- æ˜¯å¦ä¼šå¯¼è‡´äººå·¥æ ‡æ³¨ä¸»å¯¼è¯„åˆ†ï¼Ÿ

### 10.3 è¯„æµ‹å‡†ç¡®æ€§ç›¸å…³

**Q6: å¦‚ä½•éªŒè¯è¯„æµ‹ç³»ç»Ÿæœ¬èº«çš„å‡†ç¡®æ€§ï¼Ÿ**
- éœ€è¦å…ˆå»ºç«‹ground truthæ•°æ®é›†å—ï¼Ÿ
- å¦‚ä½•é¿å…è¯„æµ‹ç³»ç»Ÿçš„ç³»ç»Ÿæ€§åå·®ï¼Ÿ

**Q7: è§„åˆ™æ£€æŸ¥çš„é˜ˆå€¼å¦‚ä½•ç¡®å®šï¼Ÿ**
- å¦‚å°ºå¯¸å…è®¸10%è¯¯å·®æ˜¯å¦åˆç†ï¼Ÿ
- é¢œè‰²æ£€æµ‹çš„å‡†ç¡®æ€§å¦‚ä½•ä¿è¯ï¼Ÿ

### 10.4 å®æ–½ç»†èŠ‚ç›¸å…³

**Q8: è¯„æµ‹æ•°æ®å¦‚ä½•å­˜å‚¨ï¼Ÿ**
- ç”¨JSONæ–‡ä»¶ vs æ•°æ®åº“ï¼Ÿ
- éœ€è¦ä¿ç•™å¤šä¹…çš„å†å²æ•°æ®ï¼Ÿ

**Q9: å¹¶è¡Œæ‰§è¡Œçš„å¹¶å‘æ§åˆ¶ï¼Ÿ**
- æ˜¯å¦éœ€è¦é™åˆ¶åŒæ—¶è¿è¡Œçš„è¯„æµ‹ä»»åŠ¡æ•°é‡ï¼Ÿ
- å¦‚ä½•å¤„ç†é•¿æ—¶é—´è¿è¡Œçš„ä»»åŠ¡ï¼Ÿ

**Q10: æ˜¯å¦éœ€è¦æ”¯æŒæ‰¹é‡è¯„æµ‹ï¼Ÿ**
- ä¸€æ¬¡è¿è¡Œ10ä¸ªä»»åŠ¡ï¼Œå¯¹æ¯”2ä¸ªæ¨¡å‹
- ç”Ÿæˆæ‰¹é‡æŠ¥å‘Š

### 10.5 ç”¨æˆ·ä½“éªŒç›¸å…³

**Q11: æ˜¯å¦éœ€è¦è¯„æµ‹æ¨¡æ¿ï¼Ÿ**
- é¢„è®¾ä¸€äº›å¸¸è§çš„è¯„æµ‹åœºæ™¯
- ç”¨æˆ·å¯ä»¥åŸºäºæ¨¡æ¿å¿«é€Ÿåˆ›å»ºæ–°ä»»åŠ¡

**Q12: äººå·¥æ ‡æ³¨æ˜¯å¦æ”¯æŒå¤šäººåä½œï¼Ÿ**
- å¤šä¸ªæ ‡æ³¨å‘˜å¯¹åŒä¸€ä»»åŠ¡æ‰“åˆ†
- å–å¹³å‡æˆ–æŠ•ç¥¨

**Q13: æ˜¯å¦éœ€è¦è¯„æµ‹å†å²å¯¹æ¯”ï¼Ÿ**
- æŸ¥çœ‹åŒä¸€ä»»åŠ¡åœ¨ä¸åŒæ—¶é—´ç‚¹çš„è¯„æµ‹ç»“æœ
- è¿½è¸ªæ¨¡å‹èƒ½åŠ›çš„å˜åŒ–

---

## é™„å½•

### A. æœ¯è¯­è¡¨

| æœ¯è¯­ | è¯´æ˜ |
|------|------|
| è¯„æµ‹ä»»åŠ¡ | ä¸€ä¸ªå®Œæ•´çš„æµ‹è¯•ç”¨ä¾‹ï¼ŒåŒ…å«promptã€é¢„æœŸè¾“å‡ºã€checklist |
| è¯„æµ‹ä¼šè¯ | ä¸€æ¬¡å®Œæ•´çš„è¯„æµ‹æµç¨‹ï¼Œä»æ‰§è¡Œåˆ°æ ‡æ³¨å®Œæˆ |
| Checklist | è¯„æµ‹é¡¹åˆ—è¡¨ï¼Œæ¯é¡¹éƒ½æœ‰æ˜ç¡®çš„éªŒè¯æ ‡å‡† |
| Rule-based | åŸºäºç¡®å®šæ€§è§„åˆ™çš„è‡ªåŠ¨æ£€æŸ¥ |
| LLM Judge | ä½¿ç”¨LLMè¿›è¡Œè¯­ä¹‰å±‚é¢çš„è¯„ä¼° |
| Human Annotation | äººå·¥æ ‡æ³¨å’Œæ‰“åˆ† |
| ç”¨æˆ·æ„å›¾è¾¾æˆåº¦ | æ ¸å¿ƒè¯„æµ‹æŒ‡æ ‡ï¼Œè¡¡é‡Agentæ˜¯å¦äº¤ä»˜äº†èƒ½ç”¨çš„ä¸œè¥¿ |

### B. å‚è€ƒèµ„æ–™

- [LLM-as-a-Judgeè®ºæ–‡](https://arxiv.org/abs/2306.05685)
- [Chatbot Arenaè¯„æµ‹æ–¹æ³•](https://lmsys.org/blog/2023-05-03-arena/)
- [AlpacaEvalè¯„æµ‹æ¡†æ¶](https://github.com/tatsu-lab/alpaca_eval)

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025-01-15
**ä½œè€…**: Claude
**çŠ¶æ€**: è‰æ¡ˆï¼Œå¾…è®¨è®º
