# CreativeFlow 评测指标体系设计

## 核心理念

**Human 为主，LLM 辅助降低标注成本，Rule 过滤极端情况**

对于创意类任务（没有标准答案），好坏往往"一看便知"，这需要人类的综合判断和直觉。

自动化评测不是替代人类，而是：
1. **Rule-based**: 过滤掉明显不合格的，节省后续评测成本
2. **LLM Judge**: 提供分析，帮 Human 快速定位关键差异点
3. **Human Annotation**: 做最终判断（这是 Ground Truth）

---

## 三层评测体系

### Layer 1: Rule-based（过滤层）

**作用**：快速过滤极端情况

**适用场景**：
- 文件数量范围检查（避免 0 个或 1000 个）
- 文件格式验证
- 文件可读性检查

**设计原则**：
- 只检查**硬性要求**，不评估"好不好"
- 宁可宽松，避免误杀
- 不通过就结束，不浪费后续成本

**权重**：0-10%（甚至可以不计分，只做门槛）

**示例**：
```yaml
# 不计分，只做门槛
- check_type: file_count_range
  params: { min: 5, max: 100 }
  weight: 0
  is_required: true  # 不通过直接失败

# 或者给少量分数
- check_type: file_format_check
  params: { expected_formats: ["png", "svg"] }
  weight: 0.5
```

---

### Layer 2: LLM Judge（辅助层）

**作用**：给 Human 提供参考信息，降低标注成本

**LLM 不是打最终分数，而是**：
- 高亮关键差异点（哪些不相关、哪些混乱）
- 提供量化分析（覆盖率、冗余度）
- 给出初步评估供 Human 参考

**适用场景**：
- 相关性评估
- 完整性检查
- 组织方式评估

**Prompt 设计要点**：
1. 要求 LLM **提供具体分析**，不只是打分
2. 结构化输出，便于展示给 Human
3. 明确评分标准

**权重**：20-40%

**示例**：
```yaml
- check_type: llm_judge
  weight: 2.0
  description: "相关性分析"
  params:
    prompt: |
      分析这些图标与"{任务要求}"的匹配度。

      请逐个检查，列出：
      1. 高度相关的图标（文件名）
      2. 相关性一般的图标（说明原因）
      3. 不相关的图标（说明原因）
      4. 覆盖率评估

      输出格式：
      【高度相关】
      - file1.png: ...

      【不相关】
      - file2.png: ...

      【覆盖率】80% - 缺少 XX 场景

      【评分】4/5
      【理由】...
    score_range: [1, 5]
```

**LLM 的局限**：
- 判断不了"数量是否恰当"（15 个精品 vs 50 个含水分）
- 判断不了"哪个更好用"（需要人的使用体验）
- 可能被表面因素迷惑

---

### Layer 3: Human Annotation（决策层）

**作用**：最终判断

**为什么 Human 必须占主导（50-80%）**：
- "一看便知好坏"需要人类直觉
- 需要综合判断（数量、质量、可用性的平衡）
- LLM 和 Rule 都可能错，Human 是最终修正

**标注应该简单高效**：

```yaml
# 最小化设计（推荐）
- check_type: human_annotation
  weight: 8.0
  params:
    question: "如果你需要这些素材，你会选择哪个？"
    options: ["model_a", "model_b", "both_bad"]
    require_reason: true

# 多维度设计（可选）
- check_type: human_annotation
  weight: 6.0
  params:
    dimensions:
      - relevance: "相关性"
      - quantity_fit: "数量是否恰当"
      - usability: "组织方式是否好用"
    options: ["model_a", "model_b", "tie"]
```

**UI 设计关键**：

```
┌────────────────────────────────────────┐
│  Model A         Model B               │
│  [预览]          [预览]                │
│                                        │
│  【LLM 分析】（可折叠）                 │
│  A: 相关性 4.5/5, 覆盖率 85%           │
│  B: 相关性 3/5, 10 个不相关            │
│                                        │
│  【你的判断】                           │
│  ● A  ○ B  ○ 都不好                   │
│                                        │
│  理由：[A数量少但精品，B很多水分...]    │
│                                        │
│  [提交]                                │
└────────────────────────────────────────┘
```

**降低标注成本**：
1. LLM 预分析，Human 直接看到关键差异
2. 分层标注：简单样本 30 秒，复杂样本 3-5 分钟
3. 抽样验证：LLM 很确定的样本只抽样验证

---

## 实施流程

```
样本 → Rule-based（毫秒级）
         ↓ 不合格 → 失败，结束
         ↓ 合格
      LLM Judge（秒级）
         ↓ 生成分析报告
      Human Annotation（分钟级）
         ↓ 参考 LLM 分析
         ↓ 做最终判断
      计算最终得分
```

---

## 核心观点

1. **Human Annotation 是最终真相**，LLM 和 Rule 都是辅助
2. **LLM Judge 的价值是降低 Human 标注成本**，不是替代
3. **"一看便知好坏"的任务，Human 必须占 60%+**
4. **Rule-based 只做门槛，不要用规则评估质量**

---

**文档版本**: v2.1
**最后更新**: 2025-01-15
