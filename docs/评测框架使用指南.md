# CreativeFlow è¯„æµ‹æ¡†æ¶ä½¿ç”¨æŒ‡å—

## ğŸ“‹ è¯„æµ‹æµç¨‹æ¦‚è§ˆ

```
æ ·æœ¬å‡†å¤‡ â†’ Agentæ‰§è¡Œ â†’ è‡ªåŠ¨è¯„æµ‹ â†’ Humanæ ‡æ³¨ â†’ æœ€ç»ˆå¾—åˆ†
```

### 1. æ ·æœ¬å‡†å¤‡
åˆ›å»ºç¬¦åˆ `schemas/evaluation_sample_schema.json` æ ¼å¼çš„æ ·æœ¬æ–‡ä»¶

### 2. Agentæ‰§è¡Œ
è®©ä¸¤ä¸ªæ¨¡å‹ï¼ˆModel Aå’ŒModel Bï¼‰åˆ†åˆ«æ‰§è¡Œæ ·æœ¬ä»»åŠ¡ï¼Œè®°å½•æ‰§è¡Œç»“æœ

### 3. è‡ªåŠ¨è¯„æµ‹
ä½¿ç”¨ `run_evaluation.py` è¿è¡ŒRule-basedå’ŒLLM Judgeè‡ªåŠ¨è¯„æµ‹

### 4. Humanæ ‡æ³¨
æ‰‹åŠ¨æ ‡æ³¨è¯„æµ‹ç»“æœJSONæ–‡ä»¶çš„ `human_annotation` å­—æ®µ

### 5. æœ€ç»ˆå¾—åˆ†
åŸºäºè‡ªåŠ¨è¯„æµ‹å’Œäººå·¥æ ‡æ³¨è®¡ç®— `final_scores` å’Œ `winner`

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…ä¾èµ–
```bash
pip install requests
```

### è¿è¡Œè¯„æµ‹
```bash
python run_evaluation.py \
    --sample samples/EVAL_ICON_COLLECTION_TECH.json \
    --execution-a executions/ernie-5.0_ICON_COLLECTION.json \
    --execution-b executions/gpt-5_ICON_COLLECTION.json \
    --output results/EVAL_ICON_COLLECTION_TECH_result.json \
    --judge-model claude-sonnet-4-5-20250929 \
    --judge-base-url https://api.anthropic.com/v1 \
    --judge-api-key sk-xxx
```

### å‚æ•°è¯´æ˜
- `--sample`: æ ·æœ¬JSONæ–‡ä»¶è·¯å¾„ï¼ˆå¿…éœ€ï¼‰
- `--execution-a`: Model Açš„æ‰§è¡Œç»“æœï¼ˆå¿…éœ€ï¼‰
- `--execution-b`: Model Bçš„æ‰§è¡Œç»“æœï¼ˆå¿…éœ€ï¼‰
- `--output`: è¯„æµ‹ç»“æœè¾“å‡ºè·¯å¾„ï¼ˆå¿…éœ€ï¼‰
- `--judge-model`: LLM Judgeæ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼Œå¦‚æ ·æœ¬åŒ…å«llm_judgeåˆ™å¿…éœ€ï¼‰
- `--judge-base-url`: LLM APIåœ°å€ï¼ˆå¯é€‰ï¼‰
- `--judge-api-key`: LLM APIå¯†é’¥ï¼ˆå¯é€‰ï¼‰

---

## ğŸ“„ æ–‡ä»¶æ ¼å¼è¯´æ˜

### è¾“å…¥æ–‡ä»¶1: æ ·æœ¬æ–‡ä»¶
æ ¼å¼ï¼š`schemas/evaluation_sample_schema.json`

**å…³é”®å­—æ®µ**ï¼š
- `data_id`: æ ·æœ¬å”¯ä¸€æ ‡è¯†
- `query`: ç”¨æˆ·ä»»åŠ¡æè¿°
- `models`: å¯¹æ¯”çš„ä¸¤ä¸ªæ¨¡å‹åç§°
- `check_list`: æ£€æŸ¥é¡¹åˆ—è¡¨
  - `check_type`: æ£€æŸ¥ç±»å‹ï¼ˆfile_count_range, llm_judge, human_annotationç­‰ï¼‰
  - `params`: æ£€æŸ¥å‚æ•°
  - `weight`: æƒé‡
  - `is_required`: æ˜¯å¦ä¸ºå¿…éœ€é¡¹ï¼ˆä¸é€šè¿‡åˆ™æ•´ä½“å¤±è´¥ï¼‰

**ç¤ºä¾‹**ï¼š`samples/EVAL_ICON_COLLECTION_TECH.json`

### è¾“å…¥æ–‡ä»¶2: Agentæ‰§è¡Œç»“æœ
æ ¼å¼ï¼š`schemas/agent_execution_result_schema.json`

**å…³é”®å­—æ®µ**ï¼š
- `sample_id`: å¯¹åº”çš„æ ·æœ¬ID
- `model_name`: æ‰§è¡Œçš„æ¨¡å‹åç§°
- `status`: æ‰§è¡ŒçŠ¶æ€ï¼ˆsuccess/failed/timeout/partialï¼‰
- `final_state`: æœ€ç»ˆçŠ¶æ€ï¼ˆåŒ…å«ç”Ÿæˆçš„æ–‡ä»¶åˆ—è¡¨ï¼‰
- `conversation_history`: å®Œæ•´å¯¹è¯å†å²

**final_state.files ç¤ºä¾‹**ï¼š
```json
{
  "files": [
    {
      "path": "icons/tech/ai_icon.svg",
      "size": 2048,
      "type": "svg",
      "metadata": {
        "dimensions": {"width": 512, "height": 512},
        "category": "AI"
      }
    }
  ]
}
```

### è¾“å‡ºæ–‡ä»¶: è¯„æµ‹ç»“æœ
æ ¼å¼ï¼š`schemas/evaluation_result_schema.json`

**ç»“æ„**ï¼š
```json
{
  "sample_id": "EVAL_ICON_COLLECTION_TECH",
  "evaluated_at": "2025-01-15T10:30:00Z",
  "executions": {
    "model_a": {
      "model_name": "ernie-5.0-thinking-preview",
      "status": "success",
      "file_count": 85
    },
    "model_b": {
      "model_name": "gpt-5",
      "status": "success",
      "file_count": 120
    }
  },
  "check_results": {
    "model_a": [
      {
        "check_type": "file_count_range",
        "score": 1.0,
        "passed": true,
        "details": "ç”Ÿæˆ85ä¸ªæ–‡ä»¶ï¼Œè¦æ±‚10-200ä¸ª",
        "weight": 0
      }
    ],
    "model_b": [...]
  },
  "human_annotation": null,  // éœ€è¦æ‰‹åŠ¨æ ‡æ³¨
  "final_scores": null,      // æ ‡æ³¨åè®¡ç®—
  "winner": null             // æ ‡æ³¨ååˆ¤æ–­
}
```

---

## ğŸ” æ£€æŸ¥ç±»å‹è¯´æ˜

### Rule-based æ£€æŸ¥

#### 1. file_count_range
æ£€æŸ¥æ–‡ä»¶æ•°é‡æ˜¯å¦åœ¨èŒƒå›´å†…

**å‚æ•°**ï¼š
```json
{
  "check_type": "file_count_range",
  "params": {
    "min": 10,
    "max": 200
  },
  "weight": 0,
  "is_required": true
}
```

**è¯„åˆ†é€»è¾‘**ï¼š
- åœ¨èŒƒå›´å†…ï¼š1.0åˆ†
- å°‘äºminï¼šæŒ‰æ¯”ä¾‹ `actual / min`
- å¤šäºmaxï¼šæŒ‰æ¯”ä¾‹ `1 - (actual - max) / max`

#### 2. file_count_equals
æ£€æŸ¥æ–‡ä»¶æ•°é‡æ˜¯å¦ç­‰äºæœŸæœ›å€¼

**å‚æ•°**ï¼š
```json
{
  "check_type": "file_count_equals",
  "params": {
    "expected": 5
  },
  "weight": 1.0
}
```

#### 3. file_format_check
æ£€æŸ¥æ–‡ä»¶æ ¼å¼

**å‚æ•°**ï¼š
```json
{
  "check_type": "file_format_check",
  "params": {
    "expected_formats": ["png", "svg", "jpg"]
  },
  "weight": 2.0
}
```

**è¯„åˆ†é€»è¾‘**ï¼šåŒ¹é…æ ¼å¼çš„æ–‡ä»¶æ•° / æ€»æ–‡ä»¶æ•°

#### 4. image_size_check
æ£€æŸ¥å›¾ç‰‡å°ºå¯¸

**å‚æ•°**ï¼š
```json
{
  "check_type": "image_size_check",
  "params": {
    "width": 512,
    "height": 512,
    "tolerance": 0.1
  },
  "weight": 1.5
}
```

**è¯„åˆ†é€»è¾‘**ï¼šç¬¦åˆå°ºå¯¸çš„å›¾ç‰‡æ•° / æ€»å›¾ç‰‡æ•°

### LLM Judge æ£€æŸ¥

ä½¿ç”¨LLMè¿›è¡Œè¯­ä¹‰å±‚é¢çš„è¯„ä¼°

**å‚æ•°**ï¼š
```json
{
  "check_type": "llm_judge",
  "params": {
    "prompt": "åˆ†æè¿™äº›å›¾æ ‡ä¸'ç§‘æŠ€ç±»'ä¸»é¢˜çš„åŒ¹é…åº¦ï¼Œä»1-5åˆ†è¯„åˆ†",
    "score_range": [1, 5],
    "temperature": 0.3
  },
  "weight": 2.5
}
```

**LLMè¾“å…¥ä¸Šä¸‹æ–‡**ï¼š
- ç”Ÿæˆçš„æ–‡ä»¶åˆ—è¡¨ï¼ˆè·¯å¾„ã€ç±»å‹ã€å¤§å°ï¼‰
- ç®€åŒ–çš„å¯¹è¯å†å²ï¼ˆå‰3è½® + å3è½®ï¼‰

**æœŸæœ›LLMè¿”å›æ ¼å¼**ï¼š
```
ã€è¯„åˆ†ã€‘4/5
ã€ç†ç”±ã€‘å›¾æ ‡è¦†ç›–äº†AIã€äº‘è®¡ç®—ã€æ•°æ®ç­‰ä¸»è¦ç§‘æŠ€åœºæ™¯...
```

### Human Annotation æ£€æŸ¥

äººå·¥æ ‡æ³¨ï¼Œç”±è¯„æµ‹è„šæœ¬è·³è¿‡ï¼Œéœ€æ‰‹åŠ¨å¡«å†™

**å‚æ•°**ï¼š
```json
{
  "check_type": "human_annotation",
  "params": {
    "question": "å¦‚æœä½ éœ€è¦ä¸ºç§‘æŠ€å…¬å¸æ­å»ºå›¾æ ‡ç´ æåº“ï¼Œä½ ä¼šé€‰æ‹©å“ªä¸ªAgentçš„ç»“æœï¼Ÿ",
    "options": ["model_a", "model_b", "both_bad"],
    "require_reason": true
  },
  "weight": 8.0
}
```

**æ‰‹åŠ¨æ ‡æ³¨æ ¼å¼**ï¼š
```json
{
  "human_annotation": {
    "preference": "model_b",
    "dimensions": {
      "quantity": "model_b",
      "relevance": "tie",
      "usability": "model_a"
    },
    "reason": "Model Bå›¾æ ‡æ•°é‡æ›´åˆé€‚ï¼Œåˆ†ç±»ç»„ç»‡æ›´æ¸…æ™°",
    "annotated_by": "å¼ ä¸‰",
    "annotated_at": "2025-01-15T14:20:00Z"
  }
}
```

---

## ğŸ“Š æœ€ç»ˆå¾—åˆ†è®¡ç®—

å®ŒæˆHuman Annotationåï¼ŒæŒ‰ä»¥ä¸‹å…¬å¼è®¡ç®—ï¼š

```
final_score = Î£ (check_result.score Ã— check_result.weight) / Î£ (check_result.weight)
```

**æ³¨æ„**ï¼š
- `is_required=true` çš„æ£€æŸ¥é¡¹ä¸é€šè¿‡ï¼Œæ•´ä½“å¤±è´¥
- `is_required=false` çš„æ£€æŸ¥é¡¹æŒ‰æƒé‡å‚ä¸è®¡ç®—
- æƒé‡ä¸º0çš„æ£€æŸ¥é¡¹ä¸å‚ä¸å¾—åˆ†è®¡ç®—ï¼ˆä»…ä½œé—¨æ§›ï¼‰

**winneråˆ¤æ–­**ï¼š
- `final_score_a > final_score_b`ï¼šwinner = "model_a"
- `final_score_b > final_score_a`ï¼šwinner = "model_b"
- ç›¸ç­‰ï¼šwinner = "tie"
- å¦‚æœhuman_annotation.preference = "both_bad"ï¼Œåˆ™ winner = "tie"

---

## ğŸ¯ è¯„æµ‹ç†å¿µ

### Humanä¸ºä¸»ï¼ŒLLMè¾…åŠ©é™ä½æ ‡æ³¨æˆæœ¬ï¼ŒRuleè¿‡æ»¤æç«¯æƒ…å†µ

**æƒé‡åˆ†é…å»ºè®®**ï¼š
- Rule-based: 0-10%ï¼ˆä¸»è¦åšé—¨æ§›è¿‡æ»¤ï¼‰
- LLM Judge: 20-40%ï¼ˆæä¾›å‚è€ƒä¿¡æ¯ï¼‰
- Human Annotation: 50-80%ï¼ˆæœ€ç»ˆå†³ç­–ï¼‰

### ç¤ºä¾‹åœºæ™¯ï¼šå›¾æ ‡ç´ ææœé›†

```json
{
  "check_list": [
    {
      "check_type": "file_count_range",
      "params": {"min": 10, "max": 200},
      "weight": 0,
      "is_required": true
    },
    {
      "check_type": "llm_judge",
      "description": "ä¸»é¢˜ç›¸å…³æ€§è¯„ä¼°",
      "params": {
        "prompt": "åˆ†æå›¾æ ‡ä¸ç§‘æŠ€ä¸»é¢˜çš„åŒ¹é…åº¦...",
        "score_range": [1, 5]
      },
      "weight": 2.5
    },
    {
      "check_type": "llm_judge",
      "description": "ç»„ç»‡æ–¹å¼è¯„ä¼°",
      "params": {
        "prompt": "è¯„ä¼°å›¾æ ‡çš„ç»„ç»‡æ–¹å¼å’Œæ˜“ç”¨æ€§...",
        "score_range": [1, 5]
      },
      "weight": 2.0
    },
    {
      "check_type": "human_annotation",
      "description": "ç»¼åˆè¯„ä¼°",
      "params": {
        "question": "ç»¼åˆè€ƒè™‘æ•°é‡ã€ç›¸å…³æ€§ã€æ˜“ç”¨æ€§ï¼Œä½ ä¼šé€‰æ‹©å“ªä¸ªï¼Ÿ",
        "options": ["model_a", "model_b", "both_bad"],
        "require_reason": true
      },
      "weight": 8.0
    }
  ]
}
```

**æƒé‡å æ¯”**ï¼š
- Rule: 0% (ä»…é—¨æ§›)
- LLM: 35.3% (4.5/12.5)
- Human: 64.7% (8.0/12.5)

---

## ğŸ› ï¸ å¼€å‘è€…æŒ‡å—

### æ‰©å±•æ–°çš„Ruleæ£€æŸ¥ç±»å‹

ç¼–è¾‘ `evaluator/rule_checker.py`ï¼š

```python
def _check_custom_rule(self, execution_result, params, check_item):
    """è‡ªå®šä¹‰è§„åˆ™æ£€æŸ¥"""
    final_state = execution_result.get("final_state", {})
    files = final_state.get("files", [])

    # å®ç°æ£€æŸ¥é€»è¾‘
    passed = ...
    score = ...

    return {
        "check_type": check_item["check_type"],
        "description": check_item.get("description", "..."),
        "score": score,
        "passed": passed,
        "details": "...",
        "weight": check_item.get("weight", 1.0)
    }
```

ç„¶ååœ¨ `check()` æ–¹æ³•ä¸­æ·»åŠ è·¯ç”±ï¼š
```python
elif check_type == "custom_rule":
    return self._check_custom_rule(execution_result, params, check_item)
```

### è‡ªå®šä¹‰LLM Judge Prompt

åœ¨æ ·æœ¬çš„ `check_list` ä¸­è®¾è®¡è¯¦ç»†çš„promptï¼š

```json
{
  "check_type": "llm_judge",
  "params": {
    "prompt": "ã€è¯„ä¼°ç»´åº¦ã€‘\n1. å†…å®¹å®Œæ•´æ€§\n2. æ ¼å¼è§„èŒƒæ€§\n3. å®ç”¨ä»·å€¼\n\nã€è¯„åˆ†æ ‡å‡†ã€‘\n5åˆ†ï¼šä¼˜ç§€...\n4åˆ†ï¼šè‰¯å¥½...\n\nè¯·åˆ†æAgentç”Ÿæˆçš„å†…å®¹ï¼ŒæŒ‰1-5åˆ†è¯„åˆ†ï¼Œå¹¶è¯´æ˜ç†ç”±ã€‚\n\nè¾“å‡ºæ ¼å¼ï¼š\nã€è¯„åˆ†ã€‘X/5\nã€ç†ç”±ã€‘...",
    "score_range": [1, 5]
  }
}
```

---

## â“ å¸¸è§é—®é¢˜

### Q1: å¦‚æœæ ·æœ¬ä¸­æ²¡æœ‰llm_judgeæ£€æŸ¥é¡¹ï¼Œè¿˜éœ€è¦æä¾›LLMé…ç½®å—ï¼Ÿ
ä¸éœ€è¦ã€‚è¯„æµ‹è„šæœ¬ä¼šè‡ªåŠ¨æ£€æµ‹ï¼Œåªæœ‰åœ¨åŒ…å«llm_judgeæ—¶æ‰è¦æ±‚é…ç½®ã€‚

### Q2: Human Annotationå¯ä»¥åªå¡«preferenceï¼Œä¸å¡«dimensionså—ï¼Ÿ
å¯ä»¥ã€‚`dimensions` æ˜¯å¯é€‰å­—æ®µï¼Œä½†å»ºè®®å¡«å†™ä»¥ä¾¿åç»­åˆ†æã€‚

### Q3: å¦‚ä½•æ‰¹é‡è¯„æµ‹å¤šä¸ªæ ·æœ¬ï¼Ÿ
å¯ä»¥ç¼–å†™shellè„šæœ¬å¾ªç¯è°ƒç”¨ `run_evaluation.py`ï¼š
```bash
for sample in samples/*.json; do
    python run_evaluation.py \
        --sample "$sample" \
        --execution-a "executions/model_a_$(basename $sample)" \
        --execution-b "executions/model_b_$(basename $sample)" \
        --output "results/$(basename $sample .json)_result.json" \
        --judge-model claude-sonnet-4-5-20250929 \
        --judge-base-url ... \
        --judge-api-key ...
done
```

### Q4: å¦‚ä½•éªŒè¯JSONæ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®ï¼Ÿ
ä½¿ç”¨JSON SchemaéªŒè¯å·¥å…·ï¼Œå¦‚ `jsonschema` Pythonåº“ï¼š
```python
import json
import jsonschema

with open('schemas/evaluation_sample_schema.json') as f:
    schema = json.load(f)

with open('samples/EVAL_ICON_COLLECTION_TECH.json') as f:
    sample = json.load(f)

jsonschema.validate(sample, schema)  # éªŒè¯é€šè¿‡åˆ™æ— å¼‚å¸¸
```

### Q5: LLM Judgeè¿”å›çš„åˆ†æ•°è¶…å‡ºscore_rangeæ€ä¹ˆåŠï¼Ÿ
`LLMJudge._parse_response()` ä¼šè‡ªåŠ¨æˆªæ–­åˆ°èŒƒå›´å†…ï¼š
```python
score = max(score_range[0], min(score, score_range[1]))
```

---

## ğŸ“ é™„å½•

### ç›®å½•ç»“æ„
```
creative_agent/
â”œâ”€â”€ schemas/                        # JSON Schemaå®šä¹‰
â”‚   â”œâ”€â”€ evaluation_sample_schema.json
â”‚   â”œâ”€â”€ agent_execution_result_schema.json
â”‚   â””â”€â”€ evaluation_result_schema.json
â”œâ”€â”€ samples/                        # è¯„æµ‹æ ·æœ¬
â”‚   â”œâ”€â”€ EVAL_ICON_COLLECTION_TECH.json
â”‚   â””â”€â”€ EVAL_COMPETITOR_ANALYSIS_AI.json
â”œâ”€â”€ executions/                     # Agentæ‰§è¡Œç»“æœï¼ˆéœ€è‡ªè¡Œå‡†å¤‡ï¼‰
â”‚   â”œâ”€â”€ ernie-5.0_ICON_COLLECTION.json
â”‚   â””â”€â”€ gpt-5_ICON_COLLECTION.json
â”œâ”€â”€ results/                        # è¯„æµ‹ç»“æœè¾“å‡º
â”‚   â””â”€â”€ EVAL_ICON_COLLECTION_TECH_result.json
â”œâ”€â”€ evaluator/                      # è¯„æµ‹æ¡†æ¶ä»£ç 
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ rule_checker.py
â”‚   â”œâ”€â”€ llm_judge.py
â”‚   â””â”€â”€ orchestrator.py
â”œâ”€â”€ docs/                           # æ–‡æ¡£
â”‚   â”œâ”€â”€ è¯„æµ‹æŒ‡æ ‡ä½“ç³»è®¾è®¡.md
â”‚   â””â”€â”€ è¯„æµ‹æ¡†æ¶ä½¿ç”¨æŒ‡å—.md
â””â”€â”€ run_evaluation.py               # å‘½ä»¤è¡Œå…¥å£
```

### ç›¸å…³æ–‡æ¡£
- `docs/è¯„æµ‹æŒ‡æ ‡ä½“ç³»è®¾è®¡.md` - è¯„æµ‹ç†å¿µå’ŒæŒ‡æ ‡è®¾è®¡
- `schemas/` - å„ç±»JSONæ ¼å¼è§„èŒƒ
- `samples/` - æ ·æœ¬ç¤ºä¾‹

---

**ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025-01-15
