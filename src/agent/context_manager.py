"""Context Manager - å¯¹è¯ä¸Šä¸‹æ–‡ç®¡ç†å’Œå‹ç¼©

è´Ÿè´£:
1. Tokenè®¡æ•°å’Œä½¿ç”¨ç‡ç›‘æ§
2. è‡ªåŠ¨å‹ç¼©å†å²å¯¹è¯
3. æ™ºèƒ½ä¿ç•™å…³é”®ä¿¡æ¯
"""

from typing import List, Dict, Any
from src.utils.logger import get_logger

logger = get_logger(__name__)


class ContextManager:
    """å¯¹è¯ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""

    def __init__(self, model_name: str = "gpt-4", max_tokens: int = None):
        """åˆå§‹åŒ–Context Manager

        Args:
            model_name: æ¨¡å‹åç§°,ç”¨äºtokenè®¡æ•°
            max_tokens: æœ€å¤§context windowå¤§å°ï¼ˆNoneåˆ™è‡ªåŠ¨æ ¹æ®æ¨¡å‹æ¨æ–­ï¼‰
        """
        self.model_name = model_name

        # è‡ªåŠ¨è¯†åˆ«æ¨¡å‹çš„context windowå¤§å°
        if max_tokens is None:
            max_tokens = self._infer_max_tokens(model_name)

        self.max_tokens = max_tokens
        self.compression_threshold = 0.85  # 85%è§¦å‘å‹ç¼©ï¼ˆå……åˆ†åˆ©ç”¨200K contextï¼‰
        self.recent_turns_to_keep = 3  # ä¿ç•™æœ€è¿‘3è½®ä¸å‹ç¼©ï¼ˆå‚è€ƒAnthropicå»ºè®®ï¼‰

        logger.info(f"ContextManageråˆå§‹åŒ–: model={model_name}, max_tokens={max_tokens}, threshold={self.compression_threshold}")

    def _infer_max_tokens(self, model_name: str) -> int:
        """æ ¹æ®æ¨¡å‹åç§°æ¨æ–­context windowå¤§å°

        Args:
            model_name: æ¨¡å‹åç§°

        Returns:
            æ¨æ–­çš„max_tokenså¤§å°
        """
        model_lower = model_name.lower()

        # Claude ç³»åˆ—ï¼ˆ3.x, 4.5, Opus, Sonnet, Haikuç­‰ï¼‰- 200K tokens
        if 'claude' in model_lower:
            return 200000

        # Gemini 1.5/3.0 ç³»åˆ— - 1M tokens
        if 'gemini' in model_lower and (any(x in model_lower for x in ['1.5', '3', 'pro', 'flash'])):
            return 1000000

        # GPT-5 - è¯·ç¡®è®¤å…·ä½“çš„context windowå¤§å°
        if 'gpt-5' in model_lower:
            # TODO: ç¡®è®¤GPT-5çš„å®é™…context windowå¤§å°
            return 200000  # ä¸´æ—¶ä½¿ç”¨200Kï¼Œéœ€è¦æ ¹æ®å®˜æ–¹æ–‡æ¡£è°ƒæ•´

        # GPT-4 Turbo / GPT-4o - 128K tokens
        if any(x in model_lower for x in ['gpt-4-turbo', 'gpt-4o', 'gpt-4-0125', 'gpt-4-1106']):
            return 128000

        # GPT-4-32K - 32K tokens
        if 'gpt-4-32k' in model_lower:
            return 32000

        # GPT-4 åŸºç¡€ç‰ˆ - 8K tokens
        if 'gpt-4' in model_lower:
            return 8000

        # GLM-4 ç³»åˆ— - 128K tokens
        if 'glm-4' in model_lower:
            return 128000

        # Deepseek ç³»åˆ— - 128K tokens
        if 'deepseek' in model_lower:
            return 128000

        # é»˜è®¤128Kï¼ˆä¿å®ˆä¼°è®¡ï¼Œå¯¹æœªçŸ¥æ¨¡å‹å¦‚GPT-5å¯æ‰‹åŠ¨æŒ‡å®šmax_tokensï¼‰
        logger.warning(f"æœªè¯†åˆ«çš„æ¨¡å‹ {model_name}ï¼Œä½¿ç”¨é»˜è®¤128K context windowã€‚å¦‚éœ€è‡ªå®šä¹‰ï¼Œè¯·åœ¨åˆå§‹åŒ–æ—¶æŒ‡å®šmax_tokenså‚æ•°")
        return 128000

    def calculate_usage(self, messages: List[Dict]) -> Dict[str, Any]:
        """è®¡ç®—contextä½¿ç”¨æƒ…å†µ

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨

        Returns:
            ä½¿ç”¨æƒ…å†µç»Ÿè®¡
        """
        try:
            # æ–¹æ¡ˆ1ï¼šå°è¯•ä½¿ç”¨tiktokenï¼ˆä»…å½“å·²ç¼“å­˜æ—¶ï¼‰
            total_tokens = self._calculate_tokens_tiktoken(messages)

            if total_tokens is None:
                # æ–¹æ¡ˆ2ï¼šé™çº§åˆ°ç®€å•ä¼°ç®—ï¼ˆé¿å…ç½‘ç»œè¶…æ—¶ï¼‰
                total_tokens = self._calculate_tokens_simple(messages)

            usage_percent = (total_tokens / self.max_tokens) * 100
            should_compress = usage_percent >= (self.compression_threshold * 100)

            stats = {
                "total_tokens": total_tokens,
                "max_tokens": self.max_tokens,
                "usage_percent": round(usage_percent, 2),
                "available_tokens": self.max_tokens - total_tokens,
                "should_compress": should_compress,
                "compression_threshold": self.compression_threshold * 100
            }

            logger.info(f"Contextä½¿ç”¨ç‡: {stats['usage_percent']}% ({total_tokens}/{self.max_tokens})")

            return stats

        except Exception as e:
            logger.error(f"è®¡ç®—contextä½¿ç”¨ç‡å¤±è´¥: {str(e)}")
            # è¿”å›é»˜è®¤å€¼
            return {
                "total_tokens": 0,
                "max_tokens": self.max_tokens,
                "usage_percent": 0.0,
                "available_tokens": self.max_tokens,
                "should_compress": False,
                "compression_threshold": self.compression_threshold * 100
            }

    def _calculate_tokens_tiktoken(self, messages: List[Dict]) -> int:
        """ä½¿ç”¨tiktokenè®¡ç®—tokenæ•°ï¼ˆä½¿ç”¨é¡¹ç›®æœ¬åœ°ç¼“å­˜ï¼Œé¿å…ç½‘ç»œä¸‹è½½ï¼‰

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨

        Returns:
            tokenæ€»æ•°ï¼Œå¦‚æœå¤±è´¥è¿”å›None
        """
        try:
            import tiktoken
            import socket
            import os

            # ä¼˜å…ˆä½¿ç”¨é¡¹ç›®æœ¬åœ°ç¼“å­˜ç›®å½•ï¼ˆä».envåŠ è½½ï¼‰
            tiktoken_cache_dir = os.environ.get("TIKTOKEN_CACHE_DIR")
            if tiktoken_cache_dir:
                # ç›¸å¯¹è·¯å¾„è½¬ç»å¯¹è·¯å¾„
                if not os.path.isabs(tiktoken_cache_dir):
                    from pathlib import Path
                    tiktoken_cache_dir = str(Path.cwd() / tiktoken_cache_dir)
                os.environ["TIKTOKEN_CACHE_DIR"] = tiktoken_cache_dir
                logger.debug(f"ä½¿ç”¨tiktokenç¼“å­˜ç›®å½•: {tiktoken_cache_dir}")
            else:
                # å›é€€åˆ°é»˜è®¤ç”¨æˆ·ç›®å½•
                tiktoken_cache_dir = os.path.expanduser("~/.cache/tiktoken")

            # æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨
            cache_exists = os.path.exists(tiktoken_cache_dir) and any(
                os.path.isfile(os.path.join(tiktoken_cache_dir, f))
                for f in os.listdir(tiktoken_cache_dir)
            ) if os.path.exists(tiktoken_cache_dir) else False

            if not cache_exists:
                logger.info(f"tiktokenç¼“å­˜ä¸å­˜åœ¨({tiktoken_cache_dir})ï¼Œä½¿ç”¨ç®€å•ä¼°ç®—")
                logger.info("æç¤º: è¿è¡Œ 'python scripts/download_tiktoken_cache.py' ä¸‹è½½ç¼–ç æ–‡ä»¶")
                return None

            # è®¾ç½®æ›´çŸ­çš„è¶…æ—¶æ—¶é—´ï¼ˆé˜²æ­¢æ„å¤–ç½‘ç»œè¯·æ±‚ï¼‰
            original_timeout = socket.getdefaulttimeout()
            socket.setdefaulttimeout(2.0)  # 2ç§’è¶…æ—¶

            try:
                try:
                    encoder = tiktoken.encoding_for_model(self.model_name)
                    logger.debug(f"ä½¿ç”¨æ¨¡å‹ä¸“å±ç¼–ç : {self.model_name}")
                except KeyError:
                    # æ¨¡å‹ä¸åœ¨tiktokenä¸­ï¼Œä½¿ç”¨é»˜è®¤ç¼–ç ï¼ˆé™é»˜å¤„ç†ï¼Œè¿™æ˜¯æ­£å¸¸æƒ…å†µï¼‰
                    encoder = tiktoken.get_encoding("cl100k_base")
                    logger.debug(f"æ¨¡å‹{self.model_name}ä½¿ç”¨cl100k_baseç¼–ç ")
            finally:
                socket.setdefaulttimeout(original_timeout)

            # è®¡ç®—æ€»tokenæ•°
            total_tokens = 0
            for msg in messages:
                content = str(msg.get("content", ""))
                # Function callingç›¸å…³çš„ä¹Ÿè¦è®¡ç®—
                if msg.get("tool_calls"):
                    content += str(msg["tool_calls"])
                if msg.get("name"):
                    content += msg["name"]

                total_tokens += len(encoder.encode(content))

            logger.debug(f"ä½¿ç”¨tiktokenè®¡ç®—: {total_tokens} tokens")
            return total_tokens

        except Exception as e:
            logger.warning(f"tiktokenè®¡ç®—å¤±è´¥ï¼Œé™çº§åˆ°ç®€å•ä¼°ç®—: {str(e)}")
            return None

    def _calculate_tokens_simple(self, messages: List[Dict]) -> int:
        """ç®€å•tokenä¼°ç®—ï¼ˆä¸ä¾èµ–tiktokenï¼‰

        ä½¿ç”¨ç»éªŒå…¬å¼ï¼šè‹±æ–‡çº¦4å­—ç¬¦=1tokenï¼Œä¸­æ–‡çº¦1.5å­—ç¬¦=1token

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨

        Returns:
            ä¼°ç®—çš„tokenæ€»æ•°
        """
        total_chars = 0
        chinese_chars = 0

        for msg in messages:
            content = str(msg.get("content", ""))

            # Function callingç›¸å…³çš„ä¹Ÿè¦è®¡ç®—
            if msg.get("tool_calls"):
                content += str(msg["tool_calls"])
            if msg.get("name"):
                content += msg["name"]

            total_chars += len(content)

            # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦
            chinese_chars += sum(1 for char in content if '\u4e00' <= char <= '\u9fff')

        # ä¼°ç®—å…¬å¼ï¼šä¸­æ–‡1.5å­—ç¬¦â‰ˆ1tokenï¼Œè‹±æ–‡4å­—ç¬¦â‰ˆ1token
        # ç®€åŒ–ä¸ºï¼štotal_tokens â‰ˆ chinese_chars / 1.5 + (total_chars - chinese_chars) / 4
        estimated_tokens = int(chinese_chars / 1.5 + (total_chars - chinese_chars) / 4)

        logger.debug(f"ä½¿ç”¨ç®€å•ä¼°ç®—: {estimated_tokens} tokens (chars={total_chars}, chinese={chinese_chars})")
        return estimated_tokens

    def should_compress(self, messages: List[Dict]) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦å‹ç¼©

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨

        Returns:
            æ˜¯å¦éœ€è¦å‹ç¼©
        """
        stats = self.calculate_usage(messages)
        return stats["should_compress"]

    def compress_conversation_history(
        self,
        conversation_history: List[Dict],
        llm_client,
        merge_recent_tools: bool = False  # æ˜¯å¦ä¹Ÿåˆå¹¶æœ€è¿‘å¯¹è¯çš„toolè°ƒç”¨
    ) -> List[Dict]:
        """å‹ç¼©å¯¹è¯å†å²

        Args:
            conversation_history: å®Œæ•´å¯¹è¯å†å²
            llm_client: LLMå®¢æˆ·ç«¯,ç”¨äºç”Ÿæˆæ‘˜è¦
            merge_recent_tools: æ˜¯å¦ä¹Ÿåˆå¹¶æœ€è¿‘å¯¹è¯ä¸­çš„è¿ç»­toolè°ƒç”¨ï¼ˆé»˜è®¤Falseï¼Œä¿ç•™è¯¦ç»†ä¿¡æ¯ï¼‰

        Returns:
            å‹ç¼©åçš„å¯¹è¯å†å²
        """
        if len(conversation_history) <= self.recent_turns_to_keep * 2:
            logger.info("å¯¹è¯å†å²å¤ªçŸ­,æ— éœ€å‹ç¼©")
            return conversation_history

        # è®¡ç®—å‹ç¼©å‰çš„tokenæ•°
        before_stats = self.calculate_usage(conversation_history)
        logger.info(f"å‹ç¼©å‰: {len(conversation_history)}æ¡æ¶ˆæ¯, {before_stats['total_tokens']} tokens")

        # åˆ†ç¦»æœ€è¿‘çš„å¯¹è¯å’Œæ—§å¯¹è¯
        recent = conversation_history[-(self.recent_turns_to_keep * 2):]
        old = conversation_history[:-(self.recent_turns_to_keep * 2)]

        if not old:
            logger.info("æ²¡æœ‰æ—§å¯¹è¯å¯å‹ç¼©")
            return conversation_history

        logger.info(f"å¼€å§‹å‹ç¼©å¯¹è¯å†å²: {len(old)}æ¡æ—§å¯¹è¯ + {len(recent)}æ¡æœ€è¿‘å¯¹è¯")

        try:
            # ç¬¬é›¶æ­¥ï¼šåˆå¹¶è¿ç»­çš„åŒç±»toolè°ƒç”¨ï¼ˆç‰¹åˆ«æ˜¯web_searchï¼‰
            old_merged = self._merge_consecutive_tool_calls(old)
            logger.info(f"Toolè°ƒç”¨åˆå¹¶(æ—§å¯¹è¯): {len(old)}æ¡ â†’ {len(old_merged)}æ¡æ¶ˆæ¯")

            # å¯é€‰ï¼šä¹Ÿåˆå¹¶æœ€è¿‘å¯¹è¯çš„toolè°ƒç”¨ï¼ˆæ¿€è¿›æ¨¡å¼ï¼‰
            if merge_recent_tools:
                recent_merged = self._merge_consecutive_tool_calls(recent)
                logger.info(f"Toolè°ƒç”¨åˆå¹¶(æœ€è¿‘å¯¹è¯): {len(recent)}æ¡ â†’ {len(recent_merged)}æ¡æ¶ˆæ¯")
                recent = recent_merged

            # ç¬¬ä¸€æ­¥ï¼šæ¸…ç†æ—§å¯¹è¯ä¸­çš„toolç»“æœï¼ˆAnthropicæ¨èçš„ä½æˆæœ¬ä¼˜åŒ–ï¼‰
            old_cleaned = self._clear_tool_results(old_merged)
            logger.info(f"Toolç»“æœæ¸…ç†: {len(old_merged)}æ¡ â†’ {len(old_cleaned)}æ¡éç©ºæ¶ˆæ¯")

            # ç¬¬äºŒæ­¥ï¼šç”Ÿæˆå‹ç¼©æ‘˜è¦
            summary = self._generate_summary(old_cleaned, llm_client)

            if not summary:
                logger.error("âš ï¸  æ‘˜è¦ç”Ÿæˆå¤±è´¥æˆ–ä¸ºç©º,ä¿ç•™åŸå§‹å†å²")
                return conversation_history

            logger.info(f"æ‘˜è¦ç”ŸæˆæˆåŠŸ: {len(summary)}å­—ç¬¦")

            # æ„å»ºå‹ç¼©åçš„å†å²
            compressed = [
                {
                    "role": "system",
                    "content": f"[å†å²å¯¹è¯æ‘˜è¦ - è‡ªåŠ¨å‹ç¼©äºç¬¬{len(conversation_history)//2}è½®]\n\n{summary}\n\n---\n\n[ä»¥ä¸‹æ˜¯æœ€è¿‘çš„å¯¹è¯å†…å®¹]"
                }
            ] + recent

            # è®¡ç®—å‹ç¼©åçš„tokenæ•°
            after_stats = self.calculate_usage(compressed)
            compression_ratio = (1 - after_stats['total_tokens'] / before_stats['total_tokens']) * 100

            logger.info(f"å‹ç¼©å®Œæˆ: {len(conversation_history)}æ¡ â†’ {len(compressed)}æ¡")
            logger.info(f"Tokenå‹ç¼©: {before_stats['total_tokens']} â†’ {after_stats['total_tokens']} ({compression_ratio:.1f}%å‡å°‘)")

            # å¦‚æœå‹ç¼©åtokenæ•°åè€Œå¢åŠ ï¼Œè¿”å›åŸå§‹å†å²
            if after_stats['total_tokens'] >= before_stats['total_tokens']:
                logger.warning(f"âš ï¸  å‹ç¼©åtokenæ•°æœªå‡å°‘({before_stats['total_tokens']} â†’ {after_stats['total_tokens']})ï¼Œä¿ç•™åŸå§‹å†å²")
                return conversation_history

            return compressed

        except Exception as e:
            logger.error(f"å¯¹è¯å‹ç¼©å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return conversation_history

    def _merge_consecutive_tool_calls(self, messages: List[Dict]) -> List[Dict]:
        """åˆå¹¶è¿ç»­çš„åŒç±»toolè°ƒç”¨ï¼ˆç‰¹åˆ«æ˜¯web_searchï¼‰

        ç­–ç•¥ï¼š
        - web_searchï¼š3æ¬¡ä»¥ä¸Šåˆå¹¶ä¸ºæ‘˜è¦ï¼ˆåªä¿ç•™"æœäº†Næ¬¡+æˆåŠŸç‡"ï¼‰
        - code_executorï¼šä¿ç•™æœ€å1æ¬¡å®Œæ•´è®°å½•
        - å…¶ä»–å·¥å…·ï¼šä¸åˆå¹¶

        Args:
            messages: åŸå§‹æ¶ˆæ¯åˆ—è¡¨

        Returns:
            åˆå¹¶åçš„æ¶ˆæ¯åˆ—è¡¨
        """
        merged = []
        buffer = []  # ç¼“å­˜è¿ç»­çš„åŒç±»toolæ¶ˆæ¯

        for msg in messages:
            if msg.get('role') == 'tool':
                tool_name = msg.get('name', '')

                # å¦‚æœbufferä¸ºç©ºæˆ–åŒç±»å·¥å…·ï¼ŒåŠ å…¥buffer
                if not buffer or buffer[0].get('name') == tool_name:
                    buffer.append(msg)
                else:
                    # ä¸åŒç±»å·¥å…·ï¼Œå¤„ç†buffer
                    merged.extend(self._process_tool_buffer(buffer))
                    buffer = [msg]
            else:
                # étoolæ¶ˆæ¯ï¼Œå…ˆå¤„ç†buffer
                if buffer:
                    merged.extend(self._process_tool_buffer(buffer))
                    buffer = []
                merged.append(msg)

        # å¤„ç†æœ€åçš„buffer
        if buffer:
            merged.extend(self._process_tool_buffer(buffer))

        return merged

    def _process_tool_buffer(self, buffer: List[Dict]) -> List[Dict]:
        """å¤„ç†tool bufferï¼šå†³å®šä¿ç•™ã€åˆå¹¶è¿˜æ˜¯åˆ é™¤

        Args:
            buffer: è¿ç»­çš„åŒç±»toolæ¶ˆæ¯

        Returns:
            å¤„ç†åçš„æ¶ˆæ¯åˆ—è¡¨
        """
        if not buffer:
            return []

        if len(buffer) < 3:
            return buffer  # å°‘äº3æ¬¡ï¼Œä¸åˆå¹¶

        tool_name = buffer[0].get('name', '')

        # Web Searchï¼šåˆå¹¶ä¸ºæ‘˜è¦
        if tool_name == 'web_search':
            import json

            # ç»Ÿè®¡æˆåŠŸç‡
            success_count = 0
            total_count = len(buffer)

            for msg in buffer:
                try:
                    content = msg.get('content', '')
                    data = json.loads(content)
                    if data.get('status') == 'success':
                        success_count += 1
                except:
                    pass

            # ç”Ÿæˆæ‘˜è¦æ¶ˆæ¯
            summary_msg = {
                'role': 'tool',
                'tool_call_id': buffer[-1]['tool_call_id'],  # ç”¨æœ€åä¸€æ¬¡çš„ID
                'name': tool_name,
                'content': json.dumps({
                    'status': 'summary',
                    'data': {
                        'tool': 'web_search',
                        'total_calls': total_count,
                        'successful': success_count,
                        'failed': total_count - success_count,
                        'note': f'æ‰§è¡Œäº†{total_count}æ¬¡æœç´¢ï¼ŒæˆåŠŸ{success_count}æ¬¡ã€‚è¯¦ç»†ç»“æœå·²å‹ç¼©ä»¥èŠ‚çœcontextã€‚'
                    }
                }, ensure_ascii=False)
            }

            logger.info(f"[Toolåˆå¹¶] web_search: {total_count}æ¬¡è°ƒç”¨ â†’ 1æ¡æ‘˜è¦æ¶ˆæ¯")
            return [summary_msg]

        # Code Executorï¼šä¿ç•™æœ€å1æ¬¡
        elif tool_name == 'code_executor':
            logger.info(f"[Toolåˆå¹¶] code_executor: {len(buffer)}æ¬¡è°ƒç”¨ â†’ ä¿ç•™æœ€å1æ¬¡")
            return [buffer[-1]]

        # å…¶ä»–å·¥å…·ï¼šä¿æŒåŸæ ·
        else:
            return buffer

    def _clear_tool_results(self, messages: List[Dict]) -> List[Dict]:
        """æ¸…ç†toolç»“æœä»¥èŠ‚çœcontextï¼ˆAnthropicæ¨èçš„tool result clearingï¼‰

        ç­–ç•¥ï¼š
        1. ä¿ç•™toolè°ƒç”¨æœ¬èº«ï¼ˆassistantçš„tool_callsï¼‰
        2. å‹ç¼©å†—é•¿çš„toolç»“æœï¼ˆåªä¿ç•™æ‘˜è¦ï¼‰
        3. å®Œå…¨ç§»é™¤çº¯çŠ¶æ€åé¦ˆçš„toolæ¶ˆæ¯

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨

        Returns:
            æ¸…ç†åçš„æ¶ˆæ¯åˆ—è¡¨
        """
        cleaned = []
        for msg in messages:
            if msg.get('role') == 'tool':
                content = msg.get('content', '')

                # å¦‚æœæ˜¯çŸ­æ¶ˆæ¯ï¼ˆ<200å­—ç¬¦ï¼‰ï¼Œç›´æ¥ä¿ç•™
                if len(content) < 200:
                    cleaned.append(msg)
                    continue

                # å¯¹äºé•¿æ¶ˆæ¯ï¼Œå‹ç¼©ä¸ºæ‘˜è¦
                try:
                    # å°è¯•è§£æJSONç»“æ„
                    import json
                    data = json.loads(content)

                    # æ„å»ºç®€æ´æ‘˜è¦
                    summary_parts = []
                    if data.get('status'):
                        summary_parts.append(f"Status: {data['status']}")
                    if data.get('generated_files'):
                        files = data['generated_files']
                        summary_parts.append(f"Files: {', '.join(files[:3])}")
                    if data.get('error'):
                        summary_parts.append(f"Error: {data['error'][:100]}")

                    summary = " | ".join(summary_parts) if summary_parts else content[:150]

                    cleaned.append({
                        'role': 'tool',
                        'tool_call_id': msg.get('tool_call_id'),
                        'name': msg.get('name'),
                        'content': f"[Compressed: {len(content)} chars] {summary}"
                    })
                except:
                    # å¦‚æœä¸æ˜¯JSONï¼Œç›´æ¥æˆªæ–­
                    cleaned.append({
                        'role': 'tool',
                        'tool_call_id': msg.get('tool_call_id'),
                        'name': msg.get('name'),
                        'content': f"[Compressed: {len(content)} chars] {content[:200]}..."
                    })
            else:
                # étoolæ¶ˆæ¯ï¼Œä¿æŒåŸæ ·
                cleaned.append(msg)

        return cleaned

    def _generate_summary(self, old_conversation: List[Dict], llm_client) -> str:
        """ç”Ÿæˆå¯¹è¯æ‘˜è¦

        Args:
            old_conversation: æ—§å¯¹è¯åˆ—è¡¨
            llm_client: LLMå®¢æˆ·ç«¯

        Returns:
            æ‘˜è¦æ–‡æœ¬
        """
        # æ„é€ å¯¹è¯æ–‡æœ¬
        conversation_text = self._format_conversation_for_summary(old_conversation)

        # å‹ç¼©æç¤ºè¯
        compression_prompt = self._build_compression_prompt(conversation_text)

        logger.info(f"è°ƒç”¨LLMç”Ÿæˆå¯¹è¯æ‘˜è¦... (åŸå¯¹è¯: {len(conversation_text)}å­—ç¬¦)")

        try:
            # è°ƒç”¨LLMç”Ÿæˆæ‘˜è¦
            response = llm_client.chat(
                messages=[
                    {"role": "user", "content": compression_prompt}
                ],
                tools=None,  # ä¸ä½¿ç”¨å·¥å…·
                stream=False
            )

            # æ£€æŸ¥å“åº”æ ¼å¼
            if isinstance(response, dict):
                summary = response.get("content", "")
            elif isinstance(response, str):
                summary = response
            else:
                logger.error(f"LLMè¿”å›æœªçŸ¥æ ¼å¼: {type(response)}")
                return ""

            if not summary or not summary.strip():
                logger.error("LLMè¿”å›ç©ºæ‘˜è¦")
                return ""

            summary = summary.strip()
            logger.info(f"æ‘˜è¦ç”ŸæˆæˆåŠŸ: {len(summary)}å­—ç¬¦ (å‹ç¼©æ¯”: {len(summary)/len(conversation_text)*100:.1f}%)")

            return summary

        except Exception as e:
            logger.error(f"LLMç”Ÿæˆæ‘˜è¦å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return ""

    def _format_conversation_for_summary(self, conversation: List[Dict]) -> str:
        """æ ¼å¼åŒ–å¯¹è¯ä¸ºå¯è¯»æ–‡æœ¬

        Args:
            conversation: å¯¹è¯åˆ—è¡¨

        Returns:
            æ ¼å¼åŒ–çš„å¯¹è¯æ–‡æœ¬
        """
        lines = []
        turn_number = 1

        for i in range(0, len(conversation), 2):
            if i + 1 < len(conversation):
                user_msg = conversation[i]
                assistant_msg = conversation[i + 1]

                lines.append(f"ã€ç¬¬{turn_number}è½®å¯¹è¯ã€‘")
                lines.append(f"ç”¨æˆ·: {user_msg.get('content', '')}")
                lines.append(f"åŠ©æ‰‹: {assistant_msg.get('content', '')}")
                lines.append("")

                turn_number += 1

        return "\n".join(lines)

    def _build_compression_prompt(self, conversation_text: str) -> str:
        """æ„å»ºå‹ç¼©æç¤ºè¯ï¼ˆåŸºäºAnthropicçš„context engineeringæœ€ä½³å®è·µï¼‰

        Args:
            conversation_text: å¯¹è¯æ–‡æœ¬

        Returns:
            å‹ç¼©æç¤ºè¯
        """
        return f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¯¹è¯å†å²å‹ç¼©åŠ©æ‰‹ã€‚è¯·å°†ä»¥ä¸‹å†å²å¯¹è¯å‹ç¼©ä¸ºé«˜ä¿¡å·å¯†åº¦çš„æ‘˜è¦ã€‚

# ğŸ¯ å‹ç¼©åŸåˆ™ï¼ˆå‚è€ƒAnthropic Context Engineeringï¼‰

æ‰¾åˆ°**æœ€å°çš„é«˜ä¿¡å·tokené›†åˆ**æ¥ä¿ç•™å…³é”®ä¸Šä¸‹æ–‡ï¼Œä¸¢å¼ƒå†—ä½™ä¿¡æ¯ã€‚

## âœ… å¿…é¡»ä¿ç•™ (Critical Elements)

### 1. æ¶æ„å†³ç­– (Architectural Decisions)
   - ç”¨æˆ·é€‰æ‹©çš„æŠ€æœ¯æ–¹æ¡ˆã€å·¥å…·ã€æ–¹æ³•
   - æ˜ç¡®æ‹’ç»çš„æ–¹æ¡ˆå’ŒåŸå› 

### 2. æœªè§£å†³çš„é—®é¢˜ (Unresolved Issues)
   - é‡åˆ°çš„bugå’Œé”™è¯¯
   - å¾…è§£å†³çš„æŠ€æœ¯éš¾é¢˜
   - éœ€è¦åç»­å…³æ³¨çš„é—®é¢˜

### 3. å®ç°ç»†èŠ‚ (Implementation Details)
   - å…³é”®å‚æ•°å’Œé…ç½®ï¼ˆå°ºå¯¸ã€æ ¼å¼ã€é£æ ¼ï¼‰
   - æ–‡ä»¶åå’Œæ–‡ä»¶å…³ç³»
   - ä»£ç å®ç°çš„é‡è¦çº¦æŸ

### 4. ç”¨æˆ·åå¥½å’Œæ˜ç¡®æŒ‡ä»¤
   - è´¨é‡æ ‡å‡†å’Œçº¦æŸæ¡ä»¶
   - é£æ ¼åå¥½ï¼ˆé¢œè‰²ã€å­—ä½“ç­‰ï¼‰
   - æ˜ç¡®çš„"è¦"å’Œ"ä¸è¦"

## âŒ å¯ä»¥ä¸¢å¼ƒ (Redundant Content)

1. å†—ä½™çš„toolè¾“å‡ºï¼ˆå·²è¢«æ¸…ç†ä¸ºæ‘˜è¦ï¼‰
2. ä¸­é—´çš„æ€è€ƒè¿‡ç¨‹å’Œå°è¯•
3. é‡å¤çš„ç¡®è®¤å’Œç¤¼è²Œç”¨è¯­
4. å·²è¢«åç»­æ“ä½œæ›¿ä»£çš„ä¸´æ—¶å†…å®¹

## ğŸ“ è¾“å‡ºæ ¼å¼

ä½¿ç”¨ç®€æ´çš„ç»“æ„åŒ–æ ¼å¼ï¼š

```
ã€æ ¸å¿ƒä»»åŠ¡ã€‘
[ä¸€å¥è¯æ€»ç»“ç”¨æˆ·çš„æ•´ä½“ç›®æ ‡]

ã€å·²å®Œæˆã€‘
- ä»»åŠ¡1: ç»“æœï¼ˆæ–‡ä»¶åã€å…³é”®å‚æ•°ï¼‰
- ä»»åŠ¡2: ç»“æœ

ã€è¿›è¡Œä¸­/å¾…è§£å†³ã€‘
- é—®é¢˜A: çŠ¶æ€æè¿°
- ä»»åŠ¡B: è®¡åˆ’è¯´æ˜

ã€å…³é”®å†³ç­–ã€‘
- æŠ€æœ¯é€‰æ‹©: åŸå› 
- ç”¨æˆ·åå¥½: å…·ä½“è¦æ±‚

ã€é‡è¦æ–‡ä»¶ã€‘
- file1.ext: ç”¨é€” | å‚æ•°
- file2.ext: ç”¨é€” | å…³è”
```

---

# ğŸ“‹ éœ€è¦å‹ç¼©çš„å¯¹è¯å†…å®¹

{conversation_text}

---

**è¯·ç”Ÿæˆå‹ç¼©æ‘˜è¦ï¼ˆ200-500 tokensä¸ºä½³ï¼‰**ï¼š"""
